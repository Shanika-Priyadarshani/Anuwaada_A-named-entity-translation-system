{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import pandas, numpy, string, xgboost\n",
    "import tensorflow as tf\n",
    "\n",
    "import langid\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the provided data\n",
    "train_set = pd.read_csv('data/train_data_1.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sinhala</th>\n",
       "      <th>Tamil</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20318</td>\n",
       "      <td>20318</td>\n",
       "      <td>20318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>20292</td>\n",
       "      <td>20310</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>රත්නායක මුදියන්සේලාගේ කරුණාවතී</td>\n",
       "      <td>ஹேரத் முதியன்சேலாகே பத்மசிறி பண்டார ஹேரத்</td>\n",
       "      <td>Sinhala</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>16635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Sinhala  \\\n",
       "count                            20318   \n",
       "unique                           20292   \n",
       "top     රත්නායක මුදියන්සේලාගේ කරුණාවතී   \n",
       "freq                                 4   \n",
       "\n",
       "                                            Tamil    Class  \n",
       "count                                       20318    20318  \n",
       "unique                                      20310        3  \n",
       "top     ஹேரத் முதியன்சேலாகே பத்மசிறி பண்டார ஹேரத்  Sinhala  \n",
       "freq                                            2    16635  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = train_set['Class']\n",
    "\n",
    "train_set.drop('Class', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sinhala\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(train_set['Sinhala'], targets)\n",
    "\n",
    "#tamil\n",
    "train_x_t, valid_x_t, train_y_t, valid_y_t = model_selection.train_test_split(train_set['Tamil'], targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sinhala\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n",
    "\n",
    "#tamil\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y_t = encoder.fit_transform(train_y_t)\n",
    "valid_y_t = encoder.fit_transform(valid_y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count Vectors as features\n",
    "\n",
    "# create a count vectorizer object for sinhala\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(train_set)\n",
    "\n",
    "# transform the training and validation data using count vectorizer object for sinhala\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)\n",
    "\n",
    "\n",
    "\n",
    "# create a count vectorizer object for tamil\n",
    "count_vect_t = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect_t.fit(train_set)\n",
    "\n",
    "# transform the training and validation data using count vectorizer object for tamil\n",
    "xtrain_count_t =  count_vect_t.transform(train_x_t)\n",
    "xvalid_count_t =  count_vect_t.transform(valid_x_t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF Vectors as features\n",
    "\n",
    "# word level tf-idf for sinhala\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(train_set['Sinhala'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "# ngram level tf-idf for sinhala\n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(train_set['Sinhala'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "\n",
    "# characters level tf-idf for sinhala\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(train_set['Sinhala'])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# word level tf-idf for tamil\n",
    "tfidf_vect_t = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect_t.fit(train_set['Tamil'])\n",
    "xtrain_tfidf_t =  tfidf_vect_t.transform(train_x_t)\n",
    "xvalid_tfidf_t =  tfidf_vect_t.transform(valid_x_t)\n",
    "\n",
    "# ngram level tf-idf for tamil\n",
    "tfidf_vect_ngram_t = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_t.fit(train_set['Tamil'])\n",
    "xtrain_tfidf_ngram_t =  tfidf_vect_ngram_t.transform(train_x_t)\n",
    "xvalid_tfidf_ngram_t =  tfidf_vect_ngram_t.transform(valid_x_t)\n",
    "\n",
    "# characters level tf-idf for tamil\n",
    "tfidf_vect_ngram_chars_t = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars_t.fit(train_set['Tamil'])\n",
    "xtrain_tfidf_ngram_chars_t =  tfidf_vect_ngram_chars_t.transform(train_x_t) \n",
    "xvalid_tfidf_ngram_chars_t =  tfidf_vect_ngram_chars_t.transform(valid_x_t) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, test_valid_y, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    print(test_valid_y)\n",
    "    \n",
    "    accuracy = metrics.accuracy_score(predictions, test_valid_y)\n",
    "    precision = metrics.precision_score(test_valid_y, predictions, average='weighted')\n",
    "    recall = metrics.recall_score(test_valid_y, predictions, average='weighted')\n",
    "    f1_score = metrics.f1_score(test_valid_y, predictions, average='weighted')\n",
    "    \n",
    "    \n",
    "    return accuracy, precision, recall, f1_score, classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Sinhala Scripts\n",
    "\n",
    "# Linear Classifier on Count Vectors\n",
    "#accuracy, precision, recall, f1_score, predictions_cont = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count, test_count)\n",
    "#print (\"LC, Count Vectors: \", accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)\n",
    "\n",
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "#accuracy, precision, recall, f1_score, predictions_tf = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf, test_tfidf)\n",
    "#print (\"LC, WordLevel TF-IDF: \", accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)\n",
    "\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "#accuracy, precision, recall, f1_score, predictions_ngram = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, test_tfidf_ngram)\n",
    "#print (\"LC, N-Gram Vectors: \", accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)\n",
    "\n",
    "# Linear Classifier on Character Level TF IDF Vectors\n",
    "#accuracy, precision, recall, f1_score, predictions_char = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars, test_tfidf_ngram_chars)\n",
    "#print (\"LC, CharLevel Vectors: \", accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Sinhala Scripts\n",
    "\n",
    "# SVM on Ngram Level TF IDF Vectors\n",
    "#accuracy, precision, recall, f1_score, predictions_ngram = train_model(svm.SVC(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, test_tfidf_ngram)\n",
    "#print (\"SVM, N-Gram Vectors: \", accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Sinhala Scripts\n",
    "\n",
    "# Extereme Gradient Boosting on Count Vectors\n",
    "#accuracy, precision, recall, f1_score, predictions_cont = train_model(xgboost.XGBClassifier(), xtrain_count, train_y, xvalid_count, test_count)\n",
    "#print (\"Xgb, Count Vectors: \", accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)\n",
    "\n",
    "# Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
    "#accuracy, precision, recall, f1_score, predictions_tf = train_model(xgboost.XGBClassifier(), xtrain_tfidf, train_y, xvalid_tfidf, test_tfidf)\n",
    "#print (\"Xgb, WordLevel TF-IDF: \", accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)\n",
    "\n",
    "# Extereme Gradient Boosting on Ngram Level TF IDF Vectors\n",
    "#accuracy, precision, recall, f1_score, predictions_ngram = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, test_tfidf_ngram)\n",
    "#print (\"Xgb, N-Gram Vectors: \", accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)\n",
    "\n",
    "# Extereme Gradient Boosting on Character Level TF IDF Vectors\n",
    "#accuracy, precision, recall, f1_score, predictions_char = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars, test_tfidf_ngram_chars)\n",
    "#print (\"Xgb, CharLevel Vectors: \", accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 1 1 1]\n",
      "NB, Count Vectors for Sinhala Classifier:  0.8047244094488188       Precision:  0.6475813751627503        Recall:  0.8047244094488188      F1_Score:  0.717651262178615\n",
      "[1 1 1 ... 1 1 1]\n",
      "NB, WordLevel TF-IDF for Sinhala Classifier:  0.8862204724409449       Precision:  0.8855814783159618        Recall:  0.8862204724409449      F1_Score:  0.8581450563461351\n",
      "[1 1 1 ... 1 1 1]\n",
      "NB, N-Gram Vectors for Sinhala Classifier:  0.9289370078740158       Precision:  0.925582696663703        Recall:  0.9289370078740158      F1_Score:  0.921433901052541\n",
      "[1 1 1 ... 1 1 1]\n",
      "NB, CharLevel Vectors for Sinhala Classifier:  0.9576771653543307       Precision:  0.9567072904921449        Recall:  0.9576771653543307      F1_Score:  0.9570272461563771\n",
      "[2 1 1 ... 1 0 1]\n",
      "NB, Count Vectors for Tamil Classifier:  0.825       Precision:  0.6806249999999999        Recall:  0.825      F1_Score:  0.7458904109589041\n",
      "[2 1 1 ... 1 0 1]\n",
      "NB, WordLevel TF-IDF for Tamil Classifier:  0.8854330708661418       Precision:  0.8885157729571777        Recall:  0.8854330708661418      F1_Score:  0.858443478244163\n",
      "[2 1 1 ... 1 0 1]\n",
      "NB, N-Gram Vectors for Tamil Classifier:  0.9374015748031496       Precision:  0.9345935251992208        Recall:  0.9374015748031496      F1_Score:  0.9325358159364169\n",
      "[2 1 1 ... 1 0 1]\n",
      "NB, CharLevel Vectors for Tamil Classifier:  0.9612204724409449       Precision:  0.9600726389106659        Recall:  0.9612204724409449      F1_Score:  0.9603492971654913\n"
     ]
    }
   ],
   "source": [
    "# Sinhala  Script Classifier\n",
    "# Naive Bayes on Count Vectors\n",
    "accuracy, precision, recall, f1_score, classifier_cont = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count, valid_y)\n",
    "print (\"NB, Count Vectors for Sinhala Classifier: \",  accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)\n",
    "\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy, precision, recall, f1_score, classifier_tf = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf, valid_y)\n",
    "print (\"NB, WordLevel TF-IDF for Sinhala Classifier: \",  accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy, precision, recall, f1_score, classifier_ngram = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, valid_y)\n",
    "print (\"NB, N-Gram Vectors for Sinhala Classifier: \",  accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy, precision, recall, f1_score, classifier_char = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars, valid_y)\n",
    "print (\"NB, CharLevel Vectors for Sinhala Classifier: \",  accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)\n",
    "\n",
    "\n",
    "\n",
    "#Tamil Script Classifier\n",
    "# Naive Bayes on Count Vectors\n",
    "accuracy, precision, recall, f1_score, classifier_cont_t = train_model(naive_bayes.MultinomialNB(), xtrain_count_t, train_y_t, xvalid_count_t, valid_y_t)\n",
    "print (\"NB, Count Vectors for Tamil Classifier: \",  accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)\n",
    "\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy, precision, recall, f1_score, classifier_tf_t = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_t, train_y_t, xvalid_tfidf_t, valid_y_t)\n",
    "print (\"NB, WordLevel TF-IDF for Tamil Classifier: \",  accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy, precision, recall, f1_score, classifier_ngram_t = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_t, train_y_t, xvalid_tfidf_ngram_t, valid_y_t)\n",
    "print (\"NB, N-Gram Vectors for Tamil Classifier: \",  accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy, precision, recall, f1_score, classifier_char_t = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars_t, train_y_t, xvalid_tfidf_ngram_chars_t, valid_y_t)\n",
    "print (\"NB, CharLevel Vectors for Tamil Classifier: \",  accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MultinomialNB' object has no attribute 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-f579578dee09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mloaded_model2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"පරමේශ්වරම්\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtest_tfidf_ngram_chars\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mloaded_model2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mpredictions_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_tfidf_ngram_chars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions_char\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MultinomialNB' object has no attribute 'transform'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(classifier_char, open(filename, 'wb'))\n",
    "\n",
    "filename2 = 'tfidf_vect_ngram_chars.sav'\n",
    "pickle.dump(tfidf_vect_ngram_chars, open(filename2, 'wb'))\n",
    "\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "loaded_model2 = pickle.load(open(filename, 'rb'))\n",
    "name=\"පරමේශ්වරම්\"\n",
    "test_tfidf_ngram_chars =  loaded_model2.transform([name])\n",
    "predictions_char = loaded_model.predict(test_tfidf_ngram_chars)\n",
    "print(predictions_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the best accuracy,presicion, recall and F1-score, was given by the char level vector classifier for both Tamil and Sinhala, \n",
    "# it was choosen as the best model (with naive bayes classifier)\n",
    "test_set = pd.read_csv('data/test_data_names.csv')\n",
    "\n",
    "ethnic_type={0:'Muslim', 1:'Sinhala', 2:'Tamil'}\n",
    "predictions = []\n",
    "\n",
    "name=\"මුතෙන්දාස්\"\n",
    "\n",
    "languageID= langid.classify(name)\n",
    "#print (name)\n",
    "if(languageID[0]=='si'):\n",
    "    test_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform([name])\n",
    "    predictions_char = classifier_char.predict(test_tfidf_ngram_chars)\n",
    "    predictions.append(ethnic_type[predictions_char[0]])\n",
    "    print(predictions_char)\n",
    "else:\n",
    "    test_tfidf_ngram_chars_t =  tfidf_vect_ngram_chars_t.transform([name])\n",
    "    predictions_char_t = classifier_char_t.predict(test_tfidf_ngram_chars_t)\n",
    "    predictions.append(ethnic_type[predictions_char_t[0]])\n",
    "    print(predictions_char_t)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
