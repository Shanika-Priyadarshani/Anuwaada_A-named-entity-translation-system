{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/shirmila/miniconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/shirmila/miniconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/shirmila/miniconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/shirmila/miniconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/shirmila/miniconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/shirmila/miniconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/shirmila/miniconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/shirmila/miniconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/shirmila/miniconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/shirmila/miniconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/shirmila/miniconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/shirmila/miniconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import pandas, numpy, string, xgboost\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "\n",
    "import langid\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the provided data\n",
    "train_set = pd.read_csv('data/train_data_1.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sinhala</th>\n",
       "      <th>Tamil</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>20318</td>\n",
       "      <td>20318</td>\n",
       "      <td>20318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>unique</td>\n",
       "      <td>20292</td>\n",
       "      <td>20310</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>top</td>\n",
       "      <td>රත්නායක මුදියන්සේලාගේ කරුණාවතී</td>\n",
       "      <td>ஹேரத் முதியன்சேலாகே ரணசிங்ஹ</td>\n",
       "      <td>Sinhala</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>freq</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>16635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Sinhala                        Tamil    Class\n",
       "count                            20318                        20318    20318\n",
       "unique                           20292                        20310        3\n",
       "top     රත්නායක මුදියන්සේලාගේ කරුණාවතී  ஹேரத் முதியன்சேலாகே ரணசிங்ஹ  Sinhala\n",
       "freq                                 4                            2    16635"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = train_set['Class']\n",
    "\n",
    "train_set.drop('Class', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sinhala\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(train_set['Sinhala'], targets)\n",
    "\n",
    "#tamil\n",
    "train_x_t, valid_x_t, train_y_t, valid_y_t = model_selection.train_test_split(train_set['Tamil'], targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sinhala\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n",
    "\n",
    "#tamil\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y_t = encoder.fit_transform(train_y_t)\n",
    "valid_y_t = encoder.fit_transform(valid_y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count Vectors as features\n",
    "\n",
    "# create a count vectorizer object for sinhala\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(train_set)\n",
    "\n",
    "# transform the training and validation data using count vectorizer object for sinhala\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)\n",
    "\n",
    "\n",
    "\n",
    "# create a count vectorizer object for tamil\n",
    "count_vect_t = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect_t.fit(train_set)\n",
    "\n",
    "# transform the training and validation data using count vectorizer object for tamil\n",
    "xtrain_count_t =  count_vect_t.transform(train_x_t)\n",
    "xvalid_count_t =  count_vect_t.transform(valid_x_t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF Vectors as features\n",
    "\n",
    "# word level tf-idf for sinhala\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(train_set['Sinhala'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "# ngram level tf-idf for sinhala\n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(train_set['Sinhala'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "\n",
    "# characters level tf-idf for sinhala\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(train_set['Sinhala'])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# word level tf-idf for tamil\n",
    "tfidf_vect_t = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect_t.fit(train_set['Tamil'])\n",
    "xtrain_tfidf_t =  tfidf_vect_t.transform(train_x_t)\n",
    "xvalid_tfidf_t =  tfidf_vect_t.transform(valid_x_t)\n",
    "\n",
    "# ngram level tf-idf for tamil\n",
    "tfidf_vect_ngram_t = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_t.fit(train_set['Tamil'])\n",
    "xtrain_tfidf_ngram_t =  tfidf_vect_ngram_t.transform(train_x_t)\n",
    "xvalid_tfidf_ngram_t =  tfidf_vect_ngram_t.transform(valid_x_t)\n",
    "\n",
    "# characters level tf-idf for tamil\n",
    "tfidf_vect_ngram_chars_t = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars_t.fit(train_set['Tamil'])\n",
    "xtrain_tfidf_ngram_chars_t =  tfidf_vect_ngram_chars_t.transform(train_x_t) \n",
    "xvalid_tfidf_ngram_chars_t =  tfidf_vect_ngram_chars_t.transform(valid_x_t) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, test_valid_y, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    print(test_valid_y)\n",
    "    \n",
    "    accuracy = metrics.accuracy_score(predictions, test_valid_y)\n",
    "    precision = metrics.precision_score(test_valid_y, predictions, average='weighted')\n",
    "    recall = metrics.recall_score(test_valid_y, predictions, average='weighted')\n",
    "    f1_score = metrics.f1_score(test_valid_y, predictions, average='weighted')\n",
    "    \n",
    "    \n",
    "    return accuracy, precision, recall, f1_score, classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Sinhala Scripts\n",
    "\n",
    "# Linear Classifier on Count Vectors\n",
    "#accuracy, precision, recall, f1_score, predictions_cont = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count, test_count)\n",
    "#print (\"LC, Count Vectors: \", accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)\n",
    "\n",
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "#accuracy, precision, recall, f1_score, predictions_tf = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf, test_tfidf)\n",
    "#print (\"LC, WordLevel TF-IDF: \", accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)\n",
    "\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "#accuracy, precision, recall, f1_score, predictions_ngram = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, test_tfidf_ngram)\n",
    "#print (\"LC, N-Gram Vectors: \", accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)\n",
    "\n",
    "# Linear Classifier on Character Level TF IDF Vectors\n",
    "#accuracy, precision, recall, f1_score, predictions_char = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars, test_tfidf_ngram_chars)\n",
    "#print (\"LC, CharLevel Vectors: \", accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Sinhala Scripts\n",
    "\n",
    "# SVM on Ngram Level TF IDF Vectors\n",
    "#accuracy, precision, recall, f1_score, predictions_ngram = train_model(svm.SVC(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, test_tfidf_ngram)\n",
    "#print (\"SVM, N-Gram Vectors: \", accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Sinhala Scripts\n",
    "\n",
    "# Extereme Gradient Boosting on Count Vectors\n",
    "#accuracy, precision, recall, f1_score, predictions_cont = train_model(xgboost.XGBClassifier(), xtrain_count, train_y, xvalid_count, test_count)\n",
    "#print (\"Xgb, Count Vectors: \", accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)\n",
    "\n",
    "# Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
    "#accuracy, precision, recall, f1_score, predictions_tf = train_model(xgboost.XGBClassifier(), xtrain_tfidf, train_y, xvalid_tfidf, test_tfidf)\n",
    "#print (\"Xgb, WordLevel TF-IDF: \", accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)\n",
    "\n",
    "# Extereme Gradient Boosting on Ngram Level TF IDF Vectors\n",
    "#accuracy, precision, recall, f1_score, predictions_ngram = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, test_tfidf_ngram)\n",
    "#print (\"Xgb, N-Gram Vectors: \", accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)\n",
    "\n",
    "# Extereme Gradient Boosting on Character Level TF IDF Vectors\n",
    "#accuracy, precision, recall, f1_score, predictions_char = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars, test_tfidf_ngram_chars)\n",
    "#print (\"Xgb, CharLevel Vectors: \", accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 1 ... 2 1 1]\n",
      "NB, Count Vectors for Sinhala Classifier:  0.8253937007874016       Precision:  0.6812747612995227        Recall:  0.8253937007874016      F1_Score:  0.7464414509655074\n",
      "[2 1 1 ... 2 1 1]\n",
      "NB, WordLevel TF-IDF for Sinhala Classifier:  0.8980314960629922       Precision:  0.89906008569175        Recall:  0.8980314960629922      F1_Score:  0.8749508463589485\n",
      "[2 1 1 ... 2 1 1]\n",
      "NB, N-Gram Vectors for Sinhala Classifier:  0.9397637795275591       Precision:  0.9364443852844414        Recall:  0.9397637795275591      F1_Score:  0.9346571507859129\n",
      "[2 1 1 ... 2 1 1]\n",
      "NB, CharLevel Vectors for Sinhala Classifier:  0.9618110236220473       Precision:  0.9614878303342019        Recall:  0.9618110236220473      F1_Score:  0.9616285716066222\n",
      "[1 1 1 ... 1 1 0]\n",
      "NB, Count Vectors for Tamil Classifier:  0.8169291338582677       Precision:  0.6673732097464194        Recall:  0.8169291338582677      F1_Score:  0.7346166642495798\n",
      "[1 1 1 ... 1 1 0]\n",
      "NB, WordLevel TF-IDF for Tamil Classifier:  0.8797244094488189       Precision:  0.8798672562243565        Recall:  0.8797244094488189      F1_Score:  0.8520113819253533\n",
      "[1 1 1 ... 1 1 0]\n",
      "NB, N-Gram Vectors for Tamil Classifier:  0.9354330708661417       Precision:  0.9328481396903864        Recall:  0.9354330708661417      F1_Score:  0.9313253162867157\n",
      "[1 1 1 ... 1 1 0]\n",
      "NB, CharLevel Vectors for Tamil Classifier:  0.9568897637795276       Precision:  0.955791378621942        Recall:  0.9568897637795276      F1_Score:  0.956151361834409\n"
     ]
    }
   ],
   "source": [
    "# Sinhala  Script Classifier\n",
    "# Naive Bayes on Count Vectors\n",
    "accuracy, precision, recall, f1_score, classifier_cont = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count, valid_y)\n",
    "print (\"NB, Count Vectors for Sinhala Classifier: \",  accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)\n",
    "\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy, precision, recall, f1_score, classifier_tf = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf, valid_y)\n",
    "print (\"NB, WordLevel TF-IDF for Sinhala Classifier: \",  accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy, precision, recall, f1_score, classifier_ngram = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, valid_y)\n",
    "print (\"NB, N-Gram Vectors for Sinhala Classifier: \",  accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy, precision, recall, f1_score, classifier_char = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars, valid_y)\n",
    "print (\"NB, CharLevel Vectors for Sinhala Classifier: \",  accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)\n",
    "\n",
    "\n",
    "\n",
    "#Tamil Script Classifier\n",
    "# Naive Bayes on Count Vectors\n",
    "accuracy, precision, recall, f1_score, classifier_cont_t = train_model(naive_bayes.MultinomialNB(), xtrain_count_t, train_y_t, xvalid_count_t, valid_y_t)\n",
    "print (\"NB, Count Vectors for Tamil Classifier: \",  accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)\n",
    "\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy, precision, recall, f1_score, classifier_tf_t = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_t, train_y_t, xvalid_tfidf_t, valid_y_t)\n",
    "print (\"NB, WordLevel TF-IDF for Tamil Classifier: \",  accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy, precision, recall, f1_score, classifier_ngram_t = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_t, train_y_t, xvalid_tfidf_ngram_t, valid_y_t)\n",
    "print (\"NB, N-Gram Vectors for Tamil Classifier: \",  accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy, precision, recall, f1_score, classifier_char_t = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars_t, train_y_t, xvalid_tfidf_ngram_chars_t, valid_y_t)\n",
    "print (\"NB, CharLevel Vectors for Tamil Classifier: \",  accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the best accuracy,presicion, recall and F1-score, was given by the char level vector classifier for both Tamil and Sinhala, \n",
    "# it was choosen as the best model (with naive bayes classifier)\n",
    "test_set = pd.read_csv('data/test_data_names.csv')\n",
    "\n",
    "ethnic_type={0:'Muslim', 1:'Sinhala', 2:'Tamil'}\n",
    "predictions = []\n",
    "\n",
    "for name in test_set['Name']:\n",
    "\n",
    "    languageID= langid.classify(name)\n",
    "    #print (name)\n",
    "    if(languageID[0]=='si'):\n",
    "        test_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform([name])\n",
    "        predictions_char = classifier_char.predict(test_tfidf_ngram_chars)\n",
    "        predictions.append(ethnic_type[predictions_char[0]])\n",
    "    else:\n",
    "        test_tfidf_ngram_chars_t =  tfidf_vect_ngram_chars_t.transform([name])\n",
    "        predictions_char_t = classifier_char_t.predict(test_tfidf_ngram_chars_t)\n",
    "        predictions.append(ethnic_type[predictions_char_t[0]])\n",
    "\n",
    "test_set.Class = predictions\n",
    "test_set.to_csv(\"data/result_final_char.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
