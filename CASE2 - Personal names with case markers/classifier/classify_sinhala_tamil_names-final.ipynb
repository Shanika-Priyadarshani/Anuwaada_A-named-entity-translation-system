{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import pandas, numpy, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "\n",
    "import langid\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the provided data\n",
    "train_set = pd.read_csv('data/train_data_1.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sinhala</th>\n",
       "      <th>Tamil</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20318</td>\n",
       "      <td>20318</td>\n",
       "      <td>20318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>20292</td>\n",
       "      <td>20310</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>රත්නායක මුදියන්සේලාගේ කරුණාවතී</td>\n",
       "      <td>இராசலிங்கம் பிரபஞ்சா</td>\n",
       "      <td>Sinhala</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>16635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Sinhala                 Tamil    Class\n",
       "count                            20318                 20318    20318\n",
       "unique                           20292                 20310        3\n",
       "top     රත්නායක මුදියන්සේලාගේ කරුණාවතී  இராசலிங்கம் பிரபஞ்சா  Sinhala\n",
       "freq                                 4                     2    16635"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = train_set['Class']\n",
    "\n",
    "train_set.drop('Class', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sinhala\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(train_set['Sinhala'], targets)\n",
    "\n",
    "#tamil\n",
    "train_x_t, valid_x_t, train_y_t, valid_y_t = model_selection.train_test_split(train_set['Tamil'], targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sinhala\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n",
    "\n",
    "#tamil\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y_t = encoder.fit_transform(train_y_t)\n",
    "valid_y_t = encoder.fit_transform(valid_y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count Vectors as features\n",
    "\n",
    "# create a count vectorizer object for sinhala\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(train_set)\n",
    "\n",
    "# transform the training and validation data using count vectorizer object for sinhala\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)\n",
    "\n",
    "\n",
    "\n",
    "# create a count vectorizer object for tamil\n",
    "count_vect_t = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect_t.fit(train_set)\n",
    "\n",
    "# transform the training and validation data using count vectorizer object for tamil\n",
    "xtrain_count_t =  count_vect_t.transform(train_x_t)\n",
    "xvalid_count_t =  count_vect_t.transform(valid_x_t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF Vectors as features\n",
    "\n",
    "# word level tf-idf for sinhala\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(train_set['Sinhala'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "# ngram level tf-idf for sinhala\n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(train_set['Sinhala'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "\n",
    "# characters level tf-idf for sinhala\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(train_set['Sinhala'])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# word level tf-idf for tamil\n",
    "tfidf_vect_t = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect_t.fit(train_set['Tamil'])\n",
    "xtrain_tfidf_t =  tfidf_vect_t.transform(train_x_t)\n",
    "xvalid_tfidf_t =  tfidf_vect_t.transform(valid_x_t)\n",
    "\n",
    "# ngram level tf-idf for tamil\n",
    "tfidf_vect_ngram_t = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_t.fit(train_set['Tamil'])\n",
    "xtrain_tfidf_ngram_t =  tfidf_vect_ngram_t.transform(train_x_t)\n",
    "xvalid_tfidf_ngram_t =  tfidf_vect_ngram_t.transform(valid_x_t)\n",
    "\n",
    "# characters level tf-idf for tamil\n",
    "tfidf_vect_ngram_chars_t = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars_t.fit(train_set['Tamil'])\n",
    "xtrain_tfidf_ngram_chars_t =  tfidf_vect_ngram_chars_t.transform(train_x_t) \n",
    "xvalid_tfidf_ngram_chars_t =  tfidf_vect_ngram_chars_t.transform(valid_x_t) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, test_valid_y, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    print(test_valid_y)\n",
    "    \n",
    "    accuracy = metrics.accuracy_score(predictions, test_valid_y)\n",
    "    precision = metrics.precision_score(test_valid_y, predictions, average='weighted')\n",
    "    recall = metrics.recall_score(test_valid_y, predictions, average='weighted')\n",
    "    f1_score = metrics.f1_score(test_valid_y, predictions, average='weighted')\n",
    "    \n",
    "    \n",
    "    return accuracy, precision, recall, f1_score, classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Sinhala Scripts\n",
    "\n",
    "# Linear Classifier on Count Vectors\n",
    "#accuracy, precision, recall, f1_score, predictions_cont = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count, test_count)\n",
    "#print (\"LC, Count Vectors: \", accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)\n",
    "\n",
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "#accuracy, precision, recall, f1_score, predictions_tf = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf, test_tfidf)\n",
    "#print (\"LC, WordLevel TF-IDF: \", accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)\n",
    "\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "#accuracy, precision, recall, f1_score, predictions_ngram = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, test_tfidf_ngram)\n",
    "#print (\"LC, N-Gram Vectors: \", accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)\n",
    "\n",
    "# Linear Classifier on Character Level TF IDF Vectors\n",
    "#accuracy, precision, recall, f1_score, predictions_char = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars, test_tfidf_ngram_chars)\n",
    "#print (\"LC, CharLevel Vectors: \", accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Sinhala Scripts\n",
    "\n",
    "# SVM on Ngram Level TF IDF Vectors\n",
    "#accuracy, precision, recall, f1_score, predictions_ngram = train_model(svm.SVC(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, test_tfidf_ngram)\n",
    "#print (\"SVM, N-Gram Vectors: \", accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Sinhala Scripts\n",
    "\n",
    "# Extereme Gradient Boosting on Count Vectors\n",
    "#accuracy, precision, recall, f1_score, predictions_cont = train_model(xgboost.XGBClassifier(), xtrain_count, train_y, xvalid_count, test_count)\n",
    "#print (\"Xgb, Count Vectors: \", accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)\n",
    "\n",
    "# Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
    "#accuracy, precision, recall, f1_score, predictions_tf = train_model(xgboost.XGBClassifier(), xtrain_tfidf, train_y, xvalid_tfidf, test_tfidf)\n",
    "#print (\"Xgb, WordLevel TF-IDF: \", accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)\n",
    "\n",
    "# Extereme Gradient Boosting on Ngram Level TF IDF Vectors\n",
    "#accuracy, precision, recall, f1_score, predictions_ngram = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, test_tfidf_ngram)\n",
    "#print (\"Xgb, N-Gram Vectors: \", accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)\n",
    "\n",
    "# Extereme Gradient Boosting on Character Level TF IDF Vectors\n",
    "#accuracy, precision, recall, f1_score, predictions_char = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars, test_tfidf_ngram_chars)\n",
    "#print (\"Xgb, CharLevel Vectors: \", accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 1 1 1]\n",
      "NB, Count Vectors for Sinhala Classifier:  0.8151574803149606       Precision:  0.6644817177134354        Recall:  0.8151574803149606      F1_Score:  0.732147733648032\n",
      "[1 1 1 ... 1 1 1]\n",
      "NB, WordLevel TF-IDF for Sinhala Classifier:  0.8923228346456693       Precision:  0.8960501502496896        Recall:  0.8923228346456693      F1_Score:  0.8653029198276698\n",
      "[1 1 1 ... 1 1 1]\n",
      "NB, N-Gram Vectors for Sinhala Classifier:  0.9344488188976378       Precision:  0.9310857067980385        Recall:  0.9344488188976378      F1_Score:  0.9287873263649787\n",
      "[1 1 1 ... 1 1 1]\n",
      "NB, CharLevel Vectors for Sinhala Classifier:  0.9612204724409449       Precision:  0.960756828086432        Recall:  0.9612204724409449      F1_Score:  0.9609400433854244\n",
      "[1 0 0 ... 1 1 1]\n",
      "NB, Count Vectors for Tamil Classifier:  0.8190944881889763       Precision:  0.6709157805815611        Recall:  0.8190944881889763      F1_Score:  0.7376370880541782\n",
      "[1 0 0 ... 1 1 1]\n",
      "NB, WordLevel TF-IDF for Tamil Classifier:  0.8828740157480315       Precision:  0.888818826709329        Recall:  0.8828740157480315      F1_Score:  0.8562631245402987\n",
      "[1 0 0 ... 1 1 1]\n",
      "NB, N-Gram Vectors for Tamil Classifier:  0.9356299212598426       Precision:  0.9333181007071366        Recall:  0.9356299212598426      F1_Score:  0.9306286274305856\n",
      "[1 0 0 ... 1 1 1]\n",
      "NB, CharLevel Vectors for Tamil Classifier:  0.9590551181102362       Precision:  0.9578957098946608        Recall:  0.9590551181102362      F1_Score:  0.9582090691706153\n"
     ]
    }
   ],
   "source": [
    "# Sinhala  Script Classifier\n",
    "# Naive Bayes on Count Vectors\n",
    "accuracy, precision, recall, f1_score, classifier_cont = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count, valid_y)\n",
    "print (\"NB, Count Vectors for Sinhala Classifier: \",  accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)\n",
    "\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy, precision, recall, f1_score, classifier_tf = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf, valid_y)\n",
    "print (\"NB, WordLevel TF-IDF for Sinhala Classifier: \",  accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy, precision, recall, f1_score, classifier_ngram = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, valid_y)\n",
    "print (\"NB, N-Gram Vectors for Sinhala Classifier: \",  accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy, precision, recall, f1_score, classifier_char = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars, valid_y)\n",
    "print (\"NB, CharLevel Vectors for Sinhala Classifier: \",  accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)\n",
    "\n",
    "\n",
    "\n",
    "#Tamil Script Classifier\n",
    "# Naive Bayes on Count Vectors\n",
    "accuracy, precision, recall, f1_score, classifier_cont_t = train_model(naive_bayes.MultinomialNB(), xtrain_count_t, train_y_t, xvalid_count_t, valid_y_t)\n",
    "print (\"NB, Count Vectors for Tamil Classifier: \",  accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)\n",
    "\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy, precision, recall, f1_score, classifier_tf_t = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_t, train_y_t, xvalid_tfidf_t, valid_y_t)\n",
    "print (\"NB, WordLevel TF-IDF for Tamil Classifier: \",  accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy, precision, recall, f1_score, classifier_ngram_t = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_t, train_y_t, xvalid_tfidf_ngram_t, valid_y_t)\n",
    "print (\"NB, N-Gram Vectors for Tamil Classifier: \",  accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy, precision, recall, f1_score, classifier_char_t = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars_t, train_y_t, xvalid_tfidf_ngram_chars_t, valid_y_t)\n",
    "print (\"NB, CharLevel Vectors for Tamil Classifier: \",  accuracy, \"      Precision: \", precision, \"       Recall: \", recall, \"     F1_Score: \", f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-faa3bc40b576>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mlanguageID\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mlangid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguageID\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'si'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/langid/langid.py\u001b[0m in \u001b[0;36mclassify\u001b[0;34m(instance)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0midentifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/langid/langid.py\u001b[0m in \u001b[0;36mclassify\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0mClassify\u001b[0m \u001b[0man\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \"\"\"\n\u001b[0;32m--> 294\u001b[0;31m     \u001b[0mfv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstance2fv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_classprobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0mcl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/langid/langid.py\u001b[0m in \u001b[0;36minstance2fv\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0mstatecount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mletter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m       \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk_nextmove\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m<<\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mletter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0mstatecount\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "# Since the best accuracy,presicion, recall and F1-score, was given by the char level vector classifier for both Tamil and Sinhala, \n",
    "# it was choosen as the best model (with naive bayes classifier)\n",
    "test_set = pd.read_csv('data/test_data_locations.csv')\n",
    "\n",
    "ethnic_type={0:'Muslim', 1:'Sinhala', 2:'Tamil'}\n",
    "predictions = []\n",
    "\n",
    "for name in test_set['Name']:\n",
    "\n",
    "    languageID= langid.classify(name)\n",
    "\n",
    "    if(languageID[0]=='si'):\n",
    "        test_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform([name])\n",
    "        predictions_char = classifier_char.predict(test_tfidf_ngram_chars)\n",
    "        predictions.append(ethnic_type[predictions_char[0]])\n",
    "    else:\n",
    "        test_tfidf_ngram_chars_t =  tfidf_vect_ngram_chars_t.transform([name])\n",
    "        predictions_char_t = classifier_char_t.predict(test_tfidf_ngram_chars_t)\n",
    "        predictions.append(ethnic_type[predictions_char_t[0]])\n",
    "\n",
    "test_set.Class = predictions\n",
    "test_set.to_csv(\"data/result_test_data_locations.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
