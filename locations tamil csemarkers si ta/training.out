nohup: ignoring input
Using SCRIPTS_ROOTDIR: /home/shanika/mosesdecoder/scripts
Using single-thread GIZA
using gzip 
(1) preparing corpus @ Wed Nov 27 00:46:04 +0530 2019
Executing: mkdir -p /home/shanika/working/train/corpus
(1.0) selecting factors @ Wed Nov 27 00:46:04 +0530 2019
(1.1) running mkcls  @ Wed Nov 27 00:46:04 +0530 2019
/home/shanika/mosesdecoder/tools/mkcls -c50 -n2 -p/home/shanika/corpus/data.si-ta.clean.si -V/home/shanika/working/train/corpus/si.vcb.classes opt
Executing: /home/shanika/mosesdecoder/tools/mkcls -c50 -n2 -p/home/shanika/corpus/data.si-ta.clean.si -V/home/shanika/working/train/corpus/si.vcb.classes opt

***** 2 runs. (algorithm:TA)*****
;KategProblem:cats: 50   words: 484

start-costs: MEAN: 5.7485e+06 (5.74714e+06-5.74985e+06)  SIGMA:1354.24   
  end-costs: MEAN: 5.5097e+06 (5.50941e+06-5.50999e+06)  SIGMA:289.675   
   start-pp: MEAN: 40.1173 (39.9999-40.2347)  SIGMA:0.117396   
     end-pp: MEAN: 23.9459 (23.9309-23.9609)  SIGMA:0.0149889   
 iterations: MEAN: 13651.5 (13410-13893)  SIGMA:241.5   
       time: MEAN: 0.692676 (0.657667-0.727685)  SIGMA:0.035009   
(1.1) running mkcls  @ Wed Nov 27 00:46:05 +0530 2019
/home/shanika/mosesdecoder/tools/mkcls -c50 -n2 -p/home/shanika/corpus/data.si-ta.clean.ta -V/home/shanika/working/train/corpus/ta.vcb.classes opt
Executing: /home/shanika/mosesdecoder/tools/mkcls -c50 -n2 -p/home/shanika/corpus/data.si-ta.clean.ta -V/home/shanika/working/train/corpus/ta.vcb.classes opt

***** 2 runs. (algorithm:TA)*****
;KategProblem:cats: 50   words: 341

start-costs: MEAN: 6.02037e+06 (6.01985e+06-6.02089e+06)  SIGMA:518.281   
  end-costs: MEAN: 5.69988e+06 (5.6996e+06-5.70017e+06)  SIGMA:287.417   
   start-pp: MEAN: 35.8316 (35.7933-35.8698)  SIGMA:0.0382678   
     end-pp: MEAN: 18.5121 (18.5012-18.5231)  SIGMA:0.0109641   
 iterations: MEAN: 10414 (10107-10721)  SIGMA:307   
       time: MEAN: 0.634343 (0.60098-0.667707)  SIGMA:0.0333635   
(1.2) creating vcb file /home/shanika/working/train/corpus/si.vcb @ Wed Nov 27 00:46:07 +0530 2019
(1.2) creating vcb file /home/shanika/working/train/corpus/ta.vcb @ Wed Nov 27 00:46:07 +0530 2019
(1.3) numberizing corpus /home/shanika/working/train/corpus/si-ta-int-train.snt @ Wed Nov 27 00:46:07 +0530 2019
(1.3) numberizing corpus /home/shanika/working/train/corpus/ta-si-int-train.snt @ Wed Nov 27 00:46:07 +0530 2019
(2) running giza @ Wed Nov 27 00:46:08 +0530 2019
(2.1a) running snt2cooc si-ta @ Wed Nov 27 00:46:08 +0530 2019

Executing: mkdir -p /home/shanika/working/train/giza.si-ta
Executing: /home/shanika/mosesdecoder/tools/snt2cooc.out /home/shanika/working/train/corpus/ta.vcb /home/shanika/working/train/corpus/si.vcb /home/shanika/working/train/corpus/si-ta-int-train.snt > /home/shanika/working/train/giza.si-ta/si-ta.cooc
/home/shanika/mosesdecoder/tools/snt2cooc.out /home/shanika/working/train/corpus/ta.vcb /home/shanika/working/train/corpus/si.vcb /home/shanika/working/train/corpus/si-ta-int-train.snt > /home/shanika/working/train/giza.si-ta/si-ta.cooc
line 1000
line 2000
line 3000
line 4000
line 5000
line 6000
line 7000
line 8000
line 9000
line 10000
line 11000
line 12000
line 13000
line 14000
line 15000
line 16000
line 17000
line 18000
line 19000
line 20000
line 21000
line 22000
line 23000
line 24000
line 25000
line 26000
line 27000
line 28000
line 29000
line 30000
line 31000
line 32000
line 33000
line 34000
line 35000
line 36000
line 37000
line 38000
line 39000
line 40000
line 41000
line 42000
line 43000
line 44000
line 45000
line 46000
line 47000
line 48000
line 49000
line 50000
END.
(2.1b) running giza si-ta @ Wed Nov 27 00:46:08 +0530 2019
/home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.si-ta/si-ta.cooc -c /home/shanika/working/train/corpus/si-ta-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.si-ta/si-ta -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/ta.vcb -t /home/shanika/working/train/corpus/si.vcb
Executing: /home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.si-ta/si-ta.cooc -c /home/shanika/working/train/corpus/si-ta-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.si-ta/si-ta -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/ta.vcb -t /home/shanika/working/train/corpus/si.vcb
/home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.si-ta/si-ta.cooc -c /home/shanika/working/train/corpus/si-ta-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.si-ta/si-ta -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/ta.vcb -t /home/shanika/working/train/corpus/si.vcb
Parameter 'coocurrencefile' changed from '' to '/home/shanika/working/train/giza.si-ta/si-ta.cooc'
Parameter 'c' changed from '' to '/home/shanika/working/train/corpus/si-ta-int-train.snt'
Parameter 'm3' changed from '5' to '3'
Parameter 'm4' changed from '5' to '3'
Parameter 'model1dumpfrequency' changed from '0' to '1'
Parameter 'model4smoothfactor' changed from '0.2' to '0.4'
Parameter 'nodumps' changed from '0' to '1'
Parameter 'nsmooth' changed from '64' to '4'
Parameter 'o' changed from '119-11-27.004608.shanika' to '/home/shanika/working/train/giza.si-ta/si-ta'
Parameter 'onlyaldumps' changed from '0' to '1'
Parameter 'p0' changed from '-1' to '0.999'
Parameter 's' changed from '' to '/home/shanika/working/train/corpus/ta.vcb'
Parameter 't' changed from '' to '/home/shanika/working/train/corpus/si.vcb'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 119-11-27.004608.shanika.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/shanika/working/train/giza.si-ta/si-ta  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/shanika/working/train/corpus/si-ta-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/shanika/working/train/corpus/ta.vcb  (source vocabulary file name)
t = /home/shanika/working/train/corpus/si.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 119-11-27.004608.shanika.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/shanika/working/train/giza.si-ta/si-ta  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/shanika/working/train/corpus/si-ta-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/shanika/working/train/corpus/ta.vcb  (source vocabulary file name)
t = /home/shanika/working/train/corpus/si.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Reading vocabulary file from:/home/shanika/working/train/corpus/ta.vcb
Reading vocabulary file from:/home/shanika/working/train/corpus/si.vcb
Source vocabulary list has 342 unique tokens 
Target vocabulary list has 485 unique tokens 
Calculating vocabulary frequencies from corpus /home/shanika/working/train/corpus/si-ta-int-train.snt
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
 Train total # sentence pairs (weighted): 50001
Size of source portion of the training corpus: 435285 tokens
Size of the target portion of the training corpus: 412778 tokens 
In source portion of the training corpus, only 341 unique tokens appeared
In target portion of the training corpus, only 483 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 412778/(485286-50001)== 0.948294
There are 34113 34113 entries in table
==========================================================
Model1 Training Started at: Wed Nov 27 00:46:08 2019

-----------
Model1: Iteration 1
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (1) TRAIN CROSS-ENTROPY 9.30529 PERPLEXITY 632.662
Model1: (1) VITERBI TRAIN CROSS-ENTROPY 12.6231 PERPLEXITY 6308.68
Model 1 Iteration: 1 took: 0 seconds
-----------
Model1: Iteration 2
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (2) TRAIN CROSS-ENTROPY 6.01144 PERPLEXITY 64.5097
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 7.73222 PERPLEXITY 212.632
Model 1 Iteration: 2 took: 1 seconds
-----------
Model1: Iteration 3
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (3) TRAIN CROSS-ENTROPY 5.30469 PERPLEXITY 39.5249
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 6.36787 PERPLEXITY 82.5883
Model 1 Iteration: 3 took: 0 seconds
-----------
Model1: Iteration 4
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (4) TRAIN CROSS-ENTROPY 5.0171 PERPLEXITY 32.3815
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 5.80208 PERPLEXITY 55.7958
Model 1 Iteration: 4 took: 0 seconds
-----------
Model1: Iteration 5
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (5) TRAIN CROSS-ENTROPY 4.91717 PERPLEXITY 30.2145
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 5.57147 PERPLEXITY 47.5533
Model 1 Iteration: 5 took: 1 seconds
Entire Model1 Training took: 2 seconds
NOTE: I am doing iterations with the HMM model!
Read classes: #words: 341  #classes: 51
Read classes: #words: 484  #classes: 51

==========================================================
Hmm Training Started at: Wed Nov 27 00:46:10 2019

-----------
Hmm: Iteration 1
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 10609 parameters.
Hmm: (1) TRAIN CROSS-ENTROPY 4.87712 PERPLEXITY 29.3873
Hmm: (1) VITERBI TRAIN CROSS-ENTROPY 5.46597 PERPLEXITY 44.2

Hmm Iteration: 1 took: 2 seconds

-----------
Hmm: Iteration 2
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 10609 parameters.
Hmm: (2) TRAIN CROSS-ENTROPY 3.42887 PERPLEXITY 10.7694
Hmm: (2) VITERBI TRAIN CROSS-ENTROPY 3.69078 PERPLEXITY 12.9133

Hmm Iteration: 2 took: 2 seconds

-----------
Hmm: Iteration 3
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 10609 parameters.
Hmm: (3) TRAIN CROSS-ENTROPY 2.95311 PERPLEXITY 7.74415
Hmm: (3) VITERBI TRAIN CROSS-ENTROPY 3.10345 PERPLEXITY 8.5947

Hmm Iteration: 3 took: 2 seconds

-----------
Hmm: Iteration 4
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 10609 parameters.
Hmm: (4) TRAIN CROSS-ENTROPY 2.78736 PERPLEXITY 6.90365
Hmm: (4) VITERBI TRAIN CROSS-ENTROPY 2.8838 PERPLEXITY 7.3809

Hmm Iteration: 4 took: 2 seconds

-----------
Hmm: Iteration 5
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 10609 parameters.
Hmm: (5) TRAIN CROSS-ENTROPY 2.70835 PERPLEXITY 6.53575
Hmm: (5) VITERBI TRAIN CROSS-ENTROPY 2.77974 PERPLEXITY 6.86726

Hmm Iteration: 5 took: 2 seconds

Entire Hmm Training took: 10 seconds
==========================================================
Read classes: #words: 341  #classes: 51
Read classes: #words: 484  #classes: 51
Read classes: #words: 341  #classes: 51
Read classes: #words: 484  #classes: 51

==========================================================
Starting H333444:  Viterbi Training
 H333444 Training Started at: Wed Nov 27 00:46:20 2019


---------------------
THTo3: Iteration 1
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 110.361 #alsophisticatedcountcollection: 0 #hcsteps: 0
#peggingImprovements: 0
A/D table contains 10609 parameters.
A/D table contains 10648 parameters.
NTable contains 3420 parameter.
p0_count is 366164 and p1 is 22654.2; p0 is 0.999 p1: 0.001
THTo3: TRAIN CROSS-ENTROPY 2.30081 PERPLEXITY 4.92736
THTo3: (1) TRAIN VITERBI CROSS-ENTROPY 2.34517 PERPLEXITY 5.0812

THTo3 Viterbi Iteration : 1 took: 2 seconds

---------------------
Model3: Iteration 2
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 110.532 #alsophisticatedcountcollection: 0 #hcsteps: 1.35325
#peggingImprovements: 0
A/D table contains 10609 parameters.
A/D table contains 10648 parameters.
NTable contains 3420 parameter.
p0_count is 389343 and p1 is 11717.3; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 3.29897 PERPLEXITY 9.84216
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 3.33172 PERPLEXITY 10.0681

Model3 Viterbi Iteration : 2 took: 2 seconds

---------------------
Model3: Iteration 3
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 110.545 #alsophisticatedcountcollection: 0 #hcsteps: 1.36347
#peggingImprovements: 0
A/D table contains 10609 parameters.
A/D table contains 10648 parameters.
NTable contains 3420 parameter.
p0_count is 395292 and p1 is 8743.02; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 3.21449 PERPLEXITY 9.28233
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 3.24335 PERPLEXITY 9.46993

Model3 Viterbi Iteration : 3 took: 2 seconds

---------------------
T3To4: Iteration 4
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 110.543 #alsophisticatedcountcollection: 10.319 #hcsteps: 1.36205
#peggingImprovements: 0
D4 table contains 511763 parameters.
A/D table contains 10609 parameters.
A/D table contains 10648 parameters.
NTable contains 3420 parameter.
p0_count is 397468 and p1 is 7654.86; p0 is 0.999 p1: 0.001
T3To4: TRAIN CROSS-ENTROPY 3.17921 PERPLEXITY 9.05811
T3To4: (4) TRAIN VITERBI CROSS-ENTROPY 3.20594 PERPLEXITY 9.22749

T3To4 Viterbi Iteration : 4 took: 3 seconds

---------------------
Model4: Iteration 5
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 110.502 #alsophisticatedcountcollection: 7.32173 #hcsteps: 1.33383
#peggingImprovements: 0
D4 table contains 511763 parameters.
A/D table contains 10609 parameters.
A/D table contains 10683 parameters.
NTable contains 3420 parameter.
p0_count is 399710 and p1 is 6534.14; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 3.09884 PERPLEXITY 8.56731
Model4: (5) TRAIN VITERBI CROSS-ENTROPY 3.11845 PERPLEXITY 8.68454

Model4 Viterbi Iteration : 5 took: 4 seconds

---------------------
Model4: Iteration 6
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 110.485 #alsophisticatedcountcollection: 6.73625 #hcsteps: 1.29127
#peggingImprovements: 0
D4 table contains 511763 parameters.
A/D table contains 10609 parameters.
A/D table contains 10683 parameters.
NTable contains 3420 parameter.
p0_count is 400212 and p1 is 6282.79; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 3.05276 PERPLEXITY 8.29798
Model4: (6) TRAIN VITERBI CROSS-ENTROPY 3.07023 PERPLEXITY 8.39907

Model4 Viterbi Iteration : 6 took: 4 seconds
H333444 Training Finished at: Wed Nov 27 00:46:37 2019


Entire Viterbi H333444 Training took: 17 seconds
==========================================================

Entire Training took: 29 seconds
Program Finished at: Wed Nov 27 00:46:37 2019

==========================================================
Executing: rm -f /home/shanika/working/train/giza.si-ta/si-ta.A3.final.gz
Executing: gzip /home/shanika/working/train/giza.si-ta/si-ta.A3.final
(2.1a) running snt2cooc ta-si @ Wed Nov 27 00:46:37 +0530 2019

Executing: mkdir -p /home/shanika/working/train/giza.ta-si
Executing: /home/shanika/mosesdecoder/tools/snt2cooc.out /home/shanika/working/train/corpus/si.vcb /home/shanika/working/train/corpus/ta.vcb /home/shanika/working/train/corpus/ta-si-int-train.snt > /home/shanika/working/train/giza.ta-si/ta-si.cooc
/home/shanika/mosesdecoder/tools/snt2cooc.out /home/shanika/working/train/corpus/si.vcb /home/shanika/working/train/corpus/ta.vcb /home/shanika/working/train/corpus/ta-si-int-train.snt > /home/shanika/working/train/giza.ta-si/ta-si.cooc
line 1000
line 2000
line 3000
line 4000
line 5000
line 6000
line 7000
line 8000
line 9000
line 10000
line 11000
line 12000
line 13000
line 14000
line 15000
line 16000
line 17000
line 18000
line 19000
line 20000
line 21000
line 22000
line 23000
line 24000
line 25000
line 26000
line 27000
line 28000
line 29000
line 30000
line 31000
line 32000
line 33000
line 34000
line 35000
line 36000
line 37000
line 38000
line 39000
line 40000
line 41000
line 42000
line 43000
line 44000
line 45000
line 46000
line 47000
line 48000
line 49000
line 50000
END.
(2.1b) running giza ta-si @ Wed Nov 27 00:46:37 +0530 2019
/home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.ta-si/ta-si.cooc -c /home/shanika/working/train/corpus/ta-si-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.ta-si/ta-si -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/si.vcb -t /home/shanika/working/train/corpus/ta.vcb
Executing: /home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.ta-si/ta-si.cooc -c /home/shanika/working/train/corpus/ta-si-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.ta-si/ta-si -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/si.vcb -t /home/shanika/working/train/corpus/ta.vcb
/home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.ta-si/ta-si.cooc -c /home/shanika/working/train/corpus/ta-si-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.ta-si/ta-si -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/si.vcb -t /home/shanika/working/train/corpus/ta.vcb
Parameter 'coocurrencefile' changed from '' to '/home/shanika/working/train/giza.ta-si/ta-si.cooc'
Parameter 'c' changed from '' to '/home/shanika/working/train/corpus/ta-si-int-train.snt'
Parameter 'm3' changed from '5' to '3'
Parameter 'm4' changed from '5' to '3'
Parameter 'model1dumpfrequency' changed from '0' to '1'
Parameter 'model4smoothfactor' changed from '0.2' to '0.4'
Parameter 'nodumps' changed from '0' to '1'
Parameter 'nsmooth' changed from '64' to '4'
Parameter 'o' changed from '119-11-27.004637.shanika' to '/home/shanika/working/train/giza.ta-si/ta-si'
Parameter 'onlyaldumps' changed from '0' to '1'
Parameter 'p0' changed from '-1' to '0.999'
Parameter 's' changed from '' to '/home/shanika/working/train/corpus/si.vcb'
Parameter 't' changed from '' to '/home/shanika/working/train/corpus/ta.vcb'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 119-11-27.004637.shanika.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/shanika/working/train/giza.ta-si/ta-si  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/shanika/working/train/corpus/ta-si-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/shanika/working/train/corpus/si.vcb  (source vocabulary file name)
t = /home/shanika/working/train/corpus/ta.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 119-11-27.004637.shanika.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/shanika/working/train/giza.ta-si/ta-si  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/shanika/working/train/corpus/ta-si-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/shanika/working/train/corpus/si.vcb  (source vocabulary file name)
t = /home/shanika/working/train/corpus/ta.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Reading vocabulary file from:/home/shanika/working/train/corpus/si.vcb
Reading vocabulary file from:/home/shanika/working/train/corpus/ta.vcb
Source vocabulary list has 485 unique tokens 
Target vocabulary list has 342 unique tokens 
Calculating vocabulary frequencies from corpus /home/shanika/working/train/corpus/ta-si-int-train.snt
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
 Train total # sentence pairs (weighted): 50001
Size of source portion of the training corpus: 412778 tokens
Size of the target portion of the training corpus: 435285 tokens 
In source portion of the training corpus, only 484 unique tokens appeared
In target portion of the training corpus, only 340 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 435285/(462779-50001)== 1.05453
There are 33970 33970 entries in table
==========================================================
Model1 Training Started at: Wed Nov 27 00:46:38 2019

-----------
Model1: Iteration 1
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (1) TRAIN CROSS-ENTROPY 8.7732 PERPLEXITY 437.517
Model1: (1) VITERBI TRAIN CROSS-ENTROPY 12.0261 PERPLEXITY 4170.81
Model 1 Iteration: 1 took: 0 seconds
-----------
Model1: Iteration 2
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (2) TRAIN CROSS-ENTROPY 5.9002 PERPLEXITY 59.7224
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 7.58871 PERPLEXITY 192.5
Model 1 Iteration: 2 took: 0 seconds
-----------
Model1: Iteration 3
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (3) TRAIN CROSS-ENTROPY 5.21917 PERPLEXITY 37.2501
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 6.26148 PERPLEXITY 76.7175
Model 1 Iteration: 3 took: 0 seconds
-----------
Model1: Iteration 4
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (4) TRAIN CROSS-ENTROPY 4.94349 PERPLEXITY 30.7707
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 5.71767 PERPLEXITY 52.6247
Model 1 Iteration: 4 took: 1 seconds
-----------
Model1: Iteration 5
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (5) TRAIN CROSS-ENTROPY 4.84751 PERPLEXITY 28.7904
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 5.49692 PERPLEXITY 45.1584
Model 1 Iteration: 5 took: 0 seconds
Entire Model1 Training took: 1 seconds
NOTE: I am doing iterations with the HMM model!
Read classes: #words: 484  #classes: 51
Read classes: #words: 341  #classes: 51

==========================================================
Hmm Training Started at: Wed Nov 27 00:46:39 2019

-----------
Hmm: Iteration 1
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 10890 parameters.
Hmm: (1) TRAIN CROSS-ENTROPY 4.80862 PERPLEXITY 28.0246
Hmm: (1) VITERBI TRAIN CROSS-ENTROPY 5.39468 PERPLEXITY 42.0688

Hmm Iteration: 1 took: 2 seconds

-----------
Hmm: Iteration 2
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 10890 parameters.
Hmm: (2) TRAIN CROSS-ENTROPY 3.39856 PERPLEXITY 10.5455
Hmm: (2) VITERBI TRAIN CROSS-ENTROPY 3.67863 PERPLEXITY 12.805

Hmm Iteration: 2 took: 2 seconds

-----------
Hmm: Iteration 3
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 10890 parameters.
Hmm: (3) TRAIN CROSS-ENTROPY 2.91282 PERPLEXITY 7.53091
Hmm: (3) VITERBI TRAIN CROSS-ENTROPY 3.09041 PERPLEXITY 8.51737

Hmm Iteration: 3 took: 2 seconds

-----------
Hmm: Iteration 4
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 10890 parameters.
Hmm: (4) TRAIN CROSS-ENTROPY 2.74029 PERPLEXITY 6.68206
Hmm: (4) VITERBI TRAIN CROSS-ENTROPY 2.86472 PERPLEXITY 7.28393

Hmm Iteration: 4 took: 2 seconds

-----------
Hmm: Iteration 5
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 10890 parameters.
Hmm: (5) TRAIN CROSS-ENTROPY 2.65082 PERPLEXITY 6.28024
Hmm: (5) VITERBI TRAIN CROSS-ENTROPY 2.76145 PERPLEXITY 6.78077

Hmm Iteration: 5 took: 2 seconds

Entire Hmm Training took: 10 seconds
==========================================================
Read classes: #words: 484  #classes: 51
Read classes: #words: 341  #classes: 51
Read classes: #words: 484  #classes: 51
Read classes: #words: 341  #classes: 51

==========================================================
Starting H333444:  Viterbi Training
 H333444 Training Started at: Wed Nov 27 00:46:49 2019


---------------------
THTo3: Iteration 1
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 114.123 #alsophisticatedcountcollection: 0 #hcsteps: 0
#peggingImprovements: 0
A/D table contains 10890 parameters.
A/D table contains 10425 parameters.
NTable contains 4850 parameter.
p0_count is 376987 and p1 is 28772.9; p0 is 0.999 p1: 0.001
THTo3: TRAIN CROSS-ENTROPY 2.27859 PERPLEXITY 4.85204
THTo3: (1) TRAIN VITERBI CROSS-ENTROPY 2.36148 PERPLEXITY 5.13897

THTo3 Viterbi Iteration : 1 took: 2 seconds

---------------------
Model3: Iteration 2
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 114.213 #alsophisticatedcountcollection: 0 #hcsteps: 1.45963
#peggingImprovements: 0
A/D table contains 10890 parameters.
A/D table contains 10425 parameters.
NTable contains 4850 parameter.
p0_count is 408525 and p1 is 13380; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 3.40937 PERPLEXITY 10.6249
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 3.45571 PERPLEXITY 10.9716

Model3 Viterbi Iteration : 2 took: 1 seconds

---------------------
Model3: Iteration 3
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 114.22 #alsophisticatedcountcollection: 0 #hcsteps: 1.41679
#peggingImprovements: 0
A/D table contains 10890 parameters.
A/D table contains 10425 parameters.
NTable contains 4850 parameter.
p0_count is 415040 and p1 is 10122.6; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 3.29687 PERPLEXITY 9.82778
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 3.33233 PERPLEXITY 10.0723

Model3 Viterbi Iteration : 3 took: 2 seconds

---------------------
T3To4: Iteration 4
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 114.187 #alsophisticatedcountcollection: 11.4105 #hcsteps: 1.38027
#peggingImprovements: 0
D4 table contains 514605 parameters.
A/D table contains 10890 parameters.
A/D table contains 10425 parameters.
NTable contains 4850 parameter.
p0_count is 417696 and p1 is 8794.61; p0 is 0.999 p1: 0.001
T3To4: TRAIN CROSS-ENTROPY 3.24947 PERPLEXITY 9.51018
T3To4: (4) TRAIN VITERBI CROSS-ENTROPY 3.27894 PERPLEXITY 9.70639

T3To4 Viterbi Iteration : 4 took: 3 seconds

---------------------
Model4: Iteration 5
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 114.148 #alsophisticatedcountcollection: 7.57149 #hcsteps: 1.33285
#peggingImprovements: 0
D4 table contains 514605 parameters.
A/D table contains 10890 parameters.
A/D table contains 10428 parameters.
NTable contains 4850 parameter.
p0_count is 419519 and p1 is 7882.98; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 3.11632 PERPLEXITY 8.67171
Model4: (5) TRAIN VITERBI CROSS-ENTROPY 3.13616 PERPLEXITY 8.79184

Model4 Viterbi Iteration : 5 took: 4 seconds

---------------------
Model4: Iteration 6
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 114.133 #alsophisticatedcountcollection: 6.95052 #hcsteps: 1.29311
#peggingImprovements: 0
D4 table contains 514605 parameters.
A/D table contains 10890 parameters.
A/D table contains 10428 parameters.
NTable contains 4850 parameter.
p0_count is 420491 and p1 is 7397.1; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 3.0739 PERPLEXITY 8.42048
Model4: (6) TRAIN VITERBI CROSS-ENTROPY 3.08915 PERPLEXITY 8.50994

Model4 Viterbi Iteration : 6 took: 5 seconds
H333444 Training Finished at: Wed Nov 27 00:47:06 2019


Entire Viterbi H333444 Training took: 17 seconds
==========================================================

Entire Training took: 29 seconds
Program Finished at: Wed Nov 27 00:47:06 2019

==========================================================
Executing: rm -f /home/shanika/working/train/giza.ta-si/ta-si.A3.final.gz
Executing: gzip /home/shanika/working/train/giza.ta-si/ta-si.A3.final
(3) generate word alignment @ Wed Nov 27 00:47:06 +0530 2019
Combining forward and inverted alignment from files:
  /home/shanika/working/train/giza.si-ta/si-ta.A3.final.{bz2,gz}
  /home/shanika/working/train/giza.ta-si/ta-si.A3.final.{bz2,gz}
Executing: mkdir -p /home/shanika/working/train/model
Executing: /home/shanika/mosesdecoder/scripts/training/giza2bal.pl -d "gzip -cd /home/shanika/working/train/giza.ta-si/ta-si.A3.final.gz" -i "gzip -cd /home/shanika/working/train/giza.si-ta/si-ta.A3.final.gz" |/home/shanika/mosesdecoder/scripts/../bin/symal -alignment="grow" -diagonal="yes" -final="yes" -both="yes" > /home/shanika/working/train/model/aligned.grow-diag-final-and
symal: computing grow alignment: diagonal (1) final (1)both-uncovered (1)
skip=<0> counts=<50001>
(4) generate lexical translation table 0-0 @ Wed Nov 27 00:47:09 +0530 2019
(/home/shanika/corpus/data.si-ta.clean.si,/home/shanika/corpus/data.si-ta.clean.ta,/home/shanika/working/train/model/lex)
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
Saved: /home/shanika/working/train/model/lex.f2e and /home/shanika/working/train/model/lex.e2f
FILE: /home/shanika/corpus/data.si-ta.clean.ta
FILE: /home/shanika/corpus/data.si-ta.clean.si
FILE: /home/shanika/working/train/model/aligned.grow-diag-final-and
(5) extract phrases @ Wed Nov 27 00:47:10 +0530 2019
/home/shanika/mosesdecoder/scripts/generic/extract-parallel.perl 4 split "sort    " /home/shanika/mosesdecoder/scripts/../bin/extract /home/shanika/corpus/data.si-ta.clean.ta /home/shanika/corpus/data.si-ta.clean.si /home/shanika/working/train/model/aligned.grow-diag-final-and /home/shanika/working/train/model/extract 7 orientation --model wbe-msd --GZOutput 
Executing: /home/shanika/mosesdecoder/scripts/generic/extract-parallel.perl 4 split "sort    " /home/shanika/mosesdecoder/scripts/../bin/extract /home/shanika/corpus/data.si-ta.clean.ta /home/shanika/corpus/data.si-ta.clean.si /home/shanika/working/train/model/aligned.grow-diag-final-and /home/shanika/working/train/model/extract 7 orientation --model wbe-msd --GZOutput 
MAX 7 1 0
Started Wed Nov 27 00:47:10 2019
using gzip 
isBSDSplit=0 
Executing: mkdir -p /home/shanika/working/train/model/tmp.11391; ls -l /home/shanika/working/train/model/tmp.11391 
total=50001 line-per-split=12501 
split -d -l 12501 -a 7 /home/shanika/corpus/data.si-ta.clean.ta /home/shanika/working/train/model/tmp.11391/target.split -d -l 12501 -a 7 /home/shanika/corpus/data.si-ta.clean.si /home/shanika/working/train/model/tmp.11391/source.split -d -l 12501 -a 7 /home/shanika/working/train/model/aligned.grow-diag-final-and /home/shanika/working/train/model/tmp.11391/align.merging extract / extract.inv
gunzip -c /home/shanika/working/train/model/tmp.11391/extract.0000000.gz /home/shanika/working/train/model/tmp.11391/extract.0000001.gz /home/shanika/working/train/model/tmp.11391/extract.0000002.gz /home/shanika/working/train/model/tmp.11391/extract.0000003.gz  | LC_ALL=C sort     -T /home/shanika/working/train/model/tmp.11391 2>> /dev/stderr | gzip -c > /home/shanika/working/train/model/extract.sorted.gz 2>> /dev/stderr 
gunzip -c /home/shanika/working/train/model/tmp.11391/extract.0000000.inv.gz /home/shanika/working/train/model/tmp.11391/extract.0000001.inv.gz /home/shanika/working/train/model/tmp.11391/extract.0000002.inv.gz /home/shanika/working/train/model/tmp.11391/extract.0000003.inv.gz  | LC_ALL=C sort     -T /home/shanika/working/train/model/tmp.11391 2>> /dev/stderr | gzip -c > /home/shanika/working/train/model/extract.inv.sorted.gz 2>> /dev/stderr 
gunzip -c /home/shanika/working/train/model/tmp.11391/extract.0000000.o.gz /home/shanika/working/train/model/tmp.11391/extract.0000001.o.gz /home/shanika/working/train/model/tmp.11391/extract.0000002.o.gz /home/shanika/working/train/model/tmp.11391/extract.0000003.o.gz  | LC_ALL=C sort     -T /home/shanika/working/train/model/tmp.11391 2>> /dev/stderr | gzip -c > /home/shanika/working/train/model/extract.o.sorted.gz 2>> /dev/stderr 
Finished Wed Nov 27 00:47:25 2019
(6) score phrases @ Wed Nov 27 00:47:25 +0530 2019
(6.1)  creating table half /home/shanika/working/train/model/phrase-table.half.f2e @ Wed Nov 27 00:47:25 +0530 2019
/home/shanika/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/extract.sorted.gz /home/shanika/working/train/model/lex.f2e /home/shanika/working/train/model/phrase-table.half.f2e.gz  0 
Executing: /home/shanika/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/extract.sorted.gz /home/shanika/working/train/model/lex.f2e /home/shanika/working/train/model/phrase-table.half.f2e.gz  0 
using gzip 
Started Wed Nov 27 00:47:25 2019
/home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/tmp.11445/extract.0.gz /home/shanika/working/train/model/lex.f2e /home/shanika/working/train/model/tmp.11445/phrase-table.half.0000000.gz  2>> /dev/stderr 
/home/shanika/working/train/model/tmp.11445/run.0.sh/home/shanika/working/train/model/tmp.11445/run.1.sh/home/shanika/working/train/model/tmp.11445/run.2.sh/home/shanika/working/train/model/tmp.11445/run.3.shmv /home/shanika/working/train/model/tmp.11445/phrase-table.half.0000000.gz /home/shanika/working/train/model/phrase-table.half.f2e.gzrm -rf /home/shanika/working/train/model/tmp.11445 
Finished Wed Nov 27 00:47:30 2019
(6.3)  creating table half /home/shanika/working/train/model/phrase-table.half.e2f @ Wed Nov 27 00:47:30 +0530 2019
/home/shanika/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/extract.inv.sorted.gz /home/shanika/working/train/model/lex.e2f /home/shanika/working/train/model/phrase-table.half.e2f.gz --Inverse 1 
Executing: /home/shanika/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/extract.inv.sorted.gz /home/shanika/working/train/model/lex.e2f /home/shanika/working/train/model/phrase-table.half.e2f.gz --Inverse 1 
using gzip 
Started Wed Nov 27 00:47:30 2019
/home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/tmp.11471/extract.0.gz /home/shanika/working/train/model/lex.e2f /home/shanika/working/train/model/tmp.11471/phrase-table.half.0000000.gz --Inverse  2>> /dev/stderr 
/home/shanika/working/train/model/tmp.11471/run.0.sh/home/shanika/working/train/model/tmp.11471/run.1.sh/home/shanika/working/train/model/tmp.11471/run.2.sh/home/shanika/working/train/model/tmp.11471/run.3.shgunzip -c /home/shanika/working/train/model/tmp.11471/phrase-table.half.*.gz 2>> /dev/stderr| LC_ALL=C sort     -T /home/shanika/working/train/model/tmp.11471  | gzip -c > /home/shanika/working/train/model/phrase-table.half.e2f.gz  2>> /dev/stderr rm -rf /home/shanika/working/train/model/tmp.11471 
Finished Wed Nov 27 00:47:35 2019
(6.6) consolidating the two halves @ Wed Nov 27 00:47:35 +0530 2019
Executing: /home/shanika/mosesdecoder/scripts/../bin/consolidate /home/shanika/working/train/model/phrase-table.half.f2e.gz /home/shanika/working/train/model/phrase-table.half.e2f.gz /dev/stdout | gzip -c > /home/shanika/working/train/model/phrase-table.gz
Consolidate v2.0 written by Philipp Koehn
consolidating direct and indirect rule tables
..
Executing: rm -f /home/shanika/working/train/model/phrase-table.half.*
(7) learn reordering model @ Wed Nov 27 00:47:36 +0530 2019
(7.1) [no factors] learn reordering model @ Wed Nov 27 00:47:36 +0530 2019
(7.2) building tables @ Wed Nov 27 00:47:36 +0530 2019
Executing: /home/shanika/mosesdecoder/scripts/../bin/lexical-reordering-score /home/shanika/working/train/model/extract.o.sorted.gz 0.5 /home/shanika/working/train/model/reordering-table. --model "wbe msd wbe-msd-bidirectional-fe"
Lexical Reordering Scorer
scores lexical reordering models of several types (hierarchical, phrase-based and word-based-extraction
(8) learn generation model @ Wed Nov 27 00:47:38 +0530 2019
  no generation model requested, skipping step
(9) create moses.ini @ Wed Nov 27 00:47:38 +0530 2019
