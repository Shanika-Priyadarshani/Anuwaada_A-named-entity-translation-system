nohup: ignoring input
Using SCRIPTS_ROOTDIR: /home/shanika/mosesdecoder/scripts
Using single-thread GIZA
using gzip 
(1) preparing corpus @ Fri Nov 22 15:53:09 +0530 2019
Executing: mkdir -p /home/shanika/working/train/corpus
(1.0) selecting factors @ Fri Nov 22 15:53:09 +0530 2019
(1.1) running mkcls  @ Fri Nov 22 15:53:09 +0530 2019
/home/shanika/mosesdecoder/tools/mkcls -c50 -n2 -p/home/shanika/corpus/data.en-ta.clean.en -V/home/shanika/working/train/corpus/en.vcb.classes opt
Executing: /home/shanika/mosesdecoder/tools/mkcls -c50 -n2 -p/home/shanika/corpus/data.en-ta.clean.en -V/home/shanika/working/train/corpus/en.vcb.classes opt

***** 2 runs. (algorithm:TA)*****
;KategProblem:cats: 50   words: 873

start-costs: MEAN: 6.92001e+06 (6.91949e+06-6.92053e+06)  SIGMA:524.105   
  end-costs: MEAN: 6.8545e+06 (6.85448e+06-6.85453e+06)  SIGMA:27.4588   
   start-pp: MEAN: 10.4944 (10.4843-10.5044)  SIGMA:0.0100915   
     end-pp: MEAN: 9.30591 (9.30544-9.30638)  SIGMA:0.000468839   
 iterations: MEAN: 21353.5 (21229-21478)  SIGMA:124.5   
       time: MEAN: 0.374706 (0.37341-0.376002)  SIGMA:0.001296   
(1.1) running mkcls  @ Fri Nov 22 15:53:10 +0530 2019
/home/shanika/mosesdecoder/tools/mkcls -c50 -n2 -p/home/shanika/corpus/data.en-ta.clean.ta -V/home/shanika/working/train/corpus/ta.vcb.classes opt
Executing: /home/shanika/mosesdecoder/tools/mkcls -c50 -n2 -p/home/shanika/corpus/data.en-ta.clean.ta -V/home/shanika/working/train/corpus/ta.vcb.classes opt

***** 2 runs. (algorithm:TA)*****
;KategProblem:cats: 50   words: 970

start-costs: MEAN: 3.98835e+06 (3.98361e+06-3.99309e+06)  SIGMA:4741.85   
  end-costs: MEAN: 3.82352e+06 (3.8235e+06-3.82354e+06)  SIGMA:17.701   
   start-pp: MEAN: 42.0222 (41.4124-42.632)  SIGMA:0.6098   
     end-pp: MEAN: 25.3717 (25.3703-25.373)  SIGMA:0.00137447   
 iterations: MEAN: 27522 (27028-28016)  SIGMA:494   
       time: MEAN: 0.933615 (0.913863-0.953368)  SIGMA:0.0197525   
(1.2) creating vcb file /home/shanika/working/train/corpus/en.vcb @ Fri Nov 22 15:53:12 +0530 2019
(1.2) creating vcb file /home/shanika/working/train/corpus/ta.vcb @ Fri Nov 22 15:53:12 +0530 2019
(1.3) numberizing corpus /home/shanika/working/train/corpus/en-ta-int-train.snt @ Fri Nov 22 15:53:12 +0530 2019
(1.3) numberizing corpus /home/shanika/working/train/corpus/ta-en-int-train.snt @ Fri Nov 22 15:53:12 +0530 2019
(2) running giza @ Fri Nov 22 15:53:13 +0530 2019
(2.1a) running snt2cooc en-ta @ Fri Nov 22 15:53:13 +0530 2019

Executing: mkdir -p /home/shanika/working/train/giza.en-ta
Executing: /home/shanika/mosesdecoder/tools/snt2cooc.out /home/shanika/working/train/corpus/ta.vcb /home/shanika/working/train/corpus/en.vcb /home/shanika/working/train/corpus/en-ta-int-train.snt > /home/shanika/working/train/giza.en-ta/en-ta.cooc
/home/shanika/mosesdecoder/tools/snt2cooc.out /home/shanika/working/train/corpus/ta.vcb /home/shanika/working/train/corpus/en.vcb /home/shanika/working/train/corpus/en-ta-int-train.snt > /home/shanika/working/train/giza.en-ta/en-ta.cooc
line 1000
line 2000
line 3000
line 4000
line 5000
line 6000
line 7000
line 8000
line 9000
line 10000
line 11000
line 12000
line 13000
line 14000
line 15000
line 16000
line 17000
line 18000
line 19000
line 20000
line 21000
line 22000
line 23000
line 24000
line 25000
line 26000
line 27000
line 28000
line 29000
line 30000
line 31000
line 32000
line 33000
line 34000
line 35000
line 36000
line 37000
line 38000
line 39000
line 40000
END.
(2.1b) running giza en-ta @ Fri Nov 22 15:53:13 +0530 2019
/home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.en-ta/en-ta.cooc -c /home/shanika/working/train/corpus/en-ta-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.en-ta/en-ta -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/ta.vcb -t /home/shanika/working/train/corpus/en.vcb
Executing: /home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.en-ta/en-ta.cooc -c /home/shanika/working/train/corpus/en-ta-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.en-ta/en-ta -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/ta.vcb -t /home/shanika/working/train/corpus/en.vcb
/home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.en-ta/en-ta.cooc -c /home/shanika/working/train/corpus/en-ta-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.en-ta/en-ta -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/ta.vcb -t /home/shanika/working/train/corpus/en.vcb
Parameter 'coocurrencefile' changed from '' to '/home/shanika/working/train/giza.en-ta/en-ta.cooc'
Parameter 'c' changed from '' to '/home/shanika/working/train/corpus/en-ta-int-train.snt'
Parameter 'm3' changed from '5' to '3'
Parameter 'm4' changed from '5' to '3'
Parameter 'model1dumpfrequency' changed from '0' to '1'
Parameter 'model4smoothfactor' changed from '0.2' to '0.4'
Parameter 'nodumps' changed from '0' to '1'
Parameter 'nsmooth' changed from '64' to '4'
Parameter 'o' changed from '119-11-22.155313.shanika' to '/home/shanika/working/train/giza.en-ta/en-ta'
Parameter 'onlyaldumps' changed from '0' to '1'
Parameter 'p0' changed from '-1' to '0.999'
Parameter 's' changed from '' to '/home/shanika/working/train/corpus/ta.vcb'
Parameter 't' changed from '' to '/home/shanika/working/train/corpus/en.vcb'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 119-11-22.155313.shanika.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/shanika/working/train/giza.en-ta/en-ta  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/shanika/working/train/corpus/en-ta-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/shanika/working/train/corpus/ta.vcb  (source vocabulary file name)
t = /home/shanika/working/train/corpus/en.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 119-11-22.155313.shanika.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/shanika/working/train/giza.en-ta/en-ta  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/shanika/working/train/corpus/en-ta-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/shanika/working/train/corpus/ta.vcb  (source vocabulary file name)
t = /home/shanika/working/train/corpus/en.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Reading vocabulary file from:/home/shanika/working/train/corpus/ta.vcb
Reading vocabulary file from:/home/shanika/working/train/corpus/en.vcb
Source vocabulary list has 971 unique tokens 
Target vocabulary list has 874 unique tokens 
Calculating vocabulary frequencies from corpus /home/shanika/working/train/corpus/en-ta-int-train.snt
Reading more sentence pairs into memory ... 
WARNING: The following sentence pair has source/target sentence length ration more than
the maximum allowed limit for a source word fertility
 source length = 3 target length = 28 ratio 9.33333 ferility limit : 9
Shortening sentence 
Sent No: 20025 , No. Occurrences: 1
0 59 54 196 
17 2 12 4 13 23 19 2 19 2 57 7 6 10 8 2 14 2 3 2 14 2 18 22 9 5 11 5 
Corpus fits in memory, corpus has: 40000 sentence pairs.
 Train total # sentence pairs (weighted): 40000
Size of source portion of the training corpus: 286745 tokens
Size of the target portion of the training corpus: 505000 tokens 
In source portion of the training corpus, only 970 unique tokens appeared
In target portion of the training corpus, only 872 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 505000/(326745-40000)== 1.76115
There are 29849 29849 entries in table
==========================================================
Model1 Training Started at: Fri Nov 22 15:53:13 2019

-----------
Model1: Iteration 1
Model1: (1) TRAIN CROSS-ENTROPY 10.1926 PERPLEXITY 1170.25
Model1: (1) VITERBI TRAIN CROSS-ENTROPY 13.2777 PERPLEXITY 9930.7
Model 1 Iteration: 1 took: 0 seconds
-----------
Model1: Iteration 2
Model1: (2) TRAIN CROSS-ENTROPY 4.25324 PERPLEXITY 19.0701
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 6.57771 PERPLEXITY 95.5186
Model 1 Iteration: 2 took: 1 seconds
-----------
Model1: Iteration 3
Model1: (3) TRAIN CROSS-ENTROPY 4.08218 PERPLEXITY 16.9378
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 5.9421 PERPLEXITY 61.4825
Model 1 Iteration: 3 took: 0 seconds
-----------
Model1: Iteration 4
Model1: (4) TRAIN CROSS-ENTROPY 3.9748 PERPLEXITY 15.723
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 5.5529 PERPLEXITY 46.945
Model 1 Iteration: 4 took: 0 seconds
-----------
Model1: Iteration 5
Model1: (5) TRAIN CROSS-ENTROPY 3.91198 PERPLEXITY 15.053
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 5.30859 PERPLEXITY 39.6318
Model 1 Iteration: 5 took: 0 seconds
Entire Model1 Training took: 1 seconds
NOTE: I am doing iterations with the HMM model!
Read classes: #words: 970  #classes: 51
Read classes: #words: 873  #classes: 51

==========================================================
Hmm Training Started at: Fri Nov 22 15:53:14 2019

-----------
Hmm: Iteration 1
A/D table contains 15479 parameters.
Hmm: (1) TRAIN CROSS-ENTROPY 3.87623 PERPLEXITY 14.6846
Hmm: (1) VITERBI TRAIN CROSS-ENTROPY 5.15252 PERPLEXITY 35.5683

Hmm Iteration: 1 took: 2 seconds

-----------
Hmm: Iteration 2
A/D table contains 15479 parameters.
Hmm: (2) TRAIN CROSS-ENTROPY 3.59361 PERPLEXITY 12.0722
Hmm: (2) VITERBI TRAIN CROSS-ENTROPY 4.32568 PERPLEXITY 20.052

Hmm Iteration: 2 took: 2 seconds

-----------
Hmm: Iteration 3
A/D table contains 15479 parameters.
Hmm: (3) TRAIN CROSS-ENTROPY 3.29508 PERPLEXITY 9.81563
Hmm: (3) VITERBI TRAIN CROSS-ENTROPY 3.78693 PERPLEXITY 13.8032

Hmm Iteration: 3 took: 1 seconds

-----------
Hmm: Iteration 4
A/D table contains 15479 parameters.
Hmm: (4) TRAIN CROSS-ENTROPY 3.08924 PERPLEXITY 8.51045
Hmm: (4) VITERBI TRAIN CROSS-ENTROPY 3.45574 PERPLEXITY 10.9719

Hmm Iteration: 4 took: 2 seconds

-----------
Hmm: Iteration 5
A/D table contains 15479 parameters.
Hmm: (5) TRAIN CROSS-ENTROPY 2.98501 PERPLEXITY 7.91732
Hmm: (5) VITERBI TRAIN CROSS-ENTROPY 3.28852 PERPLEXITY 9.77112

Hmm Iteration: 5 took: 2 seconds

Entire Hmm Training took: 9 seconds
==========================================================
Read classes: #words: 970  #classes: 51
Read classes: #words: 873  #classes: 51
Read classes: #words: 970  #classes: 51
Read classes: #words: 873  #classes: 51

==========================================================
Starting H333444:  Viterbi Training
 H333444 Training Started at: Fri Nov 22 15:53:23 2019


---------------------
THTo3: Iteration 1
10000
20000
30000
40000
#centers(pre/hillclimbed/real): 1 1 1  #al: 176.12 #alsophisticatedcountcollection: 0 #hcsteps: 0
#peggingImprovements: 0
A/D table contains 15479 parameters.
A/D table contains 21232 parameters.
NTable contains 9710 parameter.
p0_count is 307029 and p1 is 98108.7; p0 is 0.999 p1: 0.001
THTo3: TRAIN CROSS-ENTROPY 2.65686 PERPLEXITY 6.30658
THTo3: (1) TRAIN VITERBI CROSS-ENTROPY 2.80628 PERPLEXITY 6.99479

THTo3 Viterbi Iteration : 1 took: 2 seconds

---------------------
Model3: Iteration 2
10000
20000
30000
40000
#centers(pre/hillclimbed/real): 1 1 1  #al: 177.405 #alsophisticatedcountcollection: 0 #hcsteps: 3.77088
#peggingImprovements: 0
A/D table contains 15479 parameters.
A/D table contains 21232 parameters.
NTable contains 9710 parameter.
p0_count is 453733 and p1 is 25633.3; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 4.3058 PERPLEXITY 19.7776
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 4.4384 PERPLEXITY 21.6816

Model3 Viterbi Iteration : 2 took: 2 seconds

---------------------
Model3: Iteration 3
10000
20000
30000
40000
#centers(pre/hillclimbed/real): 1 1 1  #al: 177.266 #alsophisticatedcountcollection: 0 #hcsteps: 3.9799
#peggingImprovements: 0
A/D table contains 15479 parameters.
A/D table contains 21232 parameters.
NTable contains 9710 parameter.
p0_count is 478409 and p1 is 13295.3; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 4.03219 PERPLEXITY 16.361
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 4.14331 PERPLEXITY 17.671

Model3 Viterbi Iteration : 3 took: 2 seconds

---------------------
T3To4: Iteration 4
10000
20000
30000
40000
#centers(pre/hillclimbed/real): 1 1 1  #al: 177.228 #alsophisticatedcountcollection: 60.3336 #hcsteps: 4.1357
#peggingImprovements: 0
D4 table contains 451066 parameters.
A/D table contains 15479 parameters.
A/D table contains 21232 parameters.
NTable contains 9710 parameter.
p0_count is 485417 and p1 is 9791.43; p0 is 0.999 p1: 0.001
T3To4: TRAIN CROSS-ENTROPY 3.97465 PERPLEXITY 15.7213
T3To4: (4) TRAIN VITERBI CROSS-ENTROPY 4.07421 PERPLEXITY 16.8446

T3To4 Viterbi Iteration : 4 took: 4 seconds

---------------------
Model4: Iteration 5
10000
20000
30000
40000
#centers(pre/hillclimbed/real): 1 1 1  #al: 177.01 #alsophisticatedcountcollection: 36.1305 #hcsteps: 3.84615
#peggingImprovements: 0
D4 table contains 451269 parameters.
A/D table contains 15479 parameters.
A/D table contains 21257 parameters.
NTable contains 9710 parameter.
p0_count is 484905 and p1 is 10047.5; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 3.6363 PERPLEXITY 12.4347
Model4: (5) TRAIN VITERBI CROSS-ENTROPY 3.69568 PERPLEXITY 12.9572

Model4 Viterbi Iteration : 5 took: 8 seconds

---------------------
Model4: Iteration 6
10000
20000
30000
40000
#centers(pre/hillclimbed/real): 1 1 1  #al: 176.928 #alsophisticatedcountcollection: 27.6166 #hcsteps: 3.85575
#peggingImprovements: 0
D4 table contains 451472 parameters.
A/D table contains 15479 parameters.
A/D table contains 21260 parameters.
NTable contains 9710 parameter.
p0_count is 488920 and p1 is 8039.75; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 3.39628 PERPLEXITY 10.5289
Model4: (6) TRAIN VITERBI CROSS-ENTROPY 3.43726 PERPLEXITY 10.8322

Model4 Viterbi Iteration : 6 took: 9 seconds
H333444 Training Finished at: Fri Nov 22 15:53:50 2019


Entire Viterbi H333444 Training took: 27 seconds
==========================================================

Entire Training took: 37 seconds
Program Finished at: Fri Nov 22 15:53:50 2019

==========================================================
Executing: rm -f /home/shanika/working/train/giza.en-ta/en-ta.A3.final.gz
Executing: gzip /home/shanika/working/train/giza.en-ta/en-ta.A3.final
(2.1a) running snt2cooc ta-en @ Fri Nov 22 15:53:50 +0530 2019

Executing: mkdir -p /home/shanika/working/train/giza.ta-en
Executing: /home/shanika/mosesdecoder/tools/snt2cooc.out /home/shanika/working/train/corpus/en.vcb /home/shanika/working/train/corpus/ta.vcb /home/shanika/working/train/corpus/ta-en-int-train.snt > /home/shanika/working/train/giza.ta-en/ta-en.cooc
/home/shanika/mosesdecoder/tools/snt2cooc.out /home/shanika/working/train/corpus/en.vcb /home/shanika/working/train/corpus/ta.vcb /home/shanika/working/train/corpus/ta-en-int-train.snt > /home/shanika/working/train/giza.ta-en/ta-en.cooc
line 1000
line 2000
line 3000
line 4000
line 5000
line 6000
line 7000
line 8000
line 9000
line 10000
line 11000
line 12000
line 13000
line 14000
line 15000
line 16000
line 17000
line 18000
line 19000
line 20000
line 21000
line 22000
line 23000
line 24000
line 25000
line 26000
line 27000
line 28000
line 29000
line 30000
line 31000
line 32000
line 33000
line 34000
line 35000
line 36000
line 37000
line 38000
line 39000
line 40000
END.
(2.1b) running giza ta-en @ Fri Nov 22 15:53:51 +0530 2019
/home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.ta-en/ta-en.cooc -c /home/shanika/working/train/corpus/ta-en-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.ta-en/ta-en -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/en.vcb -t /home/shanika/working/train/corpus/ta.vcb
Executing: /home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.ta-en/ta-en.cooc -c /home/shanika/working/train/corpus/ta-en-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.ta-en/ta-en -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/en.vcb -t /home/shanika/working/train/corpus/ta.vcb
/home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.ta-en/ta-en.cooc -c /home/shanika/working/train/corpus/ta-en-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.ta-en/ta-en -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/en.vcb -t /home/shanika/working/train/corpus/ta.vcb
Parameter 'coocurrencefile' changed from '' to '/home/shanika/working/train/giza.ta-en/ta-en.cooc'
Parameter 'c' changed from '' to '/home/shanika/working/train/corpus/ta-en-int-train.snt'
Parameter 'm3' changed from '5' to '3'
Parameter 'm4' changed from '5' to '3'
Parameter 'model1dumpfrequency' changed from '0' to '1'
Parameter 'model4smoothfactor' changed from '0.2' to '0.4'
Parameter 'nodumps' changed from '0' to '1'
Parameter 'nsmooth' changed from '64' to '4'
Parameter 'o' changed from '119-11-22.155351.shanika' to '/home/shanika/working/train/giza.ta-en/ta-en'
Parameter 'onlyaldumps' changed from '0' to '1'
Parameter 'p0' changed from '-1' to '0.999'
Parameter 's' changed from '' to '/home/shanika/working/train/corpus/en.vcb'
Parameter 't' changed from '' to '/home/shanika/working/train/corpus/ta.vcb'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 119-11-22.155351.shanika.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/shanika/working/train/giza.ta-en/ta-en  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/shanika/working/train/corpus/ta-en-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/shanika/working/train/corpus/en.vcb  (source vocabulary file name)
t = /home/shanika/working/train/corpus/ta.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 119-11-22.155351.shanika.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/shanika/working/train/giza.ta-en/ta-en  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/shanika/working/train/corpus/ta-en-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/shanika/working/train/corpus/en.vcb  (source vocabulary file name)
t = /home/shanika/working/train/corpus/ta.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Reading vocabulary file from:/home/shanika/working/train/corpus/en.vcb
Reading vocabulary file from:/home/shanika/working/train/corpus/ta.vcb
Source vocabulary list has 874 unique tokens 
Target vocabulary list has 971 unique tokens 
Calculating vocabulary frequencies from corpus /home/shanika/working/train/corpus/ta-en-int-train.snt
Reading more sentence pairs into memory ... 
Corpus fits in memory, corpus has: 40000 sentence pairs.
 Train total # sentence pairs (weighted): 40000
Size of source portion of the training corpus: 505025 tokens
Size of the target portion of the training corpus: 286745 tokens 
In source portion of the training corpus, only 873 unique tokens appeared
In target portion of the training corpus, only 969 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 286745/(545025-40000)== 0.567784
There are 29946 29946 entries in table
==========================================================
Model1 Training Started at: Fri Nov 22 15:53:51 2019

-----------
Model1: Iteration 1
Model1: (1) TRAIN CROSS-ENTROPY 10.7202 PERPLEXITY 1686.92
Model1: (1) VITERBI TRAIN CROSS-ENTROPY 14.5536 PERPLEXITY 24046.8
Model 1 Iteration: 1 took: 0 seconds
-----------
Model1: Iteration 2
Model1: (2) TRAIN CROSS-ENTROPY 6.86352 PERPLEXITY 116.446
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 9.63611 PERPLEXITY 795.715
Model 1 Iteration: 2 took: 0 seconds
-----------
Model1: Iteration 3
Model1: (3) TRAIN CROSS-ENTROPY 6.64043 PERPLEXITY 99.7631
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 8.79895 PERPLEXITY 445.398
Model 1 Iteration: 3 took: 1 seconds
-----------
Model1: Iteration 4
Model1: (4) TRAIN CROSS-ENTROPY 6.49195 PERPLEXITY 90.0059
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 8.28196 PERPLEXITY 311.257
Model 1 Iteration: 4 took: 0 seconds
-----------
Model1: Iteration 5
Model1: (5) TRAIN CROSS-ENTROPY 6.40303 PERPLEXITY 84.6263
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 7.97608 PERPLEXITY 251.79
Model 1 Iteration: 5 took: 0 seconds
Entire Model1 Training took: 1 seconds
NOTE: I am doing iterations with the HMM model!
Read classes: #words: 873  #classes: 51
Read classes: #words: 970  #classes: 51

==========================================================
Hmm Training Started at: Fri Nov 22 15:53:52 2019

-----------
Hmm: Iteration 1
A/D table contains 21240 parameters.
Hmm: (1) TRAIN CROSS-ENTROPY 6.35304 PERPLEXITY 81.7438
Hmm: (1) VITERBI TRAIN CROSS-ENTROPY 7.79789 PERPLEXITY 222.535

Hmm Iteration: 1 took: 3 seconds

-----------
Hmm: Iteration 2
A/D table contains 21240 parameters.
Hmm: (2) TRAIN CROSS-ENTROPY 5.76094 PERPLEXITY 54.2269
Hmm: (2) VITERBI TRAIN CROSS-ENTROPY 6.49255 PERPLEXITY 90.0433

Hmm Iteration: 2 took: 3 seconds

-----------
Hmm: Iteration 3
A/D table contains 21240 parameters.
Hmm: (3) TRAIN CROSS-ENTROPY 5.14802 PERPLEXITY 35.4576
Hmm: (3) VITERBI TRAIN CROSS-ENTROPY 5.57065 PERPLEXITY 47.5263

Hmm Iteration: 3 took: 3 seconds

-----------
Hmm: Iteration 4
A/D table contains 21240 parameters.
Hmm: (4) TRAIN CROSS-ENTROPY 4.87381 PERPLEXITY 29.32
Hmm: (4) VITERBI TRAIN CROSS-ENTROPY 5.18659 PERPLEXITY 36.4183

Hmm Iteration: 4 took: 3 seconds

-----------
Hmm: Iteration 5
A/D table contains 21240 parameters.
Hmm: (5) TRAIN CROSS-ENTROPY 4.77641 PERPLEXITY 27.4058
Hmm: (5) VITERBI TRAIN CROSS-ENTROPY 5.04289 PERPLEXITY 32.9656

Hmm Iteration: 5 took: 3 seconds

Entire Hmm Training took: 15 seconds
==========================================================
Read classes: #words: 873  #classes: 51
Read classes: #words: 970  #classes: 51
Read classes: #words: 873  #classes: 51
Read classes: #words: 970  #classes: 51

==========================================================
Starting H333444:  Viterbi Training
 H333444 Training Started at: Fri Nov 22 15:54:07 2019


---------------------
THTo3: Iteration 1
10000
20000
30000
40000
#centers(pre/hillclimbed/real): 1 1 1  #al: 125.919 #alsophisticatedcountcollection: 0 #hcsteps: 0
#peggingImprovements: 0
A/D table contains 21240 parameters.
A/D table contains 14862 parameters.
NTable contains 8740 parameter.
p0_count is 245439 and p1 is 20349.7; p0 is 0.999 p1: 0.001
THTo3: TRAIN CROSS-ENTROPY 4.04297 PERPLEXITY 16.4837
THTo3: (1) TRAIN VITERBI CROSS-ENTROPY 4.19915 PERPLEXITY 18.3683

THTo3 Viterbi Iteration : 1 took: 1 seconds

---------------------
Model3: Iteration 2
10000
20000
30000
40000
#centers(pre/hillclimbed/real): 1 1 1  #al: 126.307 #alsophisticatedcountcollection: 0 #hcsteps: 2.26895
#peggingImprovements: 0
A/D table contains 21240 parameters.
A/D table contains 14862 parameters.
NTable contains 8740 parameter.
p0_count is 275953 and p1 is 5396.09; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 5.52828 PERPLEXITY 46.1508
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 5.67428 PERPLEXITY 51.0654

Model3 Viterbi Iteration : 2 took: 3 seconds

---------------------
Model3: Iteration 3
10000
20000
30000
40000
#centers(pre/hillclimbed/real): 1 1 1  #al: 126.324 #alsophisticatedcountcollection: 0 #hcsteps: 2.25095
#peggingImprovements: 0
A/D table contains 21240 parameters.
A/D table contains 14862 parameters.
NTable contains 8740 parameter.
p0_count is 278137 and p1 is 4304.16; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 5.35691 PERPLEXITY 40.9816
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 5.47075 PERPLEXITY 44.3466

Model3 Viterbi Iteration : 3 took: 2 seconds

---------------------
T3To4: Iteration 4
10000
20000
30000
40000
#centers(pre/hillclimbed/real): 1 1 1  #al: 126.327 #alsophisticatedcountcollection: 37.5744 #hcsteps: 2.19813
#peggingImprovements: 0
D4 table contains 480095 parameters.
A/D table contains 21240 parameters.
A/D table contains 14862 parameters.
NTable contains 8740 parameter.
p0_count is 279067 and p1 is 3839.15; p0 is 0.999 p1: 0.001
T3To4: TRAIN CROSS-ENTROPY 5.28192 PERPLEXITY 38.906
T3To4: (4) TRAIN VITERBI CROSS-ENTROPY 5.38019 PERPLEXITY 41.6484

T3To4 Viterbi Iteration : 4 took: 3 seconds

---------------------
Model4: Iteration 5
10000
20000
30000
40000
#centers(pre/hillclimbed/real): 1 1 1  #al: 126.247 #alsophisticatedcountcollection: 24.8259 #hcsteps: 1.93145
#peggingImprovements: 0
D4 table contains 480298 parameters.
A/D table contains 21240 parameters.
A/D table contains 14896 parameters.
NTable contains 8740 parameter.
p0_count is 281312 and p1 is 2716.62; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 5.10146 PERPLEXITY 34.3314
Model4: (5) TRAIN VITERBI CROSS-ENTROPY 5.17881 PERPLEXITY 36.2223

Model4 Viterbi Iteration : 5 took: 5 seconds

---------------------
Model4: Iteration 6
10000
20000
30000
40000
#centers(pre/hillclimbed/real): 1 1 1  #al: 126.213 #alsophisticatedcountcollection: 21.9019 #hcsteps: 1.86922
#peggingImprovements: 0
D4 table contains 480501 parameters.
A/D table contains 21240 parameters.
A/D table contains 14904 parameters.
NTable contains 8740 parameter.
p0_count is 282130 and p1 is 2307.66; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 4.9694 PERPLEXITY 31.3284
Model4: (6) TRAIN VITERBI CROSS-ENTROPY 5.03783 PERPLEXITY 32.8502

Model4 Viterbi Iteration : 6 took: 5 seconds
H333444 Training Finished at: Fri Nov 22 15:54:26 2019


Entire Viterbi H333444 Training took: 19 seconds
==========================================================

Entire Training took: 35 seconds
Program Finished at: Fri Nov 22 15:54:26 2019

==========================================================
Executing: rm -f /home/shanika/working/train/giza.ta-en/ta-en.A3.final.gz
Executing: gzip /home/shanika/working/train/giza.ta-en/ta-en.A3.final
(3) generate word alignment @ Fri Nov 22 15:54:26 +0530 2019
Combining forward and inverted alignment from files:
  /home/shanika/working/train/giza.en-ta/en-ta.A3.final.{bz2,gz}
  /home/shanika/working/train/giza.ta-en/ta-en.A3.final.{bz2,gz}
Executing: mkdir -p /home/shanika/working/train/model
Executing: /home/shanika/mosesdecoder/scripts/training/giza2bal.pl -d "gzip -cd /home/shanika/working/train/giza.ta-en/ta-en.A3.final.gz" -i "gzip -cd /home/shanika/working/train/giza.en-ta/en-ta.A3.final.gz" |/home/shanika/mosesdecoder/scripts/../bin/symal -alignment="grow" -diagonal="yes" -final="yes" -both="yes" > /home/shanika/working/train/model/aligned.grow-diag-final-and
symal: computing grow alignment: diagonal (1) final (1)both-uncovered (1)
Sentence mismatch error! Line #20025
skip=<0> counts=<40000>
(4) generate lexical translation table 0-0 @ Fri Nov 22 15:54:29 +0530 2019
(/home/shanika/corpus/data.en-ta.clean.en,/home/shanika/corpus/data.en-ta.clean.ta,/home/shanika/working/train/model/lex)
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
Saved: /home/shanika/working/train/model/lex.f2e and /home/shanika/working/train/model/lex.e2f
FILE: /home/shanika/corpus/data.en-ta.clean.ta
FILE: /home/shanika/corpus/data.en-ta.clean.en
FILE: /home/shanika/working/train/model/aligned.grow-diag-final-and
(5) extract phrases @ Fri Nov 22 15:54:30 +0530 2019
/home/shanika/mosesdecoder/scripts/generic/extract-parallel.perl 4 split "sort    " /home/shanika/mosesdecoder/scripts/../bin/extract /home/shanika/corpus/data.en-ta.clean.ta /home/shanika/corpus/data.en-ta.clean.en /home/shanika/working/train/model/aligned.grow-diag-final-and /home/shanika/working/train/model/extract 7 orientation --model wbe-msd --GZOutput 
Executing: /home/shanika/mosesdecoder/scripts/generic/extract-parallel.perl 4 split "sort    " /home/shanika/mosesdecoder/scripts/../bin/extract /home/shanika/corpus/data.en-ta.clean.ta /home/shanika/corpus/data.en-ta.clean.en /home/shanika/working/train/model/aligned.grow-diag-final-and /home/shanika/working/train/model/extract 7 orientation --model wbe-msd --GZOutput 
MAX 7 1 0
Started Fri Nov 22 15:54:30 2019
using gzip 
isBSDSplit=0 
Executing: mkdir -p /home/shanika/working/train/model/tmp.6245; ls -l /home/shanika/working/train/model/tmp.6245 
total=40000 line-per-split=10001 
split -d -l 10001 -a 7 /home/shanika/corpus/data.en-ta.clean.en /home/shanika/working/train/model/tmp.6245/source.split -d -l 10001 -a 7 /home/shanika/working/train/model/aligned.grow-diag-final-and /home/shanika/working/train/model/tmp.6245/align.split -d -l 10001 -a 7 /home/shanika/corpus/data.en-ta.clean.ta /home/shanika/working/train/model/tmp.6245/target.merging extract / extract.inv
gunzip -c /home/shanika/working/train/model/tmp.6245/extract.0000000.gz /home/shanika/working/train/model/tmp.6245/extract.0000001.gz /home/shanika/working/train/model/tmp.6245/extract.0000002.gz /home/shanika/working/train/model/tmp.6245/extract.0000003.gz  | LC_ALL=C sort     -T /home/shanika/working/train/model/tmp.6245 2>> /dev/stderr | gzip -c > /home/shanika/working/train/model/extract.sorted.gz 2>> /dev/stderr 
gunzip -c /home/shanika/working/train/model/tmp.6245/extract.0000000.o.gz /home/shanika/working/train/model/tmp.6245/extract.0000001.o.gz /home/shanika/working/train/model/tmp.6245/extract.0000002.o.gz /home/shanika/working/train/model/tmp.6245/extract.0000003.o.gz  | LC_ALL=C sort     -T /home/shanika/working/train/model/tmp.6245 2>> /dev/stderr | gzip -c > /home/shanika/working/train/model/extract.o.sorted.gz 2>> /dev/stderr 
gunzip -c /home/shanika/working/train/model/tmp.6245/extract.0000000.inv.gz /home/shanika/working/train/model/tmp.6245/extract.0000001.inv.gz /home/shanika/working/train/model/tmp.6245/extract.0000002.inv.gz /home/shanika/working/train/model/tmp.6245/extract.0000003.inv.gz  | LC_ALL=C sort     -T /home/shanika/working/train/model/tmp.6245 2>> /dev/stderr | gzip -c > /home/shanika/working/train/model/extract.inv.sorted.gz 2>> /dev/stderr 
Finished Fri Nov 22 15:54:35 2019
(6) score phrases @ Fri Nov 22 15:54:35 +0530 2019
(6.1)  creating table half /home/shanika/working/train/model/phrase-table.half.f2e @ Fri Nov 22 15:54:35 +0530 2019
/home/shanika/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/extract.sorted.gz /home/shanika/working/train/model/lex.f2e /home/shanika/working/train/model/phrase-table.half.f2e.gz  0 
Executing: /home/shanika/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/extract.sorted.gz /home/shanika/working/train/model/lex.f2e /home/shanika/working/train/model/phrase-table.half.f2e.gz  0 
using gzip 
Started Fri Nov 22 15:54:35 2019
/home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/tmp.6297/extract.0.gz /home/shanika/working/train/model/lex.f2e /home/shanika/working/train/model/tmp.6297/phrase-table.half.0000000.gz  2>> /dev/stderr 
/home/shanika/working/train/model/tmp.6297/run.0.sh/home/shanika/working/train/model/tmp.6297/run.1.sh/home/shanika/working/train/model/tmp.6297/run.2.sh/home/shanika/working/train/model/tmp.6297/run.3.shmv /home/shanika/working/train/model/tmp.6297/phrase-table.half.0000000.gz /home/shanika/working/train/model/phrase-table.half.f2e.gzrm -rf /home/shanika/working/train/model/tmp.6297 
Finished Fri Nov 22 15:54:39 2019
(6.3)  creating table half /home/shanika/working/train/model/phrase-table.half.e2f @ Fri Nov 22 15:54:39 +0530 2019
/home/shanika/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/extract.inv.sorted.gz /home/shanika/working/train/model/lex.e2f /home/shanika/working/train/model/phrase-table.half.e2f.gz --Inverse 1 
Executing: /home/shanika/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/extract.inv.sorted.gz /home/shanika/working/train/model/lex.e2f /home/shanika/working/train/model/phrase-table.half.e2f.gz --Inverse 1 
using gzip 
Started Fri Nov 22 15:54:39 2019
/home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/tmp.6324/extract.0.gz /home/shanika/working/train/model/lex.e2f /home/shanika/working/train/model/tmp.6324/phrase-table.half.0000000.gz --Inverse  2>> /dev/stderr 
/home/shanika/working/train/model/tmp.6324/run.0.sh/home/shanika/working/train/model/tmp.6324/run.1.sh/home/shanika/working/train/model/tmp.6324/run.2.sh/home/shanika/working/train/model/tmp.6324/run.3.shgunzip -c /home/shanika/working/train/model/tmp.6324/phrase-table.half.*.gz 2>> /dev/stderr| LC_ALL=C sort     -T /home/shanika/working/train/model/tmp.6324  | gzip -c > /home/shanika/working/train/model/phrase-table.half.e2f.gz  2>> /dev/stderr rm -rf /home/shanika/working/train/model/tmp.6324 
Finished Fri Nov 22 15:54:42 2019
(6.6) consolidating the two halves @ Fri Nov 22 15:54:42 +0530 2019
Executing: /home/shanika/mosesdecoder/scripts/../bin/consolidate /home/shanika/working/train/model/phrase-table.half.f2e.gz /home/shanika/working/train/model/phrase-table.half.e2f.gz /dev/stdout | gzip -c > /home/shanika/working/train/model/phrase-table.gz
Consolidate v2.0 written by Philipp Koehn
consolidating direct and indirect rule tables
..
Executing: rm -f /home/shanika/working/train/model/phrase-table.half.*
(7) learn reordering model @ Fri Nov 22 15:54:43 +0530 2019
(7.1) [no factors] learn reordering model @ Fri Nov 22 15:54:43 +0530 2019
(7.2) building tables @ Fri Nov 22 15:54:43 +0530 2019
Executing: /home/shanika/mosesdecoder/scripts/../bin/lexical-reordering-score /home/shanika/working/train/model/extract.o.sorted.gz 0.5 /home/shanika/working/train/model/reordering-table. --model "wbe msd wbe-msd-bidirectional-fe"
Lexical Reordering Scorer
scores lexical reordering models of several types (hierarchical, phrase-based and word-based-extraction
(8) learn generation model @ Fri Nov 22 15:54:45 +0530 2019
  no generation model requested, skipping step
(9) create moses.ini @ Fri Nov 22 15:54:45 +0530 2019
