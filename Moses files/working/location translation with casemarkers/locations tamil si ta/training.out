nohup: ignoring input
Using SCRIPTS_ROOTDIR: /home/shanika/mosesdecoder/scripts
Using single-thread GIZA
using gzip 
(1) preparing corpus @ Sun Nov 24 10:38:32 +0530 2019
Executing: mkdir -p /home/shanika/working/train/corpus
(1.0) selecting factors @ Sun Nov 24 10:38:32 +0530 2019
(1.1) running mkcls  @ Sun Nov 24 10:38:32 +0530 2019
/home/shanika/mosesdecoder/tools/mkcls -c50 -n2 -p/home/shanika/corpus/data.si-ta.clean.si -V/home/shanika/working/train/corpus/si.vcb.classes opt
Executing: /home/shanika/mosesdecoder/tools/mkcls -c50 -n2 -p/home/shanika/corpus/data.si-ta.clean.si -V/home/shanika/working/train/corpus/si.vcb.classes opt

***** 2 runs. (algorithm:TA)*****
;KategProblem:cats: 50   words: 366

start-costs: MEAN: 614641 (614590-614693)  SIGMA:51.6099   
  end-costs: MEAN: 588760 (588642-588877)  SIGMA:117.692   
   start-pp: MEAN: 43.66 (43.6216-43.6984)  SIGMA:0.0384009   
     end-pp: MEAN: 28.0884 (28.0321-28.1448)  SIGMA:0.0563377   
 iterations: MEAN: 11247 (11226-11268)  SIGMA:21   
       time: MEAN: 0.521894 (0.520218-0.52357)  SIGMA:0.001676   
(1.1) running mkcls  @ Sun Nov 24 10:38:33 +0530 2019
/home/shanika/mosesdecoder/tools/mkcls -c50 -n2 -p/home/shanika/corpus/data.si-ta.clean.ta -V/home/shanika/working/train/corpus/ta.vcb.classes opt
Executing: /home/shanika/mosesdecoder/tools/mkcls -c50 -n2 -p/home/shanika/corpus/data.si-ta.clean.ta -V/home/shanika/working/train/corpus/ta.vcb.classes opt

***** 2 runs. (algorithm:TA)*****
;KategProblem:cats: 50   words: 291

start-costs: MEAN: 604738 (602398-607077)  SIGMA:2339.45   
  end-costs: MEAN: 575322 (575249-575395)  SIGMA:73.1267   
   start-pp: MEAN: 39.5994 (38.0097-41.1891)  SIGMA:1.58965   
     end-pp: MEAN: 23.8787 (23.8487-23.9087)  SIGMA:0.0299792   
 iterations: MEAN: 9014 (8838-9190)  SIGMA:176   
       time: MEAN: 0.474323 (0.471102-0.477544)  SIGMA:0.003221   
(1.2) creating vcb file /home/shanika/working/train/corpus/si.vcb @ Sun Nov 24 10:38:34 +0530 2019
(1.2) creating vcb file /home/shanika/working/train/corpus/ta.vcb @ Sun Nov 24 10:38:34 +0530 2019
(1.3) numberizing corpus /home/shanika/working/train/corpus/si-ta-int-train.snt @ Sun Nov 24 10:38:34 +0530 2019
(1.3) numberizing corpus /home/shanika/working/train/corpus/ta-si-int-train.snt @ Sun Nov 24 10:38:34 +0530 2019
(2) running giza @ Sun Nov 24 10:38:34 +0530 2019
(2.1a) running snt2cooc si-ta @ Sun Nov 24 10:38:34 +0530 2019

Executing: mkdir -p /home/shanika/working/train/giza.si-ta
Executing: /home/shanika/mosesdecoder/tools/snt2cooc.out /home/shanika/working/train/corpus/ta.vcb /home/shanika/working/train/corpus/si.vcb /home/shanika/working/train/corpus/si-ta-int-train.snt > /home/shanika/working/train/giza.si-ta/si-ta.cooc
/home/shanika/mosesdecoder/tools/snt2cooc.out /home/shanika/working/train/corpus/ta.vcb /home/shanika/working/train/corpus/si.vcb /home/shanika/working/train/corpus/si-ta-int-train.snt > /home/shanika/working/train/giza.si-ta/si-ta.cooc
line 1000
line 2000
line 3000
line 4000
line 5000
line 6000
line 7000
END.
(2.1b) running giza si-ta @ Sun Nov 24 10:38:34 +0530 2019
/home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.si-ta/si-ta.cooc -c /home/shanika/working/train/corpus/si-ta-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.si-ta/si-ta -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/ta.vcb -t /home/shanika/working/train/corpus/si.vcb
Executing: /home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.si-ta/si-ta.cooc -c /home/shanika/working/train/corpus/si-ta-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.si-ta/si-ta -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/ta.vcb -t /home/shanika/working/train/corpus/si.vcb
/home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.si-ta/si-ta.cooc -c /home/shanika/working/train/corpus/si-ta-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.si-ta/si-ta -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/ta.vcb -t /home/shanika/working/train/corpus/si.vcb
Parameter 'coocurrencefile' changed from '' to '/home/shanika/working/train/giza.si-ta/si-ta.cooc'
Parameter 'c' changed from '' to '/home/shanika/working/train/corpus/si-ta-int-train.snt'
Parameter 'm3' changed from '5' to '3'
Parameter 'm4' changed from '5' to '3'
Parameter 'model1dumpfrequency' changed from '0' to '1'
Parameter 'model4smoothfactor' changed from '0.2' to '0.4'
Parameter 'nodumps' changed from '0' to '1'
Parameter 'nsmooth' changed from '64' to '4'
Parameter 'o' changed from '119-11-24.103834.shanika' to '/home/shanika/working/train/giza.si-ta/si-ta'
Parameter 'onlyaldumps' changed from '0' to '1'
Parameter 'p0' changed from '-1' to '0.999'
Parameter 's' changed from '' to '/home/shanika/working/train/corpus/ta.vcb'
Parameter 't' changed from '' to '/home/shanika/working/train/corpus/si.vcb'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 119-11-24.103834.shanika.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/shanika/working/train/giza.si-ta/si-ta  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/shanika/working/train/corpus/si-ta-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/shanika/working/train/corpus/ta.vcb  (source vocabulary file name)
t = /home/shanika/working/train/corpus/si.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 119-11-24.103834.shanika.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/shanika/working/train/giza.si-ta/si-ta  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/shanika/working/train/corpus/si-ta-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/shanika/working/train/corpus/ta.vcb  (source vocabulary file name)
t = /home/shanika/working/train/corpus/si.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Reading vocabulary file from:/home/shanika/working/train/corpus/ta.vcb
Reading vocabulary file from:/home/shanika/working/train/corpus/si.vcb
Source vocabulary list has 292 unique tokens 
Target vocabulary list has 367 unique tokens 
Calculating vocabulary frequencies from corpus /home/shanika/working/train/corpus/si-ta-int-train.snt
Reading more sentence pairs into memory ... 
Corpus fits in memory, corpus has: 7500 sentence pairs.
 Train total # sentence pairs (weighted): 7500
Size of source portion of the training corpus: 50746 tokens
Size of the target portion of the training corpus: 51178 tokens 
In source portion of the training corpus, only 291 unique tokens appeared
In target portion of the training corpus, only 365 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 51178/(58246-7500)== 1.00851
There are 23304 23304 entries in table
==========================================================
Model1 Training Started at: Sun Nov 24 10:38:34 2019

-----------
Model1: Iteration 1
Model1: (1) TRAIN CROSS-ENTROPY 8.9409 PERPLEXITY 491.449
Model1: (1) VITERBI TRAIN CROSS-ENTROPY 11.9549 PERPLEXITY 3969.88
Model 1 Iteration: 1 took: 1 seconds
-----------
Model1: Iteration 2
Model1: (2) TRAIN CROSS-ENTROPY 5.78482 PERPLEXITY 55.1322
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 7.09827 PERPLEXITY 137.022
Model 1 Iteration: 2 took: 0 seconds
-----------
Model1: Iteration 3
Model1: (3) TRAIN CROSS-ENTROPY 4.93922 PERPLEXITY 30.6798
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 5.62867 PERPLEXITY 49.4763
Model 1 Iteration: 3 took: 0 seconds
-----------
Model1: Iteration 4
Model1: (4) TRAIN CROSS-ENTROPY 4.64282 PERPLEXITY 24.9821
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 5.1012 PERPLEXITY 34.3252
Model 1 Iteration: 4 took: 0 seconds
-----------
Model1: Iteration 5
Model1: (5) TRAIN CROSS-ENTROPY 4.55171 PERPLEXITY 23.4531
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 4.91015 PERPLEXITY 30.0679
Model 1 Iteration: 5 took: 0 seconds
Entire Model1 Training took: 1 seconds
NOTE: I am doing iterations with the HMM model!
Read classes: #words: 291  #classes: 51
Read classes: #words: 366  #classes: 51

==========================================================
Hmm Training Started at: Sun Nov 24 10:38:35 2019

-----------
Hmm: Iteration 1
A/D table contains 5462 parameters.
Hmm: (1) TRAIN CROSS-ENTROPY 4.51663 PERPLEXITY 22.8897
Hmm: (1) VITERBI TRAIN CROSS-ENTROPY 4.82995 PERPLEXITY 28.442

Hmm Iteration: 1 took: 0 seconds

-----------
Hmm: Iteration 2
A/D table contains 5462 parameters.
Hmm: (2) TRAIN CROSS-ENTROPY 2.76583 PERPLEXITY 6.8014
Hmm: (2) VITERBI TRAIN CROSS-ENTROPY 2.86851 PERPLEXITY 7.30312

Hmm Iteration: 2 took: 0 seconds

-----------
Hmm: Iteration 3
A/D table contains 5462 parameters.
Hmm: (3) TRAIN CROSS-ENTROPY 2.42447 PERPLEXITY 5.3683
Hmm: (3) VITERBI TRAIN CROSS-ENTROPY 2.48618 PERPLEXITY 5.60292

Hmm Iteration: 3 took: 0 seconds

-----------
Hmm: Iteration 4
A/D table contains 5462 parameters.
Hmm: (4) TRAIN CROSS-ENTROPY 2.34054 PERPLEXITY 5.06491
Hmm: (4) VITERBI TRAIN CROSS-ENTROPY 2.39018 PERPLEXITY 5.24223

Hmm Iteration: 4 took: 0 seconds

-----------
Hmm: Iteration 5
A/D table contains 5462 parameters.
Hmm: (5) TRAIN CROSS-ENTROPY 2.30796 PERPLEXITY 4.95181
Hmm: (5) VITERBI TRAIN CROSS-ENTROPY 2.34914 PERPLEXITY 5.09522

Hmm Iteration: 5 took: 1 seconds

Entire Hmm Training took: 1 seconds
==========================================================
Read classes: #words: 291  #classes: 51
Read classes: #words: 366  #classes: 51
Read classes: #words: 291  #classes: 51
Read classes: #words: 366  #classes: 51

==========================================================
Starting H333444:  Viterbi Training
 H333444 Training Started at: Sun Nov 24 10:38:36 2019


---------------------
THTo3: Iteration 1
#centers(pre/hillclimbed/real): 1 1 1  #al: 73.6968 #alsophisticatedcountcollection: 0 #hcsteps: 0
#peggingImprovements: 0
A/D table contains 5462 parameters.
A/D table contains 5917 parameters.
NTable contains 2920 parameter.
p0_count is 46564.3 and p1 is 2180.64; p0 is 0.999 p1: 0.001
THTo3: TRAIN CROSS-ENTROPY 1.88112 PERPLEXITY 3.6836
THTo3: (1) TRAIN VITERBI CROSS-ENTROPY 1.91589 PERPLEXITY 3.77348

THTo3 Viterbi Iteration : 1 took: 0 seconds

---------------------
Model3: Iteration 2
#centers(pre/hillclimbed/real): 1 1 1  #al: 73.7724 #alsophisticatedcountcollection: 0 #hcsteps: 1.24973
#peggingImprovements: 0
A/D table contains 5462 parameters.
A/D table contains 5891 parameters.
NTable contains 2920 parameter.
p0_count is 48624.5 and p1 is 1276.75; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 2.83466 PERPLEXITY 7.13373
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 2.85796 PERPLEXITY 7.24991

Model3 Viterbi Iteration : 2 took: 0 seconds

---------------------
Model3: Iteration 3
#centers(pre/hillclimbed/real): 1 1 1  #al: 73.7825 #alsophisticatedcountcollection: 0 #hcsteps: 1.26813
#peggingImprovements: 0
A/D table contains 5462 parameters.
A/D table contains 5891 parameters.
NTable contains 2920 parameter.
p0_count is 49145.9 and p1 is 1016.04; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 2.75015 PERPLEXITY 6.72788
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 2.76867 PERPLEXITY 6.81481

Model3 Viterbi Iteration : 3 took: 0 seconds

---------------------
T3To4: Iteration 4
#centers(pre/hillclimbed/real): 1 1 1  #al: 73.7892 #alsophisticatedcountcollection: 5.93 #hcsteps: 1.25693
#peggingImprovements: 0
D4 table contains 506891 parameters.
A/D table contains 5462 parameters.
A/D table contains 5915 parameters.
NTable contains 2920 parameter.
p0_count is 49291.8 and p1 is 943.089; p0 is 0.999 p1: 0.001
T3To4: TRAIN CROSS-ENTROPY 2.7117 PERPLEXITY 6.55094
T3To4: (4) TRAIN VITERBI CROSS-ENTROPY 2.72877 PERPLEXITY 6.62889

T3To4 Viterbi Iteration : 4 took: 0 seconds

---------------------
Model4: Iteration 5
#centers(pre/hillclimbed/real): 1 1 1  #al: 73.7645 #alsophisticatedcountcollection: 4.3628 #hcsteps: 1.23
#peggingImprovements: 0
D4 table contains 506891 parameters.
A/D table contains 5462 parameters.
A/D table contains 6037 parameters.
NTable contains 2920 parameter.
p0_count is 49404.8 and p1 is 886.612; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 2.72407 PERPLEXITY 6.60734
Model4: (5) TRAIN VITERBI CROSS-ENTROPY 2.73674 PERPLEXITY 6.66561

Model4 Viterbi Iteration : 5 took: 1 seconds

---------------------
Model4: Iteration 6
#centers(pre/hillclimbed/real): 1 1 1  #al: 73.7612 #alsophisticatedcountcollection: 4.0456 #hcsteps: 1.22213
#peggingImprovements: 0
D4 table contains 506891 parameters.
A/D table contains 5462 parameters.
A/D table contains 6037 parameters.
NTable contains 2920 parameter.
p0_count is 49470.7 and p1 is 853.666; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 2.69283 PERPLEXITY 6.46583
Model4: (6) TRAIN VITERBI CROSS-ENTROPY 2.70363 PERPLEXITY 6.51439

Model4 Viterbi Iteration : 6 took: 0 seconds
H333444 Training Finished at: Sun Nov 24 10:38:37 2019


Entire Viterbi H333444 Training took: 1 seconds
==========================================================

Entire Training took: 3 seconds
Program Finished at: Sun Nov 24 10:38:37 2019

==========================================================
Executing: rm -f /home/shanika/working/train/giza.si-ta/si-ta.A3.final.gz
Executing: gzip /home/shanika/working/train/giza.si-ta/si-ta.A3.final
(2.1a) running snt2cooc ta-si @ Sun Nov 24 10:38:37 +0530 2019

Executing: mkdir -p /home/shanika/working/train/giza.ta-si
Executing: /home/shanika/mosesdecoder/tools/snt2cooc.out /home/shanika/working/train/corpus/si.vcb /home/shanika/working/train/corpus/ta.vcb /home/shanika/working/train/corpus/ta-si-int-train.snt > /home/shanika/working/train/giza.ta-si/ta-si.cooc
/home/shanika/mosesdecoder/tools/snt2cooc.out /home/shanika/working/train/corpus/si.vcb /home/shanika/working/train/corpus/ta.vcb /home/shanika/working/train/corpus/ta-si-int-train.snt > /home/shanika/working/train/giza.ta-si/ta-si.cooc
line 1000
line 2000
line 3000
line 4000
line 5000
line 6000
line 7000
END.
(2.1b) running giza ta-si @ Sun Nov 24 10:38:38 +0530 2019
/home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.ta-si/ta-si.cooc -c /home/shanika/working/train/corpus/ta-si-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.ta-si/ta-si -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/si.vcb -t /home/shanika/working/train/corpus/ta.vcb
Executing: /home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.ta-si/ta-si.cooc -c /home/shanika/working/train/corpus/ta-si-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.ta-si/ta-si -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/si.vcb -t /home/shanika/working/train/corpus/ta.vcb
/home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.ta-si/ta-si.cooc -c /home/shanika/working/train/corpus/ta-si-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.ta-si/ta-si -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/si.vcb -t /home/shanika/working/train/corpus/ta.vcb
Parameter 'coocurrencefile' changed from '' to '/home/shanika/working/train/giza.ta-si/ta-si.cooc'
Parameter 'c' changed from '' to '/home/shanika/working/train/corpus/ta-si-int-train.snt'
Parameter 'm3' changed from '5' to '3'
Parameter 'm4' changed from '5' to '3'
Parameter 'model1dumpfrequency' changed from '0' to '1'
Parameter 'model4smoothfactor' changed from '0.2' to '0.4'
Parameter 'nodumps' changed from '0' to '1'
Parameter 'nsmooth' changed from '64' to '4'
Parameter 'o' changed from '119-11-24.103838.shanika' to '/home/shanika/working/train/giza.ta-si/ta-si'
Parameter 'onlyaldumps' changed from '0' to '1'
Parameter 'p0' changed from '-1' to '0.999'
Parameter 's' changed from '' to '/home/shanika/working/train/corpus/si.vcb'
Parameter 't' changed from '' to '/home/shanika/working/train/corpus/ta.vcb'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 119-11-24.103838.shanika.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/shanika/working/train/giza.ta-si/ta-si  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/shanika/working/train/corpus/ta-si-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/shanika/working/train/corpus/si.vcb  (source vocabulary file name)
t = /home/shanika/working/train/corpus/ta.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 119-11-24.103838.shanika.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/shanika/working/train/giza.ta-si/ta-si  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/shanika/working/train/corpus/ta-si-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/shanika/working/train/corpus/si.vcb  (source vocabulary file name)
t = /home/shanika/working/train/corpus/ta.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Reading vocabulary file from:/home/shanika/working/train/corpus/si.vcb
Reading vocabulary file from:/home/shanika/working/train/corpus/ta.vcb
Source vocabulary list has 367 unique tokens 
Target vocabulary list has 292 unique tokens 
Calculating vocabulary frequencies from corpus /home/shanika/working/train/corpus/ta-si-int-train.snt
Reading more sentence pairs into memory ... 
Corpus fits in memory, corpus has: 7500 sentence pairs.
 Train total # sentence pairs (weighted): 7500
Size of source portion of the training corpus: 51178 tokens
Size of the target portion of the training corpus: 50746 tokens 
In source portion of the training corpus, only 366 unique tokens appeared
In target portion of the training corpus, only 290 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 50746/(58678-7500)== 0.991559
There are 23229 23229 entries in table
==========================================================
Model1 Training Started at: Sun Nov 24 10:38:38 2019

-----------
Model1: Iteration 1
Model1: (1) TRAIN CROSS-ENTROPY 8.61537 PERPLEXITY 392.179
Model1: (1) VITERBI TRAIN CROSS-ENTROPY 11.6412 PERPLEXITY 3194.02
Model 1 Iteration: 1 took: 0 seconds
-----------
Model1: Iteration 2
Model1: (2) TRAIN CROSS-ENTROPY 5.76153 PERPLEXITY 54.2491
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 7.07048 PERPLEXITY 134.408
Model 1 Iteration: 2 took: 0 seconds
-----------
Model1: Iteration 3
Model1: (3) TRAIN CROSS-ENTROPY 4.91677 PERPLEXITY 30.2061
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 5.60931 PERPLEXITY 48.8168
Model 1 Iteration: 3 took: 0 seconds
-----------
Model1: Iteration 4
Model1: (4) TRAIN CROSS-ENTROPY 4.62486 PERPLEXITY 24.6731
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 5.09379 PERPLEXITY 34.1495
Model 1 Iteration: 4 took: 0 seconds
-----------
Model1: Iteration 5
Model1: (5) TRAIN CROSS-ENTROPY 4.53737 PERPLEXITY 23.2213
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 4.91205 PERPLEXITY 30.1075
Model 1 Iteration: 5 took: 0 seconds
Entire Model1 Training took: 0 seconds
NOTE: I am doing iterations with the HMM model!
Read classes: #words: 366  #classes: 51
Read classes: #words: 291  #classes: 51

==========================================================
Hmm Training Started at: Sun Nov 24 10:38:38 2019

-----------
Hmm: Iteration 1
A/D table contains 6180 parameters.
Hmm: (1) TRAIN CROSS-ENTROPY 4.50441 PERPLEXITY 22.6967
Hmm: (1) VITERBI TRAIN CROSS-ENTROPY 4.83524 PERPLEXITY 28.5466

Hmm Iteration: 1 took: 0 seconds

-----------
Hmm: Iteration 2
A/D table contains 6180 parameters.
Hmm: (2) TRAIN CROSS-ENTROPY 2.76304 PERPLEXITY 6.78823
Hmm: (2) VITERBI TRAIN CROSS-ENTROPY 2.868 PERPLEXITY 7.30055

Hmm Iteration: 2 took: 0 seconds

-----------
Hmm: Iteration 3
A/D table contains 6180 parameters.
Hmm: (3) TRAIN CROSS-ENTROPY 2.3978 PERPLEXITY 5.26998
Hmm: (3) VITERBI TRAIN CROSS-ENTROPY 2.45741 PERPLEXITY 5.49229

Hmm Iteration: 3 took: 0 seconds

-----------
Hmm: Iteration 4
A/D table contains 6180 parameters.
Hmm: (4) TRAIN CROSS-ENTROPY 2.30335 PERPLEXITY 4.93602
Hmm: (4) VITERBI TRAIN CROSS-ENTROPY 2.3486 PERPLEXITY 5.09328

Hmm Iteration: 4 took: 0 seconds

-----------
Hmm: Iteration 5
A/D table contains 6180 parameters.
Hmm: (5) TRAIN CROSS-ENTROPY 2.27208 PERPLEXITY 4.83019
Hmm: (5) VITERBI TRAIN CROSS-ENTROPY 2.31173 PERPLEXITY 4.9648

Hmm Iteration: 5 took: 1 seconds

Entire Hmm Training took: 1 seconds
==========================================================
Read classes: #words: 366  #classes: 51
Read classes: #words: 291  #classes: 51
Read classes: #words: 366  #classes: 51
Read classes: #words: 291  #classes: 51

==========================================================
Starting H333444:  Viterbi Training
 H333444 Training Started at: Sun Nov 24 10:38:39 2019


---------------------
THTo3: Iteration 1
#centers(pre/hillclimbed/real): 1 1 1  #al: 73.4236 #alsophisticatedcountcollection: 0 #hcsteps: 0
#peggingImprovements: 0
A/D table contains 6180 parameters.
A/D table contains 5281 parameters.
NTable contains 3670 parameter.
p0_count is 45524.3 and p1 is 2521.61; p0 is 0.999 p1: 0.001
THTo3: TRAIN CROSS-ENTROPY 1.84529 PERPLEXITY 3.59324
THTo3: (1) TRAIN VITERBI CROSS-ENTROPY 1.87597 PERPLEXITY 3.67047

THTo3 Viterbi Iteration : 1 took: 0 seconds

---------------------
Model3: Iteration 2
#centers(pre/hillclimbed/real): 1 1 1  #al: 73.5372 #alsophisticatedcountcollection: 0 #hcsteps: 1.21667
#peggingImprovements: 0
A/D table contains 6180 parameters.
A/D table contains 5281 parameters.
NTable contains 3670 parameter.
p0_count is 47265.2 and p1 is 1740.38; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 2.79627 PERPLEXITY 6.9464
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 2.81785 PERPLEXITY 7.05109

Model3 Viterbi Iteration : 2 took: 0 seconds

---------------------
Model3: Iteration 3
#centers(pre/hillclimbed/real): 1 1 1  #al: 73.5553 #alsophisticatedcountcollection: 0 #hcsteps: 1.2224
#peggingImprovements: 0
A/D table contains 6180 parameters.
A/D table contains 5281 parameters.
NTable contains 3670 parameter.
p0_count is 47752.3 and p1 is 1496.86; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 2.71347 PERPLEXITY 6.55899
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 2.73238 PERPLEXITY 6.64551

Model3 Viterbi Iteration : 3 took: 0 seconds

---------------------
T3To4: Iteration 4
#centers(pre/hillclimbed/real): 1 1 1  #al: 73.5595 #alsophisticatedcountcollection: 5.64093 #hcsteps: 1.22413
#peggingImprovements: 0
D4 table contains 508515 parameters.
A/D table contains 6180 parameters.
A/D table contains 5281 parameters.
NTable contains 3670 parameter.
p0_count is 47987.7 and p1 is 1379.17; p0 is 0.999 p1: 0.001
T3To4: TRAIN CROSS-ENTROPY 2.67563 PERPLEXITY 6.38918
T3To4: (4) TRAIN VITERBI CROSS-ENTROPY 2.69266 PERPLEXITY 6.46503

T3To4 Viterbi Iteration : 4 took: 1 seconds

---------------------
Model4: Iteration 5
#centers(pre/hillclimbed/real): 1 1 1  #al: 73.5444 #alsophisticatedcountcollection: 4.3796 #hcsteps: 1.21253
#peggingImprovements: 0
D4 table contains 508515 parameters.
A/D table contains 6180 parameters.
A/D table contains 5363 parameters.
NTable contains 3670 parameter.
p0_count is 48235.7 and p1 is 1255.16; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 2.75679 PERPLEXITY 6.75889
Model4: (5) TRAIN VITERBI CROSS-ENTROPY 2.77033 PERPLEXITY 6.82262

Model4 Viterbi Iteration : 5 took: 0 seconds

---------------------
Model4: Iteration 6
#centers(pre/hillclimbed/real): 1 1 1  #al: 73.5336 #alsophisticatedcountcollection: 4.11227 #hcsteps: 1.20667
#peggingImprovements: 0
D4 table contains 508515 parameters.
A/D table contains 6180 parameters.
A/D table contains 5363 parameters.
NTable contains 3670 parameter.
p0_count is 48374.8 and p1 is 1185.58; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 2.71911 PERPLEXITY 6.58468
Model4: (6) TRAIN VITERBI CROSS-ENTROPY 2.73043 PERPLEXITY 6.63655

Model4 Viterbi Iteration : 6 took: 0 seconds
H333444 Training Finished at: Sun Nov 24 10:38:40 2019


Entire Viterbi H333444 Training took: 1 seconds
==========================================================

Entire Training took: 2 seconds
Program Finished at: Sun Nov 24 10:38:40 2019

==========================================================
Executing: rm -f /home/shanika/working/train/giza.ta-si/ta-si.A3.final.gz
Executing: gzip /home/shanika/working/train/giza.ta-si/ta-si.A3.final
(3) generate word alignment @ Sun Nov 24 10:38:41 +0530 2019
Combining forward and inverted alignment from files:
  /home/shanika/working/train/giza.si-ta/si-ta.A3.final.{bz2,gz}
  /home/shanika/working/train/giza.ta-si/ta-si.A3.final.{bz2,gz}
Executing: mkdir -p /home/shanika/working/train/model
Executing: /home/shanika/mosesdecoder/scripts/training/giza2bal.pl -d "gzip -cd /home/shanika/working/train/giza.ta-si/ta-si.A3.final.gz" -i "gzip -cd /home/shanika/working/train/giza.si-ta/si-ta.A3.final.gz" |/home/shanika/mosesdecoder/scripts/../bin/symal -alignment="grow" -diagonal="yes" -final="yes" -both="yes" > /home/shanika/working/train/model/aligned.grow-diag-final-and
symal: computing grow alignment: diagonal (1) final (1)both-uncovered (1)
skip=<0> counts=<7500>
(4) generate lexical translation table 0-0 @ Sun Nov 24 10:38:41 +0530 2019
(/home/shanika/corpus/data.si-ta.clean.si,/home/shanika/corpus/data.si-ta.clean.ta,/home/shanika/working/train/model/lex)
!!!!!!!!
Saved: /home/shanika/working/train/model/lex.f2e and /home/shanika/working/train/model/lex.e2f
FILE: /home/shanika/corpus/data.si-ta.clean.ta
FILE: /home/shanika/corpus/data.si-ta.clean.si
FILE: /home/shanika/working/train/model/aligned.grow-diag-final-and
(5) extract phrases @ Sun Nov 24 10:38:41 +0530 2019
/home/shanika/mosesdecoder/scripts/generic/extract-parallel.perl 4 split "sort    " /home/shanika/mosesdecoder/scripts/../bin/extract /home/shanika/corpus/data.si-ta.clean.ta /home/shanika/corpus/data.si-ta.clean.si /home/shanika/working/train/model/aligned.grow-diag-final-and /home/shanika/working/train/model/extract 7 orientation --model wbe-msd --GZOutput 
Executing: /home/shanika/mosesdecoder/scripts/generic/extract-parallel.perl 4 split "sort    " /home/shanika/mosesdecoder/scripts/../bin/extract /home/shanika/corpus/data.si-ta.clean.ta /home/shanika/corpus/data.si-ta.clean.si /home/shanika/working/train/model/aligned.grow-diag-final-and /home/shanika/working/train/model/extract 7 orientation --model wbe-msd --GZOutput 
MAX 7 1 0
Started Sun Nov 24 10:38:41 2019
using gzip 
isBSDSplit=0 
Executing: mkdir -p /home/shanika/working/train/model/tmp.3529; ls -l /home/shanika/working/train/model/tmp.3529 
total=7500 line-per-split=1876 
split -d -l 1876 -a 7 /home/shanika/corpus/data.si-ta.clean.ta /home/shanika/working/train/model/tmp.3529/target.split -d -l 1876 -a 7 /home/shanika/corpus/data.si-ta.clean.si /home/shanika/working/train/model/tmp.3529/source.split -d -l 1876 -a 7 /home/shanika/working/train/model/aligned.grow-diag-final-and /home/shanika/working/train/model/tmp.3529/align.merging extract / extract.inv
gunzip -c /home/shanika/working/train/model/tmp.3529/extract.0000000.gz /home/shanika/working/train/model/tmp.3529/extract.0000001.gz /home/shanika/working/train/model/tmp.3529/extract.0000002.gz /home/shanika/working/train/model/tmp.3529/extract.0000003.gz  | LC_ALL=C sort     -T /home/shanika/working/train/model/tmp.3529 2>> /dev/stderr | gzip -c > /home/shanika/working/train/model/extract.sorted.gz 2>> /dev/stderr 
gunzip -c /home/shanika/working/train/model/tmp.3529/extract.0000000.inv.gz /home/shanika/working/train/model/tmp.3529/extract.0000001.inv.gz /home/shanika/working/train/model/tmp.3529/extract.0000002.inv.gz /home/shanika/working/train/model/tmp.3529/extract.0000003.inv.gz  | LC_ALL=C sort     -T /home/shanika/working/train/model/tmp.3529 2>> /dev/stderr | gzip -c > /home/shanika/working/train/model/extract.inv.sorted.gz 2>> /dev/stderr 
gunzip -c /home/shanika/working/train/model/tmp.3529/extract.0000000.o.gz /home/shanika/working/train/model/tmp.3529/extract.0000001.o.gz /home/shanika/working/train/model/tmp.3529/extract.0000002.o.gz /home/shanika/working/train/model/tmp.3529/extract.0000003.o.gz  | LC_ALL=C sort     -T /home/shanika/working/train/model/tmp.3529 2>> /dev/stderr | gzip -c > /home/shanika/working/train/model/extract.o.sorted.gz 2>> /dev/stderr 
Finished Sun Nov 24 10:38:43 2019
(6) score phrases @ Sun Nov 24 10:38:43 +0530 2019
(6.1)  creating table half /home/shanika/working/train/model/phrase-table.half.f2e @ Sun Nov 24 10:38:43 +0530 2019
/home/shanika/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/extract.sorted.gz /home/shanika/working/train/model/lex.f2e /home/shanika/working/train/model/phrase-table.half.f2e.gz  0 
Executing: /home/shanika/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/extract.sorted.gz /home/shanika/working/train/model/lex.f2e /home/shanika/working/train/model/phrase-table.half.f2e.gz  0 
using gzip 
Started Sun Nov 24 10:38:43 2019
/home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/tmp.3578/extract.0.gz /home/shanika/working/train/model/lex.f2e /home/shanika/working/train/model/tmp.3578/phrase-table.half.0000000.gz  2>> /dev/stderr 
/home/shanika/working/train/model/tmp.3578/run.0.sh/home/shanika/working/train/model/tmp.3578/run.1.sh/home/shanika/working/train/model/tmp.3578/run.2.sh/home/shanika/working/train/model/tmp.3578/run.3.shmv /home/shanika/working/train/model/tmp.3578/phrase-table.half.0000000.gz /home/shanika/working/train/model/phrase-table.half.f2e.gzrm -rf /home/shanika/working/train/model/tmp.3578 
Finished Sun Nov 24 10:38:44 2019
(6.3)  creating table half /home/shanika/working/train/model/phrase-table.half.e2f @ Sun Nov 24 10:38:44 +0530 2019
/home/shanika/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/extract.inv.sorted.gz /home/shanika/working/train/model/lex.e2f /home/shanika/working/train/model/phrase-table.half.e2f.gz --Inverse 1 
Executing: /home/shanika/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/extract.inv.sorted.gz /home/shanika/working/train/model/lex.e2f /home/shanika/working/train/model/phrase-table.half.e2f.gz --Inverse 1 
using gzip 
Started Sun Nov 24 10:38:44 2019
/home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/tmp.3605/extract.0.gz /home/shanika/working/train/model/lex.e2f /home/shanika/working/train/model/tmp.3605/phrase-table.half.0000000.gz --Inverse  2>> /dev/stderr 
/home/shanika/working/train/model/tmp.3605/run.0.sh/home/shanika/working/train/model/tmp.3605/run.1.sh/home/shanika/working/train/model/tmp.3605/run.2.sh/home/shanika/working/train/model/tmp.3605/run.3.shgunzip -c /home/shanika/working/train/model/tmp.3605/phrase-table.half.*.gz 2>> /dev/stderr| LC_ALL=C sort     -T /home/shanika/working/train/model/tmp.3605  | gzip -c > /home/shanika/working/train/model/phrase-table.half.e2f.gz  2>> /dev/stderr rm -rf /home/shanika/working/train/model/tmp.3605 
Finished Sun Nov 24 10:38:46 2019
(6.6) consolidating the two halves @ Sun Nov 24 10:38:46 +0530 2019
Executing: /home/shanika/mosesdecoder/scripts/../bin/consolidate /home/shanika/working/train/model/phrase-table.half.f2e.gz /home/shanika/working/train/model/phrase-table.half.e2f.gz /dev/stdout | gzip -c > /home/shanika/working/train/model/phrase-table.gz
Consolidate v2.0 written by Philipp Koehn
consolidating direct and indirect rule tables

Executing: rm -f /home/shanika/working/train/model/phrase-table.half.*
(7) learn reordering model @ Sun Nov 24 10:38:47 +0530 2019
(7.1) [no factors] learn reordering model @ Sun Nov 24 10:38:47 +0530 2019
(7.2) building tables @ Sun Nov 24 10:38:47 +0530 2019
Executing: /home/shanika/mosesdecoder/scripts/../bin/lexical-reordering-score /home/shanika/working/train/model/extract.o.sorted.gz 0.5 /home/shanika/working/train/model/reordering-table. --model "wbe msd wbe-msd-bidirectional-fe"
Lexical Reordering Scorer
scores lexical reordering models of several types (hierarchical, phrase-based and word-based-extraction
(8) learn generation model @ Sun Nov 24 10:38:47 +0530 2019
  no generation model requested, skipping step
(9) create moses.ini @ Sun Nov 24 10:38:47 +0530 2019
