nohup: ignoring input
Using SCRIPTS_ROOTDIR: /home/shanika/mosesdecoder/scripts
Using single-thread GIZA
using gzip 
(1) preparing corpus @ Tue Nov 26 00:07:22 +0530 2019
Executing: mkdir -p /home/shanika/working/train/corpus
(1.0) selecting factors @ Tue Nov 26 00:07:22 +0530 2019
(1.1) running mkcls  @ Tue Nov 26 00:07:22 +0530 2019
/home/shanika/mosesdecoder/tools/mkcls -c50 -n2 -p/home/shanika/corpus/data.ta-si.clean.ta -V/home/shanika/working/train/corpus/ta.vcb.classes opt
Executing: /home/shanika/mosesdecoder/tools/mkcls -c50 -n2 -p/home/shanika/corpus/data.ta-si.clean.ta -V/home/shanika/working/train/corpus/ta.vcb.classes opt

***** 2 runs. (algorithm:TA)*****
;KategProblem:cats: 50   words: 602

start-costs: MEAN: 5.76293e+07 (5.75321e+07-5.77266e+07)  SIGMA:97233.4   
  end-costs: MEAN: 5.53131e+07 (5.53079e+07-5.53182e+07)  SIGMA:5135.25   
   start-pp: MEAN: 36.1032 (35.2167-36.9897)  SIGMA:0.886467   
     end-pp: MEAN: 20.1066 (20.0805-20.1327)  SIGMA:0.0260788   
 iterations: MEAN: 17603.5 (17200-18007)  SIGMA:403.5   
       time: MEAN: 1.2415 (1.21943-1.26356)  SIGMA:0.022062   
(1.1) running mkcls  @ Tue Nov 26 00:07:28 +0530 2019
/home/shanika/mosesdecoder/tools/mkcls -c50 -n2 -p/home/shanika/corpus/data.ta-si.clean.si -V/home/shanika/working/train/corpus/si.vcb.classes opt
Executing: /home/shanika/mosesdecoder/tools/mkcls -c50 -n2 -p/home/shanika/corpus/data.ta-si.clean.si -V/home/shanika/working/train/corpus/si.vcb.classes opt

***** 2 runs. (algorithm:TA)*****
;KategProblem:cats: 50   words: 727

start-costs: MEAN: 5.3039e+07 (5.29014e+07-5.31766e+07)  SIGMA:137589   
  end-costs: MEAN: 5.11888e+07 (5.11871e+07-5.11905e+07)  SIGMA:1731.24   
   start-pp: MEAN: 38.8237 (37.3645-40.2829)  SIGMA:1.45921   
     end-pp: MEAN: 23.3983 (23.3872-23.4094)  SIGMA:0.0110708   
 iterations: MEAN: 20720.5 (20178-21263)  SIGMA:542.5   
       time: MEAN: 1.32132 (1.29191-1.35073)  SIGMA:0.0294135   
(1.2) creating vcb file /home/shanika/working/train/corpus/ta.vcb @ Tue Nov 26 00:07:32 +0530 2019
(1.2) creating vcb file /home/shanika/working/train/corpus/si.vcb @ Tue Nov 26 00:07:32 +0530 2019
(1.3) numberizing corpus /home/shanika/working/train/corpus/ta-si-int-train.snt @ Tue Nov 26 00:07:33 +0530 2019
(1.3) numberizing corpus /home/shanika/working/train/corpus/si-ta-int-train.snt @ Tue Nov 26 00:07:36 +0530 2019
(2) running giza @ Tue Nov 26 00:07:39 +0530 2019
(2.1a) running snt2cooc ta-si @ Tue Nov 26 00:07:39 +0530 2019

Executing: mkdir -p /home/shanika/working/train/giza.ta-si
Executing: /home/shanika/mosesdecoder/tools/snt2cooc.out /home/shanika/working/train/corpus/si.vcb /home/shanika/working/train/corpus/ta.vcb /home/shanika/working/train/corpus/ta-si-int-train.snt > /home/shanika/working/train/giza.ta-si/ta-si.cooc
/home/shanika/mosesdecoder/tools/snt2cooc.out /home/shanika/working/train/corpus/si.vcb /home/shanika/working/train/corpus/ta.vcb /home/shanika/working/train/corpus/ta-si-int-train.snt > /home/shanika/working/train/giza.ta-si/ta-si.cooc
line 1000
line 2000
line 3000
line 4000
line 5000
line 6000
line 7000
line 8000
line 9000
line 10000
line 11000
line 12000
line 13000
line 14000
line 15000
line 16000
line 17000
line 18000
line 19000
line 20000
line 21000
line 22000
line 23000
line 24000
line 25000
line 26000
line 27000
line 28000
line 29000
line 30000
line 31000
line 32000
line 33000
line 34000
line 35000
line 36000
line 37000
line 38000
line 39000
line 40000
line 41000
line 42000
line 43000
line 44000
line 45000
line 46000
line 47000
line 48000
line 49000
line 50000
line 51000
line 52000
line 53000
line 54000
line 55000
line 56000
line 57000
line 58000
line 59000
line 60000
line 61000
line 62000
line 63000
line 64000
line 65000
line 66000
line 67000
line 68000
line 69000
line 70000
line 71000
line 72000
line 73000
line 74000
line 75000
line 76000
line 77000
line 78000
line 79000
line 80000
line 81000
line 82000
line 83000
line 84000
line 85000
line 86000
line 87000
line 88000
line 89000
line 90000
line 91000
line 92000
line 93000
line 94000
line 95000
line 96000
line 97000
line 98000
line 99000
line 100000
line 101000
line 102000
line 103000
line 104000
line 105000
line 106000
line 107000
line 108000
line 109000
line 110000
line 111000
line 112000
line 113000
line 114000
line 115000
line 116000
line 117000
line 118000
line 119000
line 120000
line 121000
line 122000
line 123000
line 124000
line 125000
line 126000
line 127000
line 128000
line 129000
line 130000
line 131000
line 132000
line 133000
line 134000
line 135000
line 136000
line 137000
line 138000
line 139000
line 140000
line 141000
line 142000
line 143000
line 144000
line 145000
line 146000
line 147000
line 148000
line 149000
line 150000
line 151000
line 152000
line 153000
line 154000
line 155000
line 156000
line 157000
line 158000
line 159000
line 160000
line 161000
line 162000
line 163000
line 164000
line 165000
line 166000
line 167000
line 168000
line 169000
line 170000
line 171000
line 172000
line 173000
line 174000
line 175000
line 176000
line 177000
line 178000
line 179000
line 180000
line 181000
line 182000
line 183000
line 184000
line 185000
line 186000
line 187000
line 188000
line 189000
line 190000
line 191000
line 192000
line 193000
line 194000
line 195000
line 196000
line 197000
line 198000
line 199000
line 200000
line 201000
line 202000
line 203000
line 204000
line 205000
line 206000
line 207000
line 208000
line 209000
line 210000
line 211000
line 212000
line 213000
line 214000
line 215000
line 216000
line 217000
line 218000
line 219000
line 220000
line 221000
line 222000
line 223000
line 224000
line 225000
line 226000
line 227000
line 228000
line 229000
line 230000
line 231000
line 232000
line 233000
line 234000
line 235000
line 236000
line 237000
line 238000
line 239000
line 240000
line 241000
line 242000
line 243000
line 244000
line 245000
line 246000
line 247000
line 248000
line 249000
line 250000
line 251000
line 252000
line 253000
line 254000
line 255000
line 256000
line 257000
line 258000
line 259000
line 260000
line 261000
line 262000
line 263000
line 264000
line 265000
line 266000
line 267000
line 268000
line 269000
line 270000
line 271000
line 272000
line 273000
line 274000
line 275000
line 276000
line 277000
line 278000
line 279000
line 280000
line 281000
line 282000
line 283000
line 284000
line 285000
line 286000
line 287000
line 288000
line 289000
line 290000
line 291000
line 292000
line 293000
line 294000
line 295000
line 296000
line 297000
line 298000
line 299000
line 300000
line 301000
line 302000
line 303000
line 304000
line 305000
line 306000
line 307000
line 308000
line 309000
line 310000
line 311000
line 312000
line 313000
line 314000
line 315000
line 316000
line 317000
line 318000
line 319000
line 320000
line 321000
line 322000
line 323000
line 324000
line 325000
line 326000
line 327000
line 328000
line 329000
line 330000
line 331000
line 332000
line 333000
line 334000
line 335000
line 336000
line 337000
line 338000
line 339000
line 340000
line 341000
line 342000
line 343000
line 344000
line 345000
line 346000
line 347000
line 348000
line 349000
line 350000
line 351000
line 352000
line 353000
line 354000
line 355000
line 356000
line 357000
line 358000
line 359000
line 360000
line 361000
line 362000
line 363000
line 364000
line 365000
line 366000
line 367000
line 368000
line 369000
line 370000
line 371000
line 372000
line 373000
line 374000
line 375000
line 376000
line 377000
line 378000
line 379000
line 380000
line 381000
line 382000
line 383000
line 384000
line 385000
line 386000
line 387000
line 388000
line 389000
line 390000
line 391000
line 392000
line 393000
line 394000
line 395000
line 396000
line 397000
line 398000
line 399000
line 400000
END.
(2.1b) running giza ta-si @ Tue Nov 26 00:07:41 +0530 2019
/home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.ta-si/ta-si.cooc -c /home/shanika/working/train/corpus/ta-si-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.ta-si/ta-si -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/si.vcb -t /home/shanika/working/train/corpus/ta.vcb
Executing: /home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.ta-si/ta-si.cooc -c /home/shanika/working/train/corpus/ta-si-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.ta-si/ta-si -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/si.vcb -t /home/shanika/working/train/corpus/ta.vcb
/home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.ta-si/ta-si.cooc -c /home/shanika/working/train/corpus/ta-si-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.ta-si/ta-si -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/si.vcb -t /home/shanika/working/train/corpus/ta.vcb
Parameter 'coocurrencefile' changed from '' to '/home/shanika/working/train/giza.ta-si/ta-si.cooc'
Parameter 'c' changed from '' to '/home/shanika/working/train/corpus/ta-si-int-train.snt'
Parameter 'm3' changed from '5' to '3'
Parameter 'm4' changed from '5' to '3'
Parameter 'model1dumpfrequency' changed from '0' to '1'
Parameter 'model4smoothfactor' changed from '0.2' to '0.4'
Parameter 'nodumps' changed from '0' to '1'
Parameter 'nsmooth' changed from '64' to '4'
Parameter 'o' changed from '119-11-26.000741.shanika' to '/home/shanika/working/train/giza.ta-si/ta-si'
Parameter 'onlyaldumps' changed from '0' to '1'
Parameter 'p0' changed from '-1' to '0.999'
Parameter 's' changed from '' to '/home/shanika/working/train/corpus/si.vcb'
Parameter 't' changed from '' to '/home/shanika/working/train/corpus/ta.vcb'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 119-11-26.000741.shanika.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/shanika/working/train/giza.ta-si/ta-si  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/shanika/working/train/corpus/ta-si-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/shanika/working/train/corpus/si.vcb  (source vocabulary file name)
t = /home/shanika/working/train/corpus/ta.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 119-11-26.000741.shanika.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/shanika/working/train/giza.ta-si/ta-si  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/shanika/working/train/corpus/ta-si-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/shanika/working/train/corpus/si.vcb  (source vocabulary file name)
t = /home/shanika/working/train/corpus/ta.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Reading vocabulary file from:/home/shanika/working/train/corpus/si.vcb
Reading vocabulary file from:/home/shanika/working/train/corpus/ta.vcb
Source vocabulary list has 728 unique tokens 
Target vocabulary list has 603 unique tokens 
Calculating vocabulary frequencies from corpus /home/shanika/working/train/corpus/ta-si-int-train.snt
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:200000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:300000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:400000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
 Train total # sentence pairs (weighted): 400001
Size of source portion of the training corpus: 3.25899e+06 tokens
Size of the target portion of the training corpus: 3.55924e+06 tokens 
In source portion of the training corpus, only 727 unique tokens appeared
In target portion of the training corpus, only 601 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 3.55924e+06/(3.65899e+06-400001)== 1.09213
There are 64861 64861 entries in table
==========================================================
Model1 Training Started at: Tue Nov 26 00:07:42 2019

-----------
Model1: Iteration 1
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:200000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:300000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:400000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (1) TRAIN CROSS-ENTROPY 9.58518 PERPLEXITY 768.116
Model1: (1) VITERBI TRAIN CROSS-ENTROPY 12.8226 PERPLEXITY 7243.97
Model 1 Iteration: 1 took: 3 seconds
-----------
Model1: Iteration 2
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:200000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:300000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:400000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (2) TRAIN CROSS-ENTROPY 5.8819 PERPLEXITY 58.9695
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 7.59063 PERPLEXITY 192.756
Model 1 Iteration: 2 took: 3 seconds
-----------
Model1: Iteration 3
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:200000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:300000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:400000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (3) TRAIN CROSS-ENTROPY 5.2133 PERPLEXITY 37.0988
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 6.276 PERPLEXITY 77.4935
Model 1 Iteration: 3 took: 2 seconds
-----------
Model1: Iteration 4
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:200000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:300000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:400000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (4) TRAIN CROSS-ENTROPY 4.95654 PERPLEXITY 31.0504
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 5.7433 PERPLEXITY 53.5679
Model 1 Iteration: 4 took: 3 seconds
-----------
Model1: Iteration 5
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:200000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:300000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:400000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (5) TRAIN CROSS-ENTROPY 4.86602 PERPLEXITY 29.162
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 5.52001 PERPLEXITY 45.887
Model 1 Iteration: 5 took: 2 seconds
Entire Model1 Training took: 13 seconds
NOTE: I am doing iterations with the HMM model!
Read classes: #words: 727  #classes: 51
Read classes: #words: 602  #classes: 51

==========================================================
Hmm Training Started at: Tue Nov 26 00:07:56 2019

-----------
Hmm: Iteration 1
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:200000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:300000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:400000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 11641 parameters.
Hmm: (1) TRAIN CROSS-ENTROPY 4.82901 PERPLEXITY 28.4234
Hmm: (1) VITERBI TRAIN CROSS-ENTROPY 5.41565 PERPLEXITY 42.6849

Hmm Iteration: 1 took: 15 seconds

-----------
Hmm: Iteration 2
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:200000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:300000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:400000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 11641 parameters.
Hmm: (2) TRAIN CROSS-ENTROPY 3.45091 PERPLEXITY 10.9353
Hmm: (2) VITERBI TRAIN CROSS-ENTROPY 3.70789 PERPLEXITY 13.0673

Hmm Iteration: 2 took: 16 seconds

-----------
Hmm: Iteration 3
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:200000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:300000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:400000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 11641 parameters.
Hmm: (3) TRAIN CROSS-ENTROPY 2.97744 PERPLEXITY 7.87587
Hmm: (3) VITERBI TRAIN CROSS-ENTROPY 3.13765 PERPLEXITY 8.80092

Hmm Iteration: 3 took: 16 seconds

-----------
Hmm: Iteration 4
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:200000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:300000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:400000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 11641 parameters.
Hmm: (4) TRAIN CROSS-ENTROPY 2.85233 PERPLEXITY 7.22168
Hmm: (4) VITERBI TRAIN CROSS-ENTROPY 2.98249 PERPLEXITY 7.90349

Hmm Iteration: 4 took: 16 seconds

-----------
Hmm: Iteration 5
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:200000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:300000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:400000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 11641 parameters.
Hmm: (5) TRAIN CROSS-ENTROPY 2.81081 PERPLEXITY 7.0168
Hmm: (5) VITERBI TRAIN CROSS-ENTROPY 2.92728 PERPLEXITY 7.60674

Hmm Iteration: 5 took: 15 seconds

Entire Hmm Training took: 78 seconds
==========================================================
Read classes: #words: 727  #classes: 51
Read classes: #words: 602  #classes: 51
Read classes: #words: 727  #classes: 51
Read classes: #words: 602  #classes: 51

==========================================================
Starting H333444:  Viterbi Training
 H333444 Training Started at: Tue Nov 26 00:09:14 2019


---------------------
THTo3: Iteration 1
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
[sent:100000]
60000
70000
80000
90000
100000
Reading more sentence pairs into memory ... 
110000
120000
130000
140000
150000
Reading more sentence pairs into memory ... 
[sent:200000]
160000
170000
180000
190000
200000
Reading more sentence pairs into memory ... 
210000
220000
230000
240000
250000
Reading more sentence pairs into memory ... 
[sent:300000]
260000
270000
280000
290000
300000
Reading more sentence pairs into memory ... 
310000
320000
330000
340000
350000
Reading more sentence pairs into memory ... 
[sent:400000]
360000
370000
380000
390000
400000
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 116.016 #alsophisticatedcountcollection: 0 #hcsteps: 0
#peggingImprovements: 0
A/D table contains 11641 parameters.
A/D table contains 12668 parameters.
NTable contains 7280 parameter.
p0_count is 3.05713e+06 and p1 is 246498; p0 is 0.999 p1: 0.001
THTo3: TRAIN CROSS-ENTROPY 2.46887 PERPLEXITY 5.5361
THTo3: (1) TRAIN VITERBI CROSS-ENTROPY 2.55564 PERPLEXITY 5.87928

THTo3 Viterbi Iteration : 1 took: 16 seconds

---------------------
Model3: Iteration 2
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
[sent:100000]
60000
70000
80000
90000
100000
Reading more sentence pairs into memory ... 
110000
120000
130000
140000
150000
Reading more sentence pairs into memory ... 
[sent:200000]
160000
170000
180000
190000
200000
Reading more sentence pairs into memory ... 
210000
220000
230000
240000
250000
Reading more sentence pairs into memory ... 
[sent:300000]
260000
270000
280000
290000
300000
Reading more sentence pairs into memory ... 
310000
320000
330000
340000
350000
Reading more sentence pairs into memory ... 
[sent:400000]
360000
370000
380000
390000
400000
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 116.348 #alsophisticatedcountcollection: 0 #hcsteps: 1.49285
#peggingImprovements: 0
A/D table contains 11641 parameters.
A/D table contains 12667 parameters.
NTable contains 7280 parameter.
p0_count is 3.35874e+06 and p1 is 100248; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 3.6125 PERPLEXITY 12.2313
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 3.66897 PERPLEXITY 12.7195

Model3 Viterbi Iteration : 2 took: 15 seconds

---------------------
Model3: Iteration 3
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
[sent:100000]
60000
70000
80000
90000
100000
Reading more sentence pairs into memory ... 
110000
120000
130000
140000
150000
Reading more sentence pairs into memory ... 
[sent:200000]
160000
170000
180000
190000
200000
Reading more sentence pairs into memory ... 
210000
220000
230000
240000
250000
Reading more sentence pairs into memory ... 
[sent:300000]
260000
270000
280000
290000
300000
Reading more sentence pairs into memory ... 
310000
320000
330000
340000
350000
Reading more sentence pairs into memory ... 
[sent:400000]
360000
370000
380000
390000
400000
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 116.543 #alsophisticatedcountcollection: 0 #hcsteps: 1.54874
#peggingImprovements: 0
A/D table contains 11641 parameters.
A/D table contains 12667 parameters.
NTable contains 7280 parameter.
p0_count is 3.41995e+06 and p1 is 69644.3; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 3.49471 PERPLEXITY 11.2723
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 3.53674 PERPLEXITY 11.6055

Model3 Viterbi Iteration : 3 took: 16 seconds

---------------------
T3To4: Iteration 4
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
[sent:100000]
60000
70000
80000
90000
100000
Reading more sentence pairs into memory ... 
110000
120000
130000
140000
150000
Reading more sentence pairs into memory ... 
[sent:200000]
160000
170000
180000
190000
200000
Reading more sentence pairs into memory ... 
210000
220000
230000
240000
250000
Reading more sentence pairs into memory ... 
[sent:300000]
260000
270000
280000
290000
300000
Reading more sentence pairs into memory ... 
310000
320000
330000
340000
350000
Reading more sentence pairs into memory ... 
[sent:400000]
360000
370000
380000
390000
400000
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 116.565 #alsophisticatedcountcollection: 15.8166 #hcsteps: 1.49249
#peggingImprovements: 0
D4 table contains 517650 parameters.
A/D table contains 11641 parameters.
A/D table contains 12667 parameters.
NTable contains 7280 parameter.
p0_count is 3.4331e+06 and p1 is 63070.3; p0 is 0.999 p1: 0.001
T3To4: TRAIN CROSS-ENTROPY 3.44362 PERPLEXITY 10.8801
T3To4: (4) TRAIN VITERBI CROSS-ENTROPY 3.47472 PERPLEXITY 11.1172

T3To4 Viterbi Iteration : 4 took: 21 seconds

---------------------
Model4: Iteration 5
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
[sent:100000]
60000
70000
80000
90000
100000
Reading more sentence pairs into memory ... 
110000
120000
130000
140000
150000
Reading more sentence pairs into memory ... 
[sent:200000]
160000
170000
180000
190000
200000
Reading more sentence pairs into memory ... 
210000
220000
230000
240000
250000
Reading more sentence pairs into memory ... 
[sent:300000]
260000
270000
280000
290000
300000
Reading more sentence pairs into memory ... 
310000
320000
330000
340000
350000
Reading more sentence pairs into memory ... 
[sent:400000]
360000
370000
380000
390000
400000
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 116.517 #alsophisticatedcountcollection: 9.32115 #hcsteps: 1.42048
#peggingImprovements: 0
D4 table contains 517650 parameters.
A/D table contains 11641 parameters.
A/D table contains 12670 parameters.
NTable contains 7280 parameter.
p0_count is 3.44562e+06 and p1 is 56810.4; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 3.23255 PERPLEXITY 9.39929
Model4: (5) TRAIN VITERBI CROSS-ENTROPY 3.25389 PERPLEXITY 9.53936

Model4 Viterbi Iteration : 5 took: 36 seconds

---------------------
Model4: Iteration 6
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
[sent:100000]
60000
70000
80000
90000
100000
Reading more sentence pairs into memory ... 
110000
120000
130000
140000
150000
Reading more sentence pairs into memory ... 
[sent:200000]
160000
170000
180000
190000
200000
Reading more sentence pairs into memory ... 
210000
220000
230000
240000
250000
Reading more sentence pairs into memory ... 
[sent:300000]
260000
270000
280000
290000
300000
Reading more sentence pairs into memory ... 
310000
320000
330000
340000
350000
Reading more sentence pairs into memory ... 
[sent:400000]
360000
370000
380000
390000
400000
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 116.475 #alsophisticatedcountcollection: 8.17611 #hcsteps: 1.3736
#peggingImprovements: 0
D4 table contains 517650 parameters.
A/D table contains 11641 parameters.
A/D table contains 12670 parameters.
NTable contains 7280 parameter.
p0_count is 3.45593e+06 and p1 is 51652.8; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 3.17444 PERPLEXITY 9.02823
Model4: (6) TRAIN VITERBI CROSS-ENTROPY 3.19279 PERPLEXITY 9.14375

Model4 Viterbi Iteration : 6 took: 37 seconds
H333444 Training Finished at: Tue Nov 26 00:11:35 2019


Entire Viterbi H333444 Training took: 141 seconds
==========================================================

Entire Training took: 234 seconds
Program Finished at: Tue Nov 26 00:11:35 2019

==========================================================
Executing: rm -f /home/shanika/working/train/giza.ta-si/ta-si.A3.final.gz
Executing: gzip /home/shanika/working/train/giza.ta-si/ta-si.A3.final
(2.1a) running snt2cooc si-ta @ Tue Nov 26 00:11:38 +0530 2019

Executing: mkdir -p /home/shanika/working/train/giza.si-ta
Executing: /home/shanika/mosesdecoder/tools/snt2cooc.out /home/shanika/working/train/corpus/ta.vcb /home/shanika/working/train/corpus/si.vcb /home/shanika/working/train/corpus/si-ta-int-train.snt > /home/shanika/working/train/giza.si-ta/si-ta.cooc
/home/shanika/mosesdecoder/tools/snt2cooc.out /home/shanika/working/train/corpus/ta.vcb /home/shanika/working/train/corpus/si.vcb /home/shanika/working/train/corpus/si-ta-int-train.snt > /home/shanika/working/train/giza.si-ta/si-ta.cooc
line 1000
line 2000
line 3000
line 4000
line 5000
line 6000
line 7000
line 8000
line 9000
line 10000
line 11000
line 12000
line 13000
line 14000
line 15000
line 16000
line 17000
line 18000
line 19000
line 20000
line 21000
line 22000
line 23000
line 24000
line 25000
line 26000
line 27000
line 28000
line 29000
line 30000
line 31000
line 32000
line 33000
line 34000
line 35000
line 36000
line 37000
line 38000
line 39000
line 40000
line 41000
line 42000
line 43000
line 44000
line 45000
line 46000
line 47000
line 48000
line 49000
line 50000
line 51000
line 52000
line 53000
line 54000
line 55000
line 56000
line 57000
line 58000
line 59000
line 60000
line 61000
line 62000
line 63000
line 64000
line 65000
line 66000
line 67000
line 68000
line 69000
line 70000
line 71000
line 72000
line 73000
line 74000
line 75000
line 76000
line 77000
line 78000
line 79000
line 80000
line 81000
line 82000
line 83000
line 84000
line 85000
line 86000
line 87000
line 88000
line 89000
line 90000
line 91000
line 92000
line 93000
line 94000
line 95000
line 96000
line 97000
line 98000
line 99000
line 100000
line 101000
line 102000
line 103000
line 104000
line 105000
line 106000
line 107000
line 108000
line 109000
line 110000
line 111000
line 112000
line 113000
line 114000
line 115000
line 116000
line 117000
line 118000
line 119000
line 120000
line 121000
line 122000
line 123000
line 124000
line 125000
line 126000
line 127000
line 128000
line 129000
line 130000
line 131000
line 132000
line 133000
line 134000
line 135000
line 136000
line 137000
line 138000
line 139000
line 140000
line 141000
line 142000
line 143000
line 144000
line 145000
line 146000
line 147000
line 148000
line 149000
line 150000
line 151000
line 152000
line 153000
line 154000
line 155000
line 156000
line 157000
line 158000
line 159000
line 160000
line 161000
line 162000
line 163000
line 164000
line 165000
line 166000
line 167000
line 168000
line 169000
line 170000
line 171000
line 172000
line 173000
line 174000
line 175000
line 176000
line 177000
line 178000
line 179000
line 180000
line 181000
line 182000
line 183000
line 184000
line 185000
line 186000
line 187000
line 188000
line 189000
line 190000
line 191000
line 192000
line 193000
line 194000
line 195000
line 196000
line 197000
line 198000
line 199000
line 200000
line 201000
line 202000
line 203000
line 204000
line 205000
line 206000
line 207000
line 208000
line 209000
line 210000
line 211000
line 212000
line 213000
line 214000
line 215000
line 216000
line 217000
line 218000
line 219000
line 220000
line 221000
line 222000
line 223000
line 224000
line 225000
line 226000
line 227000
line 228000
line 229000
line 230000
line 231000
line 232000
line 233000
line 234000
line 235000
line 236000
line 237000
line 238000
line 239000
line 240000
line 241000
line 242000
line 243000
line 244000
line 245000
line 246000
line 247000
line 248000
line 249000
line 250000
line 251000
line 252000
line 253000
line 254000
line 255000
line 256000
line 257000
line 258000
line 259000
line 260000
line 261000
line 262000
line 263000
line 264000
line 265000
line 266000
line 267000
line 268000
line 269000
line 270000
line 271000
line 272000
line 273000
line 274000
line 275000
line 276000
line 277000
line 278000
line 279000
line 280000
line 281000
line 282000
line 283000
line 284000
line 285000
line 286000
line 287000
line 288000
line 289000
line 290000
line 291000
line 292000
line 293000
line 294000
line 295000
line 296000
line 297000
line 298000
line 299000
line 300000
line 301000
line 302000
line 303000
line 304000
line 305000
line 306000
line 307000
line 308000
line 309000
line 310000
line 311000
line 312000
line 313000
line 314000
line 315000
line 316000
line 317000
line 318000
line 319000
line 320000
line 321000
line 322000
line 323000
line 324000
line 325000
line 326000
line 327000
line 328000
line 329000
line 330000
line 331000
line 332000
line 333000
line 334000
line 335000
line 336000
line 337000
line 338000
line 339000
line 340000
line 341000
line 342000
line 343000
line 344000
line 345000
line 346000
line 347000
line 348000
line 349000
line 350000
line 351000
line 352000
line 353000
line 354000
line 355000
line 356000
line 357000
line 358000
line 359000
line 360000
line 361000
line 362000
line 363000
line 364000
line 365000
line 366000
line 367000
line 368000
line 369000
line 370000
line 371000
line 372000
line 373000
line 374000
line 375000
line 376000
line 377000
line 378000
line 379000
line 380000
line 381000
line 382000
line 383000
line 384000
line 385000
line 386000
line 387000
line 388000
line 389000
line 390000
line 391000
line 392000
line 393000
line 394000
line 395000
line 396000
line 397000
line 398000
line 399000
line 400000
END.
(2.1b) running giza si-ta @ Tue Nov 26 00:11:40 +0530 2019
/home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.si-ta/si-ta.cooc -c /home/shanika/working/train/corpus/si-ta-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.si-ta/si-ta -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/ta.vcb -t /home/shanika/working/train/corpus/si.vcb
Executing: /home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.si-ta/si-ta.cooc -c /home/shanika/working/train/corpus/si-ta-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.si-ta/si-ta -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/ta.vcb -t /home/shanika/working/train/corpus/si.vcb
/home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.si-ta/si-ta.cooc -c /home/shanika/working/train/corpus/si-ta-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.si-ta/si-ta -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/ta.vcb -t /home/shanika/working/train/corpus/si.vcb
Parameter 'coocurrencefile' changed from '' to '/home/shanika/working/train/giza.si-ta/si-ta.cooc'
Parameter 'c' changed from '' to '/home/shanika/working/train/corpus/si-ta-int-train.snt'
Parameter 'm3' changed from '5' to '3'
Parameter 'm4' changed from '5' to '3'
Parameter 'model1dumpfrequency' changed from '0' to '1'
Parameter 'model4smoothfactor' changed from '0.2' to '0.4'
Parameter 'nodumps' changed from '0' to '1'
Parameter 'nsmooth' changed from '64' to '4'
Parameter 'o' changed from '119-11-26.001140.shanika' to '/home/shanika/working/train/giza.si-ta/si-ta'
Parameter 'onlyaldumps' changed from '0' to '1'
Parameter 'p0' changed from '-1' to '0.999'
Parameter 's' changed from '' to '/home/shanika/working/train/corpus/ta.vcb'
Parameter 't' changed from '' to '/home/shanika/working/train/corpus/si.vcb'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 119-11-26.001140.shanika.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/shanika/working/train/giza.si-ta/si-ta  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/shanika/working/train/corpus/si-ta-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/shanika/working/train/corpus/ta.vcb  (source vocabulary file name)
t = /home/shanika/working/train/corpus/si.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 119-11-26.001140.shanika.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/shanika/working/train/giza.si-ta/si-ta  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/shanika/working/train/corpus/si-ta-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/shanika/working/train/corpus/ta.vcb  (source vocabulary file name)
t = /home/shanika/working/train/corpus/si.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Reading vocabulary file from:/home/shanika/working/train/corpus/ta.vcb
Reading vocabulary file from:/home/shanika/working/train/corpus/si.vcb
Source vocabulary list has 603 unique tokens 
Target vocabulary list has 728 unique tokens 
Calculating vocabulary frequencies from corpus /home/shanika/working/train/corpus/si-ta-int-train.snt
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:200000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:300000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:400000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
 Train total # sentence pairs (weighted): 400001
Size of source portion of the training corpus: 3.55924e+06 tokens
Size of the target portion of the training corpus: 3.25899e+06 tokens 
In source portion of the training corpus, only 602 unique tokens appeared
In target portion of the training corpus, only 726 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 3.25899e+06/(3.95924e+06-400001)== 0.915642
There are 64986 64986 entries in table
==========================================================
Model1 Training Started at: Tue Nov 26 00:11:41 2019

-----------
Model1: Iteration 1
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:200000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:300000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:400000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (1) TRAIN CROSS-ENTROPY 9.90339 PERPLEXITY 957.675
Model1: (1) VITERBI TRAIN CROSS-ENTROPY 13.2506 PERPLEXITY 9746.14
Model 1 Iteration: 1 took: 3 seconds
-----------
Model1: Iteration 2
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:200000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:300000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:400000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (2) TRAIN CROSS-ENTROPY 6.01799 PERPLEXITY 64.8032
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 7.77744 PERPLEXITY 219.403
Model 1 Iteration: 2 took: 3 seconds
-----------
Model1: Iteration 3
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:200000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:300000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:400000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (3) TRAIN CROSS-ENTROPY 5.30828 PERPLEXITY 39.6233
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 6.40636 PERPLEXITY 84.8214
Model 1 Iteration: 3 took: 2 seconds
-----------
Model1: Iteration 4
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:200000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:300000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:400000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (4) TRAIN CROSS-ENTROPY 5.03644 PERPLEXITY 32.8186
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 5.85943 PERPLEXITY 58.0584
Model 1 Iteration: 4 took: 3 seconds
-----------
Model1: Iteration 5
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:200000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:300000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:400000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (5) TRAIN CROSS-ENTROPY 4.94192 PERPLEXITY 30.7374
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 5.63657 PERPLEXITY 49.7483
Model 1 Iteration: 5 took: 3 seconds
Entire Model1 Training took: 14 seconds
NOTE: I am doing iterations with the HMM model!
Read classes: #words: 602  #classes: 51
Read classes: #words: 727  #classes: 51

==========================================================
Hmm Training Started at: Tue Nov 26 00:11:55 2019

-----------
Hmm: Iteration 1
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:200000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:300000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:400000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 12946 parameters.
Hmm: (1) TRAIN CROSS-ENTROPY 4.90383 PERPLEXITY 29.9364
Hmm: (1) VITERBI TRAIN CROSS-ENTROPY 5.53408 PERPLEXITY 46.3365

Hmm Iteration: 1 took: 17 seconds

-----------
Hmm: Iteration 2
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:200000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:300000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:400000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 12946 parameters.
Hmm: (2) TRAIN CROSS-ENTROPY 3.48302 PERPLEXITY 11.1813
Hmm: (2) VITERBI TRAIN CROSS-ENTROPY 3.75448 PERPLEXITY 13.4962

Hmm Iteration: 2 took: 18 seconds

-----------
Hmm: Iteration 3
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:200000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:300000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:400000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 12946 parameters.
Hmm: (3) TRAIN CROSS-ENTROPY 2.99092 PERPLEXITY 7.94979
Hmm: (3) VITERBI TRAIN CROSS-ENTROPY 3.15062 PERPLEXITY 8.88035

Hmm Iteration: 3 took: 17 seconds

-----------
Hmm: Iteration 4
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:200000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:300000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:400000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 12946 parameters.
Hmm: (4) TRAIN CROSS-ENTROPY 2.83977 PERPLEXITY 7.15904
Hmm: (4) VITERBI TRAIN CROSS-ENTROPY 2.94921 PERPLEXITY 7.72325

Hmm Iteration: 4 took: 17 seconds

-----------
Hmm: Iteration 5
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:200000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:300000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:400000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 12946 parameters.
Hmm: (5) TRAIN CROSS-ENTROPY 2.77974 PERPLEXITY 6.86728
Hmm: (5) VITERBI TRAIN CROSS-ENTROPY 2.86281 PERPLEXITY 7.27429

Hmm Iteration: 5 took: 18 seconds

Entire Hmm Training took: 87 seconds
==========================================================
Read classes: #words: 602  #classes: 51
Read classes: #words: 727  #classes: 51
Read classes: #words: 602  #classes: 51
Read classes: #words: 727  #classes: 51

==========================================================
Starting H333444:  Viterbi Training
 H333444 Training Started at: Tue Nov 26 00:13:22 2019


---------------------
THTo3: Iteration 1
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
[sent:100000]
60000
70000
80000
90000
100000
Reading more sentence pairs into memory ... 
110000
120000
130000
140000
150000
Reading more sentence pairs into memory ... 
[sent:200000]
160000
170000
180000
190000
200000
Reading more sentence pairs into memory ... 
210000
220000
230000
240000
250000
Reading more sentence pairs into memory ... 
[sent:300000]
260000
270000
280000
290000
300000
Reading more sentence pairs into memory ... 
310000
320000
330000
340000
350000
Reading more sentence pairs into memory ... 
[sent:400000]
360000
370000
380000
390000
400000
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 110.344 #alsophisticatedcountcollection: 0 #hcsteps: 0
#peggingImprovements: 0
A/D table contains 12946 parameters.
A/D table contains 11351 parameters.
NTable contains 6030 parameter.
p0_count is 2.92917e+06 and p1 is 157979; p0 is 0.999 p1: 0.001
THTo3: TRAIN CROSS-ENTROPY 2.37328 PERPLEXITY 5.18118
THTo3: (1) TRAIN VITERBI CROSS-ENTROPY 2.42947 PERPLEXITY 5.38694

THTo3 Viterbi Iteration : 1 took: 15 seconds

---------------------
Model3: Iteration 2
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
[sent:100000]
60000
70000
80000
90000
100000
Reading more sentence pairs into memory ... 
110000
120000
130000
140000
150000
Reading more sentence pairs into memory ... 
[sent:200000]
160000
170000
180000
190000
200000
Reading more sentence pairs into memory ... 
210000
220000
230000
240000
250000
Reading more sentence pairs into memory ... 
[sent:300000]
260000
270000
280000
290000
300000
Reading more sentence pairs into memory ... 
310000
320000
330000
340000
350000
Reading more sentence pairs into memory ... 
[sent:400000]
360000
370000
380000
390000
400000
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 110.605 #alsophisticatedcountcollection: 0 #hcsteps: 1.35344
#peggingImprovements: 0
A/D table contains 12946 parameters.
A/D table contains 11350 parameters.
NTable contains 6030 parameter.
p0_count is 3.11252e+06 and p1 is 73233.8; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 3.34278 PERPLEXITY 10.1456
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 3.37837 PERPLEXITY 10.399

Model3 Viterbi Iteration : 2 took: 16 seconds

---------------------
Model3: Iteration 3
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
[sent:100000]
60000
70000
80000
90000
100000
Reading more sentence pairs into memory ... 
110000
120000
130000
140000
150000
Reading more sentence pairs into memory ... 
[sent:200000]
160000
170000
180000
190000
200000
Reading more sentence pairs into memory ... 
210000
220000
230000
240000
250000
Reading more sentence pairs into memory ... 
[sent:300000]
260000
270000
280000
290000
300000
Reading more sentence pairs into memory ... 
310000
320000
330000
340000
350000
Reading more sentence pairs into memory ... 
[sent:400000]
360000
370000
380000
390000
400000
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 110.646 #alsophisticatedcountcollection: 0 #hcsteps: 1.35934
#peggingImprovements: 0
A/D table contains 12946 parameters.
A/D table contains 11350 parameters.
NTable contains 6030 parameter.
p0_count is 3.14399e+06 and p1 is 57498.2; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 3.25876 PERPLEXITY 9.57157
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 3.28924 PERPLEXITY 9.77597

Model3 Viterbi Iteration : 3 took: 16 seconds

---------------------
T3To4: Iteration 4
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
[sent:100000]
60000
70000
80000
90000
100000
Reading more sentence pairs into memory ... 
110000
120000
130000
140000
150000
Reading more sentence pairs into memory ... 
[sent:200000]
160000
170000
180000
190000
200000
Reading more sentence pairs into memory ... 
210000
220000
230000
240000
250000
Reading more sentence pairs into memory ... 
[sent:300000]
260000
270000
280000
290000
300000
Reading more sentence pairs into memory ... 
310000
320000
330000
340000
350000
Reading more sentence pairs into memory ... 
[sent:400000]
360000
370000
380000
390000
400000
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 110.673 #alsophisticatedcountcollection: 12.3264 #hcsteps: 1.36972
#peggingImprovements: 0
D4 table contains 526582 parameters.
A/D table contains 12946 parameters.
A/D table contains 11350 parameters.
NTable contains 6030 parameter.
p0_count is 3.15405e+06 and p1 is 52471.8; p0 is 0.999 p1: 0.001
T3To4: TRAIN CROSS-ENTROPY 3.22825 PERPLEXITY 9.37133
T3To4: (4) TRAIN VITERBI CROSS-ENTROPY 3.25763 PERPLEXITY 9.56411

T3To4 Viterbi Iteration : 4 took: 20 seconds

---------------------
Model4: Iteration 5
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
[sent:100000]
60000
70000
80000
90000
100000
Reading more sentence pairs into memory ... 
110000
120000
130000
140000
150000
Reading more sentence pairs into memory ... 
[sent:200000]
160000
170000
180000
190000
200000
Reading more sentence pairs into memory ... 
210000
220000
230000
240000
250000
Reading more sentence pairs into memory ... 
[sent:300000]
260000
270000
280000
290000
300000
Reading more sentence pairs into memory ... 
310000
320000
330000
340000
350000
Reading more sentence pairs into memory ... 
[sent:400000]
360000
370000
380000
390000
400000
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 110.639 #alsophisticatedcountcollection: 8.22863 #hcsteps: 1.30103
#peggingImprovements: 0
D4 table contains 526582 parameters.
A/D table contains 12946 parameters.
A/D table contains 11351 parameters.
NTable contains 6030 parameter.
p0_count is 3.16014e+06 and p1 is 49426.8; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 3.14948 PERPLEXITY 8.87336
Model4: (5) TRAIN VITERBI CROSS-ENTROPY 3.16923 PERPLEXITY 8.99564

Model4 Viterbi Iteration : 5 took: 34 seconds

---------------------
Model4: Iteration 6
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
[sent:100000]
60000
70000
80000
90000
100000
Reading more sentence pairs into memory ... 
110000
120000
130000
140000
150000
Reading more sentence pairs into memory ... 
[sent:200000]
160000
170000
180000
190000
200000
Reading more sentence pairs into memory ... 
210000
220000
230000
240000
250000
Reading more sentence pairs into memory ... 
[sent:300000]
260000
270000
280000
290000
300000
Reading more sentence pairs into memory ... 
310000
320000
330000
340000
350000
Reading more sentence pairs into memory ... 
[sent:400000]
360000
370000
380000
390000
400000
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 110.623 #alsophisticatedcountcollection: 7.61892 #hcsteps: 1.29055
#peggingImprovements: 0
D4 table contains 526582 parameters.
A/D table contains 12946 parameters.
A/D table contains 11351 parameters.
NTable contains 6030 parameter.
p0_count is 3.1649e+06 and p1 is 47042.6; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 3.10226 PERPLEXITY 8.58764
Model4: (6) TRAIN VITERBI CROSS-ENTROPY 3.11996 PERPLEXITY 8.69364

Model4 Viterbi Iteration : 6 took: 35 seconds
H333444 Training Finished at: Tue Nov 26 00:15:38 2019


Entire Viterbi H333444 Training took: 136 seconds
==========================================================

Entire Training took: 238 seconds
Program Finished at: Tue Nov 26 00:15:38 2019

==========================================================
Executing: rm -f /home/shanika/working/train/giza.si-ta/si-ta.A3.final.gz
Executing: gzip /home/shanika/working/train/giza.si-ta/si-ta.A3.final
(3) generate word alignment @ Tue Nov 26 00:15:40 +0530 2019
Combining forward and inverted alignment from files:
  /home/shanika/working/train/giza.ta-si/ta-si.A3.final.{bz2,gz}
  /home/shanika/working/train/giza.si-ta/si-ta.A3.final.{bz2,gz}
Executing: mkdir -p /home/shanika/working/train/model
Executing: /home/shanika/mosesdecoder/scripts/training/giza2bal.pl -d "gzip -cd /home/shanika/working/train/giza.si-ta/si-ta.A3.final.gz" -i "gzip -cd /home/shanika/working/train/giza.ta-si/ta-si.A3.final.gz" |/home/shanika/mosesdecoder/scripts/../bin/symal -alignment="grow" -diagonal="yes" -final="yes" -both="yes" > /home/shanika/working/train/model/aligned.grow-diag-final-and
symal: computing grow alignment: diagonal (1) final (1)both-uncovered (1)
skip=<0> counts=<400001>
(4) generate lexical translation table 0-0 @ Tue Nov 26 00:16:05 +0530 2019
(/home/shanika/corpus/data.ta-si.clean.ta,/home/shanika/corpus/data.ta-si.clean.si,/home/shanika/working/train/model/lex)
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
Saved: /home/shanika/working/train/model/lex.f2e and /home/shanika/working/train/model/lex.e2f
FILE: /home/shanika/corpus/data.ta-si.clean.si
FILE: /home/shanika/corpus/data.ta-si.clean.ta
FILE: /home/shanika/working/train/model/aligned.grow-diag-final-and
(5) extract phrases @ Tue Nov 26 00:16:16 +0530 2019
/home/shanika/mosesdecoder/scripts/generic/extract-parallel.perl 4 split "sort    " /home/shanika/mosesdecoder/scripts/../bin/extract /home/shanika/corpus/data.ta-si.clean.si /home/shanika/corpus/data.ta-si.clean.ta /home/shanika/working/train/model/aligned.grow-diag-final-and /home/shanika/working/train/model/extract 7 orientation --model wbe-msd --GZOutput 
Executing: /home/shanika/mosesdecoder/scripts/generic/extract-parallel.perl 4 split "sort    " /home/shanika/mosesdecoder/scripts/../bin/extract /home/shanika/corpus/data.ta-si.clean.si /home/shanika/corpus/data.ta-si.clean.ta /home/shanika/working/train/model/aligned.grow-diag-final-and /home/shanika/working/train/model/extract 7 orientation --model wbe-msd --GZOutput 
MAX 7 1 0
Started Tue Nov 26 00:16:16 2019
using gzip 
isBSDSplit=0 
Executing: mkdir -p /home/shanika/working/train/model/tmp.3491; ls -l /home/shanika/working/train/model/tmp.3491 
total=400001 line-per-split=100001 
split -d -l 100001 -a 7 /home/shanika/corpus/data.ta-si.clean.si /home/shanika/working/train/model/tmp.3491/target.split -d -l 100001 -a 7 /home/shanika/corpus/data.ta-si.clean.ta /home/shanika/working/train/model/tmp.3491/source.split -d -l 100001 -a 7 /home/shanika/working/train/model/aligned.grow-diag-final-and /home/shanika/working/train/model/tmp.3491/align.merging extract / extract.inv
gunzip -c /home/shanika/working/train/model/tmp.3491/extract.0000000.gz /home/shanika/working/train/model/tmp.3491/extract.0000001.gz /home/shanika/working/train/model/tmp.3491/extract.0000002.gz /home/shanika/working/train/model/tmp.3491/extract.0000003.gz  | LC_ALL=C sort     -T /home/shanika/working/train/model/tmp.3491 2>> /dev/stderr | gzip -c > /home/shanika/working/train/model/extract.sorted.gz 2>> /dev/stderr 
gunzip -c /home/shanika/working/train/model/tmp.3491/extract.0000000.inv.gz /home/shanika/working/train/model/tmp.3491/extract.0000001.inv.gz /home/shanika/working/train/model/tmp.3491/extract.0000002.inv.gz /home/shanika/working/train/model/tmp.3491/extract.0000003.inv.gz  | LC_ALL=C sort     -T /home/shanika/working/train/model/tmp.3491 2>> /dev/stderr | gzip -c > /home/shanika/working/train/model/extract.inv.sorted.gz 2>> /dev/stderr 
gunzip -c /home/shanika/working/train/model/tmp.3491/extract.0000000.o.gz /home/shanika/working/train/model/tmp.3491/extract.0000001.o.gz /home/shanika/working/train/model/tmp.3491/extract.0000002.o.gz /home/shanika/working/train/model/tmp.3491/extract.0000003.o.gz  | LC_ALL=C sort     -T /home/shanika/working/train/model/tmp.3491 2>> /dev/stderr | gzip -c > /home/shanika/working/train/model/extract.o.sorted.gz 2>> /dev/stderr 
Finished Tue Nov 26 00:20:11 2019
(6) score phrases @ Tue Nov 26 00:20:11 +0530 2019
(6.1)  creating table half /home/shanika/working/train/model/phrase-table.half.f2e @ Tue Nov 26 00:20:11 +0530 2019
/home/shanika/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/extract.sorted.gz /home/shanika/working/train/model/lex.f2e /home/shanika/working/train/model/phrase-table.half.f2e.gz  0 
Executing: /home/shanika/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/extract.sorted.gz /home/shanika/working/train/model/lex.f2e /home/shanika/working/train/model/phrase-table.half.f2e.gz  0 
using gzip 
Started Tue Nov 26 00:20:11 2019
/home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/tmp.3563/extract.0.gz /home/shanika/working/train/model/lex.f2e /home/shanika/working/train/model/tmp.3563/phrase-table.half.0000000.gz  2>> /dev/stderr 
/home/shanika/working/train/model/tmp.3563/run.0.sh/home/shanika/working/train/model/tmp.3563/run.1.sh/home/shanika/working/train/model/tmp.3563/run.2.sh/home/shanika/working/train/model/tmp.3563/run.3.shmv /home/shanika/working/train/model/tmp.3563/phrase-table.half.0000000.gz /home/shanika/working/train/model/phrase-table.half.f2e.gzrm -rf /home/shanika/working/train/model/tmp.3563 
Finished Tue Nov 26 00:20:37 2019
(6.3)  creating table half /home/shanika/working/train/model/phrase-table.half.e2f @ Tue Nov 26 00:20:37 +0530 2019
/home/shanika/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/extract.inv.sorted.gz /home/shanika/working/train/model/lex.e2f /home/shanika/working/train/model/phrase-table.half.e2f.gz --Inverse 1 
Executing: /home/shanika/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/extract.inv.sorted.gz /home/shanika/working/train/model/lex.e2f /home/shanika/working/train/model/phrase-table.half.e2f.gz --Inverse 1 
using gzip 
Started Tue Nov 26 00:20:37 2019
/home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/tmp.3591/extract.0.gz /home/shanika/working/train/model/lex.e2f /home/shanika/working/train/model/tmp.3591/phrase-table.half.0000000.gz --Inverse  2>> /dev/stderr 
/home/shanika/working/train/model/tmp.3591/run.0.sh/home/shanika/working/train/model/tmp.3591/run.1.sh/home/shanika/working/train/model/tmp.3591/run.2.sh/home/shanika/working/train/model/tmp.3591/run.3.shgunzip -c /home/shanika/working/train/model/tmp.3591/phrase-table.half.*.gz 2>> /dev/stderr| LC_ALL=C sort     -T /home/shanika/working/train/model/tmp.3591  | gzip -c > /home/shanika/working/train/model/phrase-table.half.e2f.gz  2>> /dev/stderr rm -rf /home/shanika/working/train/model/tmp.3591 
Finished Tue Nov 26 00:21:07 2019
(6.6) consolidating the two halves @ Tue Nov 26 00:21:07 +0530 2019
Executing: /home/shanika/mosesdecoder/scripts/../bin/consolidate /home/shanika/working/train/model/phrase-table.half.f2e.gz /home/shanika/working/train/model/phrase-table.half.e2f.gz /dev/stdout | gzip -c > /home/shanika/working/train/model/phrase-table.gz
Consolidate v2.0 written by Philipp Koehn
consolidating direct and indirect rule tables
..............
Executing: rm -f /home/shanika/working/train/model/phrase-table.half.*
(7) learn reordering model @ Tue Nov 26 00:21:15 +0530 2019
(7.1) [no factors] learn reordering model @ Tue Nov 26 00:21:15 +0530 2019
(7.2) building tables @ Tue Nov 26 00:21:15 +0530 2019
Executing: /home/shanika/mosesdecoder/scripts/../bin/lexical-reordering-score /home/shanika/working/train/model/extract.o.sorted.gz 0.5 /home/shanika/working/train/model/reordering-table. --model "wbe msd wbe-msd-bidirectional-fe"
Lexical Reordering Scorer
scores lexical reordering models of several types (hierarchical, phrase-based and word-based-extraction
(8) learn generation model @ Tue Nov 26 00:21:26 +0530 2019
  no generation model requested, skipping step
(9) create moses.ini @ Tue Nov 26 00:21:26 +0530 2019
