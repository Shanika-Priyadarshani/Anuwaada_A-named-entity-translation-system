nohup: ignoring input
Using SCRIPTS_ROOTDIR: /home/shanika/mosesdecoder/scripts
Using single-thread GIZA
using gzip 
(1) preparing corpus @ Sat Nov 23 17:01:25 +0530 2019
Executing: mkdir -p /home/shanika/working/train/corpus
(1.0) selecting factors @ Sat Nov 23 17:01:25 +0530 2019
(1.1) running mkcls  @ Sat Nov 23 17:01:25 +0530 2019
/home/shanika/mosesdecoder/tools/mkcls -c50 -n2 -p/home/shanika/corpus/data.si-ta.clean.si -V/home/shanika/working/train/corpus/si.vcb.classes opt
Executing: /home/shanika/mosesdecoder/tools/mkcls -c50 -n2 -p/home/shanika/corpus/data.si-ta.clean.si -V/home/shanika/working/train/corpus/si.vcb.classes opt

***** 2 runs. (algorithm:TA)*****
;KategProblem:cats: 50   words: 562

start-costs: MEAN: 5.28872e+06 (5.28843e+06-5.28901e+06)  SIGMA:288.114   
  end-costs: MEAN: 5.09579e+06 (5.09499e+06-5.09659e+06)  SIGMA:798.34   
   start-pp: MEAN: 42.9312 (42.9021-42.9603)  SIGMA:0.029129   
     end-pp: MEAN: 27.2555 (27.2043-27.3068)  SIGMA:0.0512426   
 iterations: MEAN: 16447 (15573-17321)  SIGMA:874   
       time: MEAN: 0.890648 (0.841579-0.939718)  SIGMA:0.0490695   
(1.1) running mkcls  @ Sat Nov 23 17:01:27 +0530 2019
/home/shanika/mosesdecoder/tools/mkcls -c50 -n2 -p/home/shanika/corpus/data.si-ta.clean.ta -V/home/shanika/working/train/corpus/ta.vcb.classes opt
Executing: /home/shanika/mosesdecoder/tools/mkcls -c50 -n2 -p/home/shanika/corpus/data.si-ta.clean.ta -V/home/shanika/working/train/corpus/ta.vcb.classes opt

***** 2 runs. (algorithm:TA)*****
;KategProblem:cats: 50   words: 500

start-costs: MEAN: 5.45764e+06 (5.45513e+06-5.46015e+06)  SIGMA:2511.83   
  end-costs: MEAN: 5.28872e+06 (5.28864e+06-5.28881e+06)  SIGMA:81.9613   
   start-pp: MEAN: 37.479 (37.2646-37.6935)  SIGMA:0.214437   
     end-pp: MEAN: 25.5083 (25.5036-25.5131)  SIGMA:0.00476229   
 iterations: MEAN: 14683.5 (14469-14898)  SIGMA:214.5   
       time: MEAN: 0.834102 (0.814977-0.853227)  SIGMA:0.019125   
(1.2) creating vcb file /home/shanika/working/train/corpus/si.vcb @ Sat Nov 23 17:01:28 +0530 2019
(1.2) creating vcb file /home/shanika/working/train/corpus/ta.vcb @ Sat Nov 23 17:01:28 +0530 2019
(1.3) numberizing corpus /home/shanika/working/train/corpus/si-ta-int-train.snt @ Sat Nov 23 17:01:28 +0530 2019
(1.3) numberizing corpus /home/shanika/working/train/corpus/ta-si-int-train.snt @ Sat Nov 23 17:01:29 +0530 2019
(2) running giza @ Sat Nov 23 17:01:29 +0530 2019
(2.1a) running snt2cooc si-ta @ Sat Nov 23 17:01:29 +0530 2019

Executing: mkdir -p /home/shanika/working/train/giza.si-ta
Executing: /home/shanika/mosesdecoder/tools/snt2cooc.out /home/shanika/working/train/corpus/ta.vcb /home/shanika/working/train/corpus/si.vcb /home/shanika/working/train/corpus/si-ta-int-train.snt > /home/shanika/working/train/giza.si-ta/si-ta.cooc
/home/shanika/mosesdecoder/tools/snt2cooc.out /home/shanika/working/train/corpus/ta.vcb /home/shanika/working/train/corpus/si.vcb /home/shanika/working/train/corpus/si-ta-int-train.snt > /home/shanika/working/train/giza.si-ta/si-ta.cooc
line 1000
line 2000
line 3000
line 4000
line 5000
line 6000
line 7000
line 8000
line 9000
line 10000
line 11000
line 12000
line 13000
line 14000
line 15000
line 16000
line 17000
line 18000
line 19000
line 20000
line 21000
line 22000
line 23000
line 24000
line 25000
line 26000
line 27000
line 28000
line 29000
line 30000
line 31000
line 32000
line 33000
line 34000
line 35000
line 36000
line 37000
line 38000
line 39000
line 40000
line 41000
line 42000
line 43000
line 44000
line 45000
line 46000
line 47000
line 48000
line 49000
line 50000
line 51000
line 52000
line 53000
line 54000
line 55000
END.
(2.1b) running giza si-ta @ Sat Nov 23 17:01:30 +0530 2019
/home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.si-ta/si-ta.cooc -c /home/shanika/working/train/corpus/si-ta-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.si-ta/si-ta -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/ta.vcb -t /home/shanika/working/train/corpus/si.vcb
Executing: /home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.si-ta/si-ta.cooc -c /home/shanika/working/train/corpus/si-ta-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.si-ta/si-ta -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/ta.vcb -t /home/shanika/working/train/corpus/si.vcb
/home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.si-ta/si-ta.cooc -c /home/shanika/working/train/corpus/si-ta-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.si-ta/si-ta -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/ta.vcb -t /home/shanika/working/train/corpus/si.vcb
Parameter 'coocurrencefile' changed from '' to '/home/shanika/working/train/giza.si-ta/si-ta.cooc'
Parameter 'c' changed from '' to '/home/shanika/working/train/corpus/si-ta-int-train.snt'
Parameter 'm3' changed from '5' to '3'
Parameter 'm4' changed from '5' to '3'
Parameter 'model1dumpfrequency' changed from '0' to '1'
Parameter 'model4smoothfactor' changed from '0.2' to '0.4'
Parameter 'nodumps' changed from '0' to '1'
Parameter 'nsmooth' changed from '64' to '4'
Parameter 'o' changed from '119-11-23.170130.shanika' to '/home/shanika/working/train/giza.si-ta/si-ta'
Parameter 'onlyaldumps' changed from '0' to '1'
Parameter 'p0' changed from '-1' to '0.999'
Parameter 's' changed from '' to '/home/shanika/working/train/corpus/ta.vcb'
Parameter 't' changed from '' to '/home/shanika/working/train/corpus/si.vcb'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 119-11-23.170130.shanika.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/shanika/working/train/giza.si-ta/si-ta  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/shanika/working/train/corpus/si-ta-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/shanika/working/train/corpus/ta.vcb  (source vocabulary file name)
t = /home/shanika/working/train/corpus/si.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 119-11-23.170130.shanika.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/shanika/working/train/giza.si-ta/si-ta  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/shanika/working/train/corpus/si-ta-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/shanika/working/train/corpus/ta.vcb  (source vocabulary file name)
t = /home/shanika/working/train/corpus/si.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Reading vocabulary file from:/home/shanika/working/train/corpus/ta.vcb
Reading vocabulary file from:/home/shanika/working/train/corpus/si.vcb
Source vocabulary list has 501 unique tokens 
Target vocabulary list has 563 unique tokens 
Calculating vocabulary frequencies from corpus /home/shanika/working/train/corpus/si-ta-int-train.snt
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
 Train total # sentence pairs (weighted): 55000
Size of source portion of the training corpus: 384011 tokens
Size of the target portion of the training corpus: 369630 tokens 
In source portion of the training corpus, only 500 unique tokens appeared
In target portion of the training corpus, only 561 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 369630/(439011-55000)== 0.962551
There are 47379 47379 entries in table
==========================================================
Model1 Training Started at: Sat Nov 23 17:01:30 2019

-----------
Model1: Iteration 1
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (1) TRAIN CROSS-ENTROPY 9.56987 PERPLEXITY 760.007
Model1: (1) VITERBI TRAIN CROSS-ENTROPY 12.6247 PERPLEXITY 6315.38
Model 1 Iteration: 1 took: 0 seconds
-----------
Model1: Iteration 2
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (2) TRAIN CROSS-ENTROPY 5.84967 PERPLEXITY 57.6667
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 7.22799 PERPLEXITY 149.914
Model 1 Iteration: 2 took: 0 seconds
-----------
Model1: Iteration 3
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (3) TRAIN CROSS-ENTROPY 5.00349 PERPLEXITY 32.0775
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 5.76729 PERPLEXITY 54.4661
Model 1 Iteration: 3 took: 1 seconds
-----------
Model1: Iteration 4
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (4) TRAIN CROSS-ENTROPY 4.73167 PERPLEXITY 26.5689
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 5.26308 PERPLEXITY 38.4012
Model 1 Iteration: 4 took: 0 seconds
-----------
Model1: Iteration 5
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (5) TRAIN CROSS-ENTROPY 4.64769 PERPLEXITY 25.0665
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 5.07604 PERPLEXITY 33.7318
Model 1 Iteration: 5 took: 0 seconds
Entire Model1 Training took: 1 seconds
NOTE: I am doing iterations with the HMM model!
Read classes: #words: 500  #classes: 51
Read classes: #words: 562  #classes: 51

==========================================================
Hmm Training Started at: Sat Nov 23 17:01:31 2019

-----------
Hmm: Iteration 1
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 8467 parameters.
Hmm: (1) TRAIN CROSS-ENTROPY 4.61544 PERPLEXITY 24.5125
Hmm: (1) VITERBI TRAIN CROSS-ENTROPY 4.99677 PERPLEXITY 31.9284

Hmm Iteration: 1 took: 2 seconds

-----------
Hmm: Iteration 2
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 8467 parameters.
Hmm: (2) TRAIN CROSS-ENTROPY 2.94907 PERPLEXITY 7.72249
Hmm: (2) VITERBI TRAIN CROSS-ENTROPY 3.07765 PERPLEXITY 8.44237

Hmm Iteration: 2 took: 2 seconds

-----------
Hmm: Iteration 3
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 8467 parameters.
Hmm: (3) TRAIN CROSS-ENTROPY 2.59055 PERPLEXITY 6.0233
Hmm: (3) VITERBI TRAIN CROSS-ENTROPY 2.67097 PERPLEXITY 6.36857

Hmm Iteration: 3 took: 1 seconds

-----------
Hmm: Iteration 4
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 8467 parameters.
Hmm: (4) TRAIN CROSS-ENTROPY 2.49743 PERPLEXITY 5.6468
Hmm: (4) VITERBI TRAIN CROSS-ENTROPY 2.56148 PERPLEXITY 5.90312

Hmm Iteration: 4 took: 2 seconds

-----------
Hmm: Iteration 5
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 8467 parameters.
Hmm: (5) TRAIN CROSS-ENTROPY 2.46412 PERPLEXITY 5.5179
Hmm: (5) VITERBI TRAIN CROSS-ENTROPY 2.51789 PERPLEXITY 5.72745

Hmm Iteration: 5 took: 2 seconds

Entire Hmm Training took: 9 seconds
==========================================================
Read classes: #words: 500  #classes: 51
Read classes: #words: 562  #classes: 51
Read classes: #words: 500  #classes: 51
Read classes: #words: 562  #classes: 51

==========================================================
Starting H333444:  Viterbi Training
 H333444 Training Started at: Sat Nov 23 17:01:40 2019


---------------------
THTo3: Iteration 1
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 74.0527 #alsophisticatedcountcollection: 0 #hcsteps: 0
#peggingImprovements: 0
A/D table contains 8467 parameters.
A/D table contains 7530 parameters.
NTable contains 5010 parameter.
p0_count is 338552 and p1 is 14440.6; p0 is 0.999 p1: 0.001
THTo3: TRAIN CROSS-ENTROPY 2.02988 PERPLEXITY 4.08372
THTo3: (1) TRAIN VITERBI CROSS-ENTROPY 2.07053 PERPLEXITY 4.20041

THTo3 Viterbi Iteration : 1 took: 1 seconds

---------------------
Model3: Iteration 2
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 74.2065 #alsophisticatedcountcollection: 0 #hcsteps: 1.24505
#peggingImprovements: 0
A/D table contains 8467 parameters.
A/D table contains 7529 parameters.
NTable contains 5010 parameter.
p0_count is 353276 and p1 is 8177.02; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 2.94391 PERPLEXITY 7.69494
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 2.97344 PERPLEXITY 7.85409

Model3 Viterbi Iteration : 2 took: 2 seconds

---------------------
Model3: Iteration 3
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 74.2423 #alsophisticatedcountcollection: 0 #hcsteps: 1.28235
#peggingImprovements: 0
A/D table contains 8467 parameters.
A/D table contains 7528 parameters.
NTable contains 5010 parameter.
p0_count is 355642 and p1 is 6993.83; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 2.875 PERPLEXITY 7.33601
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 2.90377 PERPLEXITY 7.48382

Model3 Viterbi Iteration : 3 took: 1 seconds

---------------------
T3To4: Iteration 4
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 74.2607 #alsophisticatedcountcollection: 8.00224 #hcsteps: 1.3012
#peggingImprovements: 0
D4 table contains 527394 parameters.
A/D table contains 8467 parameters.
A/D table contains 7528 parameters.
NTable contains 5010 parameter.
p0_count is 356214 and p1 is 6708.01; p0 is 0.999 p1: 0.001
T3To4: TRAIN CROSS-ENTROPY 2.8429 PERPLEXITY 7.17462
T3To4: (4) TRAIN VITERBI CROSS-ENTROPY 2.86856 PERPLEXITY 7.30335

T3To4 Viterbi Iteration : 4 took: 2 seconds

---------------------
Model4: Iteration 5
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 74.2316 #alsophisticatedcountcollection: 5.34529 #hcsteps: 1.28185
#peggingImprovements: 0
D4 table contains 527394 parameters.
A/D table contains 8467 parameters.
A/D table contains 7566 parameters.
NTable contains 5010 parameter.
p0_count is 357209 and p1 is 6210.3; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 2.87856 PERPLEXITY 7.35413
Model4: (5) TRAIN VITERBI CROSS-ENTROPY 2.8995 PERPLEXITY 7.46166

Model4 Viterbi Iteration : 5 took: 4 seconds

---------------------
Model4: Iteration 6
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 74.2202 #alsophisticatedcountcollection: 4.93307 #hcsteps: 1.26364
#peggingImprovements: 0
D4 table contains 527394 parameters.
A/D table contains 8467 parameters.
A/D table contains 7566 parameters.
NTable contains 5010 parameter.
p0_count is 357518 and p1 is 6055.9; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 2.83357 PERPLEXITY 7.12836
Model4: (6) TRAIN VITERBI CROSS-ENTROPY 2.85385 PERPLEXITY 7.22928

Model4 Viterbi Iteration : 6 took: 3 seconds
H333444 Training Finished at: Sat Nov 23 17:01:53 2019


Entire Viterbi H333444 Training took: 13 seconds
==========================================================

Entire Training took: 23 seconds
Program Finished at: Sat Nov 23 17:01:53 2019

==========================================================
Executing: rm -f /home/shanika/working/train/giza.si-ta/si-ta.A3.final.gz
Executing: gzip /home/shanika/working/train/giza.si-ta/si-ta.A3.final
(2.1a) running snt2cooc ta-si @ Sat Nov 23 17:01:53 +0530 2019

Executing: mkdir -p /home/shanika/working/train/giza.ta-si
Executing: /home/shanika/mosesdecoder/tools/snt2cooc.out /home/shanika/working/train/corpus/si.vcb /home/shanika/working/train/corpus/ta.vcb /home/shanika/working/train/corpus/ta-si-int-train.snt > /home/shanika/working/train/giza.ta-si/ta-si.cooc
/home/shanika/mosesdecoder/tools/snt2cooc.out /home/shanika/working/train/corpus/si.vcb /home/shanika/working/train/corpus/ta.vcb /home/shanika/working/train/corpus/ta-si-int-train.snt > /home/shanika/working/train/giza.ta-si/ta-si.cooc
line 1000
line 2000
line 3000
line 4000
line 5000
line 6000
line 7000
line 8000
line 9000
line 10000
line 11000
line 12000
line 13000
line 14000
line 15000
line 16000
line 17000
line 18000
line 19000
line 20000
line 21000
line 22000
line 23000
line 24000
line 25000
line 26000
line 27000
line 28000
line 29000
line 30000
line 31000
line 32000
line 33000
line 34000
line 35000
line 36000
line 37000
line 38000
line 39000
line 40000
line 41000
line 42000
line 43000
line 44000
line 45000
line 46000
line 47000
line 48000
line 49000
line 50000
line 51000
line 52000
line 53000
line 54000
line 55000
END.
(2.1b) running giza ta-si @ Sat Nov 23 17:01:54 +0530 2019
/home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.ta-si/ta-si.cooc -c /home/shanika/working/train/corpus/ta-si-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.ta-si/ta-si -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/si.vcb -t /home/shanika/working/train/corpus/ta.vcb
Executing: /home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.ta-si/ta-si.cooc -c /home/shanika/working/train/corpus/ta-si-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.ta-si/ta-si -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/si.vcb -t /home/shanika/working/train/corpus/ta.vcb
/home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.ta-si/ta-si.cooc -c /home/shanika/working/train/corpus/ta-si-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.ta-si/ta-si -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/si.vcb -t /home/shanika/working/train/corpus/ta.vcb
Parameter 'coocurrencefile' changed from '' to '/home/shanika/working/train/giza.ta-si/ta-si.cooc'
Parameter 'c' changed from '' to '/home/shanika/working/train/corpus/ta-si-int-train.snt'
Parameter 'm3' changed from '5' to '3'
Parameter 'm4' changed from '5' to '3'
Parameter 'model1dumpfrequency' changed from '0' to '1'
Parameter 'model4smoothfactor' changed from '0.2' to '0.4'
Parameter 'nodumps' changed from '0' to '1'
Parameter 'nsmooth' changed from '64' to '4'
Parameter 'o' changed from '119-11-23.170154.shanika' to '/home/shanika/working/train/giza.ta-si/ta-si'
Parameter 'onlyaldumps' changed from '0' to '1'
Parameter 'p0' changed from '-1' to '0.999'
Parameter 's' changed from '' to '/home/shanika/working/train/corpus/si.vcb'
Parameter 't' changed from '' to '/home/shanika/working/train/corpus/ta.vcb'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 119-11-23.170154.shanika.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/shanika/working/train/giza.ta-si/ta-si  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/shanika/working/train/corpus/ta-si-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/shanika/working/train/corpus/si.vcb  (source vocabulary file name)
t = /home/shanika/working/train/corpus/ta.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 119-11-23.170154.shanika.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/shanika/working/train/giza.ta-si/ta-si  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/shanika/working/train/corpus/ta-si-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/shanika/working/train/corpus/si.vcb  (source vocabulary file name)
t = /home/shanika/working/train/corpus/ta.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Reading vocabulary file from:/home/shanika/working/train/corpus/si.vcb
Reading vocabulary file from:/home/shanika/working/train/corpus/ta.vcb
Source vocabulary list has 563 unique tokens 
Target vocabulary list has 501 unique tokens 
Calculating vocabulary frequencies from corpus /home/shanika/working/train/corpus/ta-si-int-train.snt
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
 Train total # sentence pairs (weighted): 55000
Size of source portion of the training corpus: 369630 tokens
Size of the target portion of the training corpus: 384011 tokens 
In source portion of the training corpus, only 562 unique tokens appeared
In target portion of the training corpus, only 499 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 384011/(424630-55000)== 1.03891
There are 47317 47317 entries in table
==========================================================
Model1 Training Started at: Sat Nov 23 17:01:54 2019

-----------
Model1: Iteration 1
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (1) TRAIN CROSS-ENTROPY 9.38006 PERPLEXITY 666.314
Model1: (1) VITERBI TRAIN CROSS-ENTROPY 12.3867 PERPLEXITY 5355.18
Model 1 Iteration: 1 took: 0 seconds
-----------
Model1: Iteration 2
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (2) TRAIN CROSS-ENTROPY 5.69305 PERPLEXITY 51.7342
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 7.05268 PERPLEXITY 132.761
Model 1 Iteration: 2 took: 0 seconds
-----------
Model1: Iteration 3
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (3) TRAIN CROSS-ENTROPY 4.87257 PERPLEXITY 29.2948
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 5.62585 PERPLEXITY 49.3797
Model 1 Iteration: 3 took: 1 seconds
-----------
Model1: Iteration 4
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (4) TRAIN CROSS-ENTROPY 4.60652 PERPLEXITY 24.3613
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 5.1268 PERPLEXITY 34.9398
Model 1 Iteration: 4 took: 0 seconds
-----------
Model1: Iteration 5
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (5) TRAIN CROSS-ENTROPY 4.52239 PERPLEXITY 22.9813
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 4.93749 PERPLEXITY 30.643
Model 1 Iteration: 5 took: 0 seconds
Entire Model1 Training took: 1 seconds
NOTE: I am doing iterations with the HMM model!
Read classes: #words: 562  #classes: 51
Read classes: #words: 500  #classes: 51

==========================================================
Hmm Training Started at: Sat Nov 23 17:01:55 2019

-----------
Hmm: Iteration 1
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 7757 parameters.
Hmm: (1) TRAIN CROSS-ENTROPY 4.48938 PERPLEXITY 22.4614
Hmm: (1) VITERBI TRAIN CROSS-ENTROPY 4.85719 PERPLEXITY 28.984

Hmm Iteration: 1 took: 2 seconds

-----------
Hmm: Iteration 2
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 7757 parameters.
Hmm: (2) TRAIN CROSS-ENTROPY 2.83399 PERPLEXITY 7.13045
Hmm: (2) VITERBI TRAIN CROSS-ENTROPY 2.95903 PERPLEXITY 7.77599

Hmm Iteration: 2 took: 1 seconds

-----------
Hmm: Iteration 3
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 7757 parameters.
Hmm: (3) TRAIN CROSS-ENTROPY 2.46556 PERPLEXITY 5.52343
Hmm: (3) VITERBI TRAIN CROSS-ENTROPY 2.54654 PERPLEXITY 5.8423

Hmm Iteration: 3 took: 2 seconds

-----------
Hmm: Iteration 4
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 7757 parameters.
Hmm: (4) TRAIN CROSS-ENTROPY 2.36442 PERPLEXITY 5.14946
Hmm: (4) VITERBI TRAIN CROSS-ENTROPY 2.42651 PERPLEXITY 5.37591

Hmm Iteration: 4 took: 1 seconds

-----------
Hmm: Iteration 5
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 7757 parameters.
Hmm: (5) TRAIN CROSS-ENTROPY 2.32078 PERPLEXITY 4.99603
Hmm: (5) VITERBI TRAIN CROSS-ENTROPY 2.376 PERPLEXITY 5.19097

Hmm Iteration: 5 took: 2 seconds

Entire Hmm Training took: 8 seconds
==========================================================
Read classes: #words: 562  #classes: 51
Read classes: #words: 500  #classes: 51
Read classes: #words: 562  #classes: 51
Read classes: #words: 500  #classes: 51

==========================================================
Starting H333444:  Viterbi Training
 H333444 Training Started at: Sat Nov 23 17:02:03 2019


---------------------
THTo3: Iteration 1
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 75.642 #alsophisticatedcountcollection: 0 #hcsteps: 0
#peggingImprovements: 0
A/D table contains 7757 parameters.
A/D table contains 8169 parameters.
NTable contains 5630 parameter.
p0_count is 341573 and p1 is 20229.1; p0 is 0.999 p1: 0.001
THTo3: TRAIN CROSS-ENTROPY 1.90828 PERPLEXITY 3.75361
THTo3: (1) TRAIN VITERBI CROSS-ENTROPY 1.95441 PERPLEXITY 3.87556

THTo3 Viterbi Iteration : 1 took: 1 seconds

---------------------
Model3: Iteration 2
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 75.7617 #alsophisticatedcountcollection: 0 #hcsteps: 1.26464
#peggingImprovements: 0
A/D table contains 7757 parameters.
A/D table contains 8168 parameters.
NTable contains 5630 parameter.
p0_count is 362707 and p1 is 10651.8; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 2.94574 PERPLEXITY 7.70472
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 2.97036 PERPLEXITY 7.83734

Model3 Viterbi Iteration : 2 took: 2 seconds

---------------------
Model3: Iteration 3
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 75.7807 #alsophisticatedcountcollection: 0 #hcsteps: 1.27091
#peggingImprovements: 0
A/D table contains 7757 parameters.
A/D table contains 8168 parameters.
NTable contains 5630 parameter.
p0_count is 366277 and p1 is 8867.11; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 2.8716 PERPLEXITY 7.31874
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 2.89393 PERPLEXITY 7.43294

Model3 Viterbi Iteration : 3 took: 2 seconds

---------------------
T3To4: Iteration 4
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 75.8134 #alsophisticatedcountcollection: 8.0032 #hcsteps: 1.28255
#peggingImprovements: 0
D4 table contains 527191 parameters.
A/D table contains 7757 parameters.
A/D table contains 8193 parameters.
NTable contains 5630 parameter.
p0_count is 367804 and p1 is 8103.29; p0 is 0.999 p1: 0.001
T3To4: TRAIN CROSS-ENTROPY 2.84406 PERPLEXITY 7.18037
T3To4: (4) TRAIN VITERBI CROSS-ENTROPY 2.86512 PERPLEXITY 7.28596

T3To4 Viterbi Iteration : 4 took: 2 seconds

---------------------
Model4: Iteration 5
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 75.7725 #alsophisticatedcountcollection: 5.42869 #hcsteps: 1.26013
#peggingImprovements: 0
D4 table contains 527191 parameters.
A/D table contains 7757 parameters.
A/D table contains 8266 parameters.
NTable contains 5630 parameter.
p0_count is 370037 and p1 is 6987.02; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 2.82835 PERPLEXITY 7.10262
Model4: (5) TRAIN VITERBI CROSS-ENTROPY 2.84425 PERPLEXITY 7.18131

Model4 Viterbi Iteration : 5 took: 3 seconds

---------------------
Model4: Iteration 6
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 75.7631 #alsophisticatedcountcollection: 4.95722 #hcsteps: 1.25995
#peggingImprovements: 0
D4 table contains 527191 parameters.
A/D table contains 7757 parameters.
A/D table contains 8266 parameters.
NTable contains 5630 parameter.
p0_count is 371171 and p1 is 6419.86; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 2.78457 PERPLEXITY 6.89032
Model4: (6) TRAIN VITERBI CROSS-ENTROPY 2.79835 PERPLEXITY 6.95645

Model4 Viterbi Iteration : 6 took: 3 seconds
H333444 Training Finished at: Sat Nov 23 17:02:16 2019


Entire Viterbi H333444 Training took: 13 seconds
==========================================================

Entire Training took: 22 seconds
Program Finished at: Sat Nov 23 17:02:16 2019

==========================================================
Executing: rm -f /home/shanika/working/train/giza.ta-si/ta-si.A3.final.gz
Executing: gzip /home/shanika/working/train/giza.ta-si/ta-si.A3.final
(3) generate word alignment @ Sat Nov 23 17:02:17 +0530 2019
Combining forward and inverted alignment from files:
  /home/shanika/working/train/giza.si-ta/si-ta.A3.final.{bz2,gz}
  /home/shanika/working/train/giza.ta-si/ta-si.A3.final.{bz2,gz}
Executing: mkdir -p /home/shanika/working/train/model
Executing: /home/shanika/mosesdecoder/scripts/training/giza2bal.pl -d "gzip -cd /home/shanika/working/train/giza.ta-si/ta-si.A3.final.gz" -i "gzip -cd /home/shanika/working/train/giza.si-ta/si-ta.A3.final.gz" |/home/shanika/mosesdecoder/scripts/../bin/symal -alignment="grow" -diagonal="yes" -final="yes" -both="yes" > /home/shanika/working/train/model/aligned.grow-diag-final-and
symal: computing grow alignment: diagonal (1) final (1)both-uncovered (1)
skip=<0> counts=<55000>
(4) generate lexical translation table 0-0 @ Sat Nov 23 17:02:19 +0530 2019
(/home/shanika/corpus/data.si-ta.clean.si,/home/shanika/corpus/data.si-ta.clean.ta,/home/shanika/working/train/model/lex)
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
Saved: /home/shanika/working/train/model/lex.f2e and /home/shanika/working/train/model/lex.e2f
FILE: /home/shanika/corpus/data.si-ta.clean.ta
FILE: /home/shanika/corpus/data.si-ta.clean.si
FILE: /home/shanika/working/train/model/aligned.grow-diag-final-and
(5) extract phrases @ Sat Nov 23 17:02:20 +0530 2019
/home/shanika/mosesdecoder/scripts/generic/extract-parallel.perl 4 split "sort    " /home/shanika/mosesdecoder/scripts/../bin/extract /home/shanika/corpus/data.si-ta.clean.ta /home/shanika/corpus/data.si-ta.clean.si /home/shanika/working/train/model/aligned.grow-diag-final-and /home/shanika/working/train/model/extract 7 orientation --model wbe-msd --GZOutput 
Executing: /home/shanika/mosesdecoder/scripts/generic/extract-parallel.perl 4 split "sort    " /home/shanika/mosesdecoder/scripts/../bin/extract /home/shanika/corpus/data.si-ta.clean.ta /home/shanika/corpus/data.si-ta.clean.si /home/shanika/working/train/model/aligned.grow-diag-final-and /home/shanika/working/train/model/extract 7 orientation --model wbe-msd --GZOutput 
MAX 7 1 0
Started Sat Nov 23 17:02:20 2019
using gzip 
isBSDSplit=0 
Executing: mkdir -p /home/shanika/working/train/model/tmp.8805; ls -l /home/shanika/working/train/model/tmp.8805 
total=55000 line-per-split=13751 
split -d -l 13751 -a 7 /home/shanika/corpus/data.si-ta.clean.ta /home/shanika/working/train/model/tmp.8805/target.split -d -l 13751 -a 7 /home/shanika/corpus/data.si-ta.clean.si /home/shanika/working/train/model/tmp.8805/source.split -d -l 13751 -a 7 /home/shanika/working/train/model/aligned.grow-diag-final-and /home/shanika/working/train/model/tmp.8805/align.merging extract / extract.inv
gunzip -c /home/shanika/working/train/model/tmp.8805/extract.0000000.gz /home/shanika/working/train/model/tmp.8805/extract.0000001.gz /home/shanika/working/train/model/tmp.8805/extract.0000002.gz /home/shanika/working/train/model/tmp.8805/extract.0000003.gz  | LC_ALL=C sort     -T /home/shanika/working/train/model/tmp.8805 2>> /dev/stderr | gzip -c > /home/shanika/working/train/model/extract.sorted.gz 2>> /dev/stderr 
gunzip -c /home/shanika/working/train/model/tmp.8805/extract.0000000.inv.gz /home/shanika/working/train/model/tmp.8805/extract.0000001.inv.gz /home/shanika/working/train/model/tmp.8805/extract.0000002.inv.gz /home/shanika/working/train/model/tmp.8805/extract.0000003.inv.gz  | LC_ALL=C sort     -T /home/shanika/working/train/model/tmp.8805 2>> /dev/stderr | gzip -c > /home/shanika/working/train/model/extract.inv.sorted.gz 2>> /dev/stderr 
gunzip -c /home/shanika/working/train/model/tmp.8805/extract.0000000.o.gz /home/shanika/working/train/model/tmp.8805/extract.0000001.o.gz /home/shanika/working/train/model/tmp.8805/extract.0000002.o.gz /home/shanika/working/train/model/tmp.8805/extract.0000003.o.gz  | LC_ALL=C sort     -T /home/shanika/working/train/model/tmp.8805 2>> /dev/stderr | gzip -c > /home/shanika/working/train/model/extract.o.sorted.gz 2>> /dev/stderr 
Finished Sat Nov 23 17:02:32 2019
(6) score phrases @ Sat Nov 23 17:02:32 +0530 2019
(6.1)  creating table half /home/shanika/working/train/model/phrase-table.half.f2e @ Sat Nov 23 17:02:32 +0530 2019
/home/shanika/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/extract.sorted.gz /home/shanika/working/train/model/lex.f2e /home/shanika/working/train/model/phrase-table.half.f2e.gz  0 
Executing: /home/shanika/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/extract.sorted.gz /home/shanika/working/train/model/lex.f2e /home/shanika/working/train/model/phrase-table.half.f2e.gz  0 
using gzip 
Started Sat Nov 23 17:02:32 2019
/home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/tmp.8857/extract.0.gz /home/shanika/working/train/model/lex.f2e /home/shanika/working/train/model/tmp.8857/phrase-table.half.0000000.gz  2>> /dev/stderr 
/home/shanika/working/train/model/tmp.8857/run.0.sh/home/shanika/working/train/model/tmp.8857/run.1.sh/home/shanika/working/train/model/tmp.8857/run.2.sh/home/shanika/working/train/model/tmp.8857/run.3.shmv /home/shanika/working/train/model/tmp.8857/phrase-table.half.0000000.gz /home/shanika/working/train/model/phrase-table.half.f2e.gzrm -rf /home/shanika/working/train/model/tmp.8857 
Finished Sat Nov 23 17:02:40 2019
(6.3)  creating table half /home/shanika/working/train/model/phrase-table.half.e2f @ Sat Nov 23 17:02:40 +0530 2019
/home/shanika/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/extract.inv.sorted.gz /home/shanika/working/train/model/lex.e2f /home/shanika/working/train/model/phrase-table.half.e2f.gz --Inverse 1 
Executing: /home/shanika/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/extract.inv.sorted.gz /home/shanika/working/train/model/lex.e2f /home/shanika/working/train/model/phrase-table.half.e2f.gz --Inverse 1 
using gzip 
Started Sat Nov 23 17:02:40 2019
/home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/tmp.8944/extract.0.gz /home/shanika/working/train/model/lex.e2f /home/shanika/working/train/model/tmp.8944/phrase-table.half.0000000.gz --Inverse  2>> /dev/stderr 
/home/shanika/working/train/model/tmp.8944/run.0.sh/home/shanika/working/train/model/tmp.8944/run.1.sh/home/shanika/working/train/model/tmp.8944/run.2.sh/home/shanika/working/train/model/tmp.8944/run.3.shgunzip -c /home/shanika/working/train/model/tmp.8944/phrase-table.half.*.gz 2>> /dev/stderr| LC_ALL=C sort     -T /home/shanika/working/train/model/tmp.8944  | gzip -c > /home/shanika/working/train/model/phrase-table.half.e2f.gz  2>> /dev/stderr rm -rf /home/shanika/working/train/model/tmp.8944 
Finished Sat Nov 23 17:02:48 2019
(6.6) consolidating the two halves @ Sat Nov 23 17:02:48 +0530 2019
Executing: /home/shanika/mosesdecoder/scripts/../bin/consolidate /home/shanika/working/train/model/phrase-table.half.f2e.gz /home/shanika/working/train/model/phrase-table.half.e2f.gz /dev/stdout | gzip -c > /home/shanika/working/train/model/phrase-table.gz
Consolidate v2.0 written by Philipp Koehn
consolidating direct and indirect rule tables
....
Executing: rm -f /home/shanika/working/train/model/phrase-table.half.*
(7) learn reordering model @ Sat Nov 23 17:02:50 +0530 2019
(7.1) [no factors] learn reordering model @ Sat Nov 23 17:02:50 +0530 2019
(7.2) building tables @ Sat Nov 23 17:02:50 +0530 2019
Executing: /home/shanika/mosesdecoder/scripts/../bin/lexical-reordering-score /home/shanika/working/train/model/extract.o.sorted.gz 0.5 /home/shanika/working/train/model/reordering-table. --model "wbe msd wbe-msd-bidirectional-fe"
Lexical Reordering Scorer
scores lexical reordering models of several types (hierarchical, phrase-based and word-based-extraction
(8) learn generation model @ Sat Nov 23 17:02:53 +0530 2019
  no generation model requested, skipping step
(9) create moses.ini @ Sat Nov 23 17:02:53 +0530 2019
