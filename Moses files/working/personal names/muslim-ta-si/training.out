nohup: ignoring input
Using SCRIPTS_ROOTDIR: /home/anuadmin/moses/mosesdecoder/scripts
Using single-thread GIZA
using gzip 
(1) preparing corpus @ Tue Nov  5 13:06:45 +0530 2019
Executing: mkdir -p /home/anuadmin/moses/working/train/corpus
(1.0) selecting factors @ Tue Nov  5 13:06:45 +0530 2019
(1.1) running mkcls  @ Tue Nov  5 13:06:45 +0530 2019
/home/anuadmin/moses/mosesdecoder/tools/mkcls -c50 -n2 -p/home/anuadmin/moses/corpus/muslim-ta-si/data.ta-si.clean.ta -V/home/anuadmin/moses/working/train/corpus/ta.vcb.classes opt
Executing: /home/anuadmin/moses/mosesdecoder/tools/mkcls -c50 -n2 -p/home/anuadmin/moses/corpus/muslim-ta-si/data.ta-si.clean.ta -V/home/anuadmin/moses/working/train/corpus/ta.vcb.classes opt

***** 2 runs. (algorithm:TA)*****
;KategProblem:cats: 50   words: 268

start-costs: MEAN: 7.516e+06 (7.5002e+06-7.5318e+06)  SIGMA:15798.6   
  end-costs: MEAN: 7.10517e+06 (7.10471e+06-7.10563e+06)  SIGMA:461.058   
   start-pp: MEAN: 30.4221 (29.6175-31.2268)  SIGMA:0.80465   
     end-pp: MEAN: 15.2847 (15.2729-15.2965)  SIGMA:0.0118008   
 iterations: MEAN: 8002 (7743-8261)  SIGMA:259   
       time: MEAN: 0.76314 (0.737132-0.789147)  SIGMA:0.0260075   
(1.1) running mkcls  @ Tue Nov  5 13:06:47 +0530 2019
/home/anuadmin/moses/mosesdecoder/tools/mkcls -c50 -n2 -p/home/anuadmin/moses/corpus/muslim-ta-si/data.ta-si.clean.si -V/home/anuadmin/moses/working/train/corpus/si.vcb.classes opt
Executing: /home/anuadmin/moses/mosesdecoder/tools/mkcls -c50 -n2 -p/home/anuadmin/moses/corpus/muslim-ta-si/data.ta-si.clean.si -V/home/anuadmin/moses/working/train/corpus/si.vcb.classes opt

***** 2 runs. (algorithm:TA)*****
;KategProblem:cats: 50   words: 339

start-costs: MEAN: 6.85603e+06 (6.83814e+06-6.87392e+06)  SIGMA:17892.2   
  end-costs: MEAN: 6.5615e+06 (6.56104e+06-6.56195e+06)  SIGMA:450.815   
   start-pp: MEAN: 29.668 (28.6997-30.6362)  SIGMA:0.968236   
     end-pp: MEAN: 17.3242 (17.3099-17.3384)  SIGMA:0.0142507   
 iterations: MEAN: 10535.5 (10034-11037)  SIGMA:501.5   
       time: MEAN: 0.865835 (0.824943-0.906728)  SIGMA:0.0408925   
(1.2) creating vcb file /home/anuadmin/moses/working/train/corpus/ta.vcb @ Tue Nov  5 13:06:49 +0530 2019
(1.2) creating vcb file /home/anuadmin/moses/working/train/corpus/si.vcb @ Tue Nov  5 13:06:49 +0530 2019
(1.3) numberizing corpus /home/anuadmin/moses/working/train/corpus/ta-si-int-train.snt @ Tue Nov  5 13:06:49 +0530 2019
(1.3) numberizing corpus /home/anuadmin/moses/working/train/corpus/si-ta-int-train.snt @ Tue Nov  5 13:06:50 +0530 2019
(2) running giza @ Tue Nov  5 13:06:51 +0530 2019
(2.1a) running snt2cooc ta-si @ Tue Nov  5 13:06:51 +0530 2019

Executing: mkdir -p /home/anuadmin/moses/working/train/giza.ta-si
Executing: /home/anuadmin/moses/mosesdecoder/tools/snt2cooc.out /home/anuadmin/moses/working/train/corpus/si.vcb /home/anuadmin/moses/working/train/corpus/ta.vcb /home/anuadmin/moses/working/train/corpus/ta-si-int-train.snt > /home/anuadmin/moses/working/train/giza.ta-si/ta-si.cooc
/home/anuadmin/moses/mosesdecoder/tools/snt2cooc.out /home/anuadmin/moses/working/train/corpus/si.vcb /home/anuadmin/moses/working/train/corpus/ta.vcb /home/anuadmin/moses/working/train/corpus/ta-si-int-train.snt > /home/anuadmin/moses/working/train/giza.ta-si/ta-si.cooc
line 1000
line 2000
line 3000
line 4000
line 5000
line 6000
line 7000
line 8000
line 9000
line 10000
line 11000
line 12000
line 13000
line 14000
line 15000
line 16000
line 17000
line 18000
line 19000
line 20000
line 21000
line 22000
line 23000
line 24000
line 25000
line 26000
line 27000
line 28000
line 29000
line 30000
line 31000
line 32000
line 33000
line 34000
line 35000
line 36000
line 37000
line 38000
line 39000
line 40000
line 41000
line 42000
line 43000
line 44000
line 45000
line 46000
line 47000
line 48000
line 49000
line 50000
line 51000
line 52000
line 53000
line 54000
line 55000
line 56000
line 57000
line 58000
line 59000
line 60000
line 61000
line 62000
line 63000
line 64000
line 65000
line 66000
line 67000
line 68000
line 69000
line 70000
line 71000
line 72000
line 73000
line 74000
line 75000
line 76000
line 77000
line 78000
line 79000
line 80000
line 81000
line 82000
line 83000
line 84000
line 85000
END.
(2.1b) running giza ta-si @ Tue Nov  5 13:06:52 +0530 2019
/home/anuadmin/moses/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/anuadmin/moses/working/train/giza.ta-si/ta-si.cooc -c /home/anuadmin/moses/working/train/corpus/ta-si-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/anuadmin/moses/working/train/giza.ta-si/ta-si -onlyaldumps 1 -p0 0.999 -s /home/anuadmin/moses/working/train/corpus/si.vcb -t /home/anuadmin/moses/working/train/corpus/ta.vcb
Executing: /home/anuadmin/moses/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/anuadmin/moses/working/train/giza.ta-si/ta-si.cooc -c /home/anuadmin/moses/working/train/corpus/ta-si-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/anuadmin/moses/working/train/giza.ta-si/ta-si -onlyaldumps 1 -p0 0.999 -s /home/anuadmin/moses/working/train/corpus/si.vcb -t /home/anuadmin/moses/working/train/corpus/ta.vcb
/home/anuadmin/moses/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/anuadmin/moses/working/train/giza.ta-si/ta-si.cooc -c /home/anuadmin/moses/working/train/corpus/ta-si-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/anuadmin/moses/working/train/giza.ta-si/ta-si -onlyaldumps 1 -p0 0.999 -s /home/anuadmin/moses/working/train/corpus/si.vcb -t /home/anuadmin/moses/working/train/corpus/ta.vcb
Parameter 'coocurrencefile' changed from '' to '/home/anuadmin/moses/working/train/giza.ta-si/ta-si.cooc'
Parameter 'c' changed from '' to '/home/anuadmin/moses/working/train/corpus/ta-si-int-train.snt'
Parameter 'm3' changed from '5' to '3'
Parameter 'm4' changed from '5' to '3'
Parameter 'model1dumpfrequency' changed from '0' to '1'
Parameter 'model4smoothfactor' changed from '0.2' to '0.4'
Parameter 'nodumps' changed from '0' to '1'
Parameter 'nsmooth' changed from '64' to '4'
Parameter 'o' changed from '119-11-05.130652.anuadmin' to '/home/anuadmin/moses/working/train/giza.ta-si/ta-si'
Parameter 'onlyaldumps' changed from '0' to '1'
Parameter 'p0' changed from '-1' to '0.999'
Parameter 's' changed from '' to '/home/anuadmin/moses/working/train/corpus/si.vcb'
Parameter 't' changed from '' to '/home/anuadmin/moses/working/train/corpus/ta.vcb'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 119-11-05.130652.anuadmin.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/anuadmin/moses/working/train/giza.ta-si/ta-si  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/anuadmin/moses/working/train/corpus/ta-si-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/anuadmin/moses/working/train/corpus/si.vcb  (source vocabulary file name)
t = /home/anuadmin/moses/working/train/corpus/ta.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 119-11-05.130652.anuadmin.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/anuadmin/moses/working/train/giza.ta-si/ta-si  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/anuadmin/moses/working/train/corpus/ta-si-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/anuadmin/moses/working/train/corpus/si.vcb  (source vocabulary file name)
t = /home/anuadmin/moses/working/train/corpus/ta.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Reading vocabulary file from:/home/anuadmin/moses/working/train/corpus/si.vcb
Reading vocabulary file from:/home/anuadmin/moses/working/train/corpus/ta.vcb
Source vocabulary list has 340 unique tokens 
Target vocabulary list has 269 unique tokens 
Calculating vocabulary frequencies from corpus /home/anuadmin/moses/working/train/corpus/ta-si-int-train.snt
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
 Train total # sentence pairs (weighted): 85000
Size of source portion of the training corpus: 463044 tokens
Size of the target portion of the training corpus: 512174 tokens 
In source portion of the training corpus, only 339 unique tokens appeared
In target portion of the training corpus, only 267 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 512174/(548044-85000)== 1.1061
There are 21701 21701 entries in table
==========================================================
Model1 Training Started at: Tue Nov  5 13:06:52 2019

-----------
Model1: Iteration 1
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (1) TRAIN CROSS-ENTROPY 8.54084 PERPLEXITY 372.435
Model1: (1) VITERBI TRAIN CROSS-ENTROPY 11.258 PERPLEXITY 2448.97
Model 1 Iteration: 1 took: 0 seconds
-----------
Model1: Iteration 2
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (2) TRAIN CROSS-ENTROPY 5.44314 PERPLEXITY 43.5058
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 6.53036 PERPLEXITY 92.4347
Model 1 Iteration: 2 took: 1 seconds
-----------
Model1: Iteration 3
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (3) TRAIN CROSS-ENTROPY 4.5404 PERPLEXITY 23.27
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 5.18013 PERPLEXITY 36.2555
Model 1 Iteration: 3 took: 0 seconds
-----------
Model1: Iteration 4
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (4) TRAIN CROSS-ENTROPY 4.28561 PERPLEXITY 19.5028
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 4.79148 PERPLEXITY 27.6937
Model 1 Iteration: 4 took: 1 seconds
-----------
Model1: Iteration 5
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (5) TRAIN CROSS-ENTROPY 4.21075 PERPLEXITY 18.5166
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 4.6532 PERPLEXITY 25.1625
Model 1 Iteration: 5 took: 1 seconds
Entire Model1 Training took: 3 seconds
NOTE: I am doing iterations with the HMM model!
Read classes: #words: 339  #classes: 51
Read classes: #words: 268  #classes: 51

==========================================================
Hmm Training Started at: Tue Nov  5 13:06:55 2019

-----------
Hmm: Iteration 1
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 1905 parameters.
Hmm: (1) TRAIN CROSS-ENTROPY 4.18159 PERPLEXITY 18.1461
Hmm: (1) VITERBI TRAIN CROSS-ENTROPY 4.58885 PERPLEXITY 24.0648

Hmm Iteration: 1 took: 2 seconds

-----------
Hmm: Iteration 2
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 1905 parameters.
Hmm: (2) TRAIN CROSS-ENTROPY 2.5511 PERPLEXITY 5.8608
Hmm: (2) VITERBI TRAIN CROSS-ENTROPY 2.74288 PERPLEXITY 6.69406

Hmm Iteration: 2 took: 2 seconds

-----------
Hmm: Iteration 3
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 1905 parameters.
Hmm: (3) TRAIN CROSS-ENTROPY 2.23164 PERPLEXITY 4.69667
Hmm: (3) VITERBI TRAIN CROSS-ENTROPY 2.3493 PERPLEXITY 5.09579

Hmm Iteration: 3 took: 2 seconds

-----------
Hmm: Iteration 4
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 1905 parameters.
Hmm: (4) TRAIN CROSS-ENTROPY 2.08454 PERPLEXITY 4.24139
Hmm: (4) VITERBI TRAIN CROSS-ENTROPY 2.16964 PERPLEXITY 4.49912

Hmm Iteration: 4 took: 2 seconds

-----------
Hmm: Iteration 5
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 1905 parameters.
Hmm: (5) TRAIN CROSS-ENTROPY 2.00121 PERPLEXITY 4.00337
Hmm: (5) VITERBI TRAIN CROSS-ENTROPY 2.07269 PERPLEXITY 4.2067

Hmm Iteration: 5 took: 3 seconds

Entire Hmm Training took: 11 seconds
==========================================================
Read classes: #words: 339  #classes: 51
Read classes: #words: 268  #classes: 51
Read classes: #words: 339  #classes: 51
Read classes: #words: 268  #classes: 51

==========================================================
Starting H333444:  Viterbi Training
 H333444 Training Started at: Tue Nov  5 13:07:06 2019


---------------------
THTo3: Iteration 1
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
60000
70000
80000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 51.0486 #alsophisticatedcountcollection: 0 #hcsteps: 0
#peggingImprovements: 0
A/D table contains 1905 parameters.
A/D table contains 2044 parameters.
NTable contains 3400 parameter.
p0_count is 467420 and p1 is 22375.9; p0 is 0.999 p1: 0.001
THTo3: TRAIN CROSS-ENTROPY 1.51004 PERPLEXITY 2.84817
THTo3: (1) TRAIN VITERBI CROSS-ENTROPY 1.56069 PERPLEXITY 2.94995

THTo3 Viterbi Iteration : 1 took: 2 seconds

---------------------
Model3: Iteration 2
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
60000
70000
80000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 50.7226 #alsophisticatedcountcollection: 0 #hcsteps: 1.23259
#peggingImprovements: 0
A/D table contains 1905 parameters.
A/D table contains 2043 parameters.
NTable contains 3400 parameter.
p0_count is 499948 and p1 is 6113.05; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 2.24562 PERPLEXITY 4.74241
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 2.25786 PERPLEXITY 4.78281

Model3 Viterbi Iteration : 2 took: 3 seconds

---------------------
Model3: Iteration 3
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
60000
70000
80000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 50.6953 #alsophisticatedcountcollection: 0 #hcsteps: 1.04824
#peggingImprovements: 0
A/D table contains 1905 parameters.
A/D table contains 2043 parameters.
NTable contains 3400 parameter.
p0_count is 503177 and p1 is 4498.58; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 2.16048 PERPLEXITY 4.47063
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 2.16596 PERPLEXITY 4.48766

Model3 Viterbi Iteration : 3 took: 2 seconds

---------------------
T3To4: Iteration 4
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
60000
70000
80000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 50.6946 #alsophisticatedcountcollection: 2.76008 #hcsteps: 1.04655
#peggingImprovements: 0
D4 table contains 453502 parameters.
A/D table contains 1905 parameters.
A/D table contains 2043 parameters.
NTable contains 3400 parameter.
p0_count is 503481 and p1 is 4346.32; p0 is 0.999 p1: 0.001
T3To4: TRAIN CROSS-ENTROPY 2.14323 PERPLEXITY 4.41751
T3To4: (4) TRAIN VITERBI CROSS-ENTROPY 2.1467 PERPLEXITY 4.42814

T3To4 Viterbi Iteration : 4 took: 4 seconds

---------------------
Model4: Iteration 5
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
60000
70000
80000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 50.6921 #alsophisticatedcountcollection: 2.1302 #hcsteps: 1.04261
#peggingImprovements: 0
D4 table contains 453502 parameters.
A/D table contains 1905 parameters.
A/D table contains 2029 parameters.
NTable contains 3400 parameter.
p0_count is 503677 and p1 is 4248.65; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 2.18397 PERPLEXITY 4.54401
Model4: (5) TRAIN VITERBI CROSS-ENTROPY 2.18755 PERPLEXITY 4.55532

Model4 Viterbi Iteration : 5 took: 5 seconds

---------------------
Model4: Iteration 6
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
60000
70000
80000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 50.6919 #alsophisticatedcountcollection: 1.83606 #hcsteps: 1.06399
#peggingImprovements: 0
D4 table contains 453502 parameters.
A/D table contains 1905 parameters.
A/D table contains 2011 parameters.
NTable contains 3400 parameter.
p0_count is 503807 and p1 is 4183.31; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 2.1765 PERPLEXITY 4.52057
Model4: (6) TRAIN VITERBI CROSS-ENTROPY 2.18103 PERPLEXITY 4.53477

Model4 Viterbi Iteration : 6 took: 5 seconds
H333444 Training Finished at: Tue Nov  5 13:07:27 2019


Entire Viterbi H333444 Training took: 21 seconds
==========================================================

Entire Training took: 35 seconds
Program Finished at: Tue Nov  5 13:07:27 2019

==========================================================
Executing: rm -f /home/anuadmin/moses/working/train/giza.ta-si/ta-si.A3.final.gz
Executing: gzip /home/anuadmin/moses/working/train/giza.ta-si/ta-si.A3.final
(2.1a) running snt2cooc si-ta @ Tue Nov  5 13:07:28 +0530 2019

Executing: mkdir -p /home/anuadmin/moses/working/train/giza.si-ta
Executing: /home/anuadmin/moses/mosesdecoder/tools/snt2cooc.out /home/anuadmin/moses/working/train/corpus/ta.vcb /home/anuadmin/moses/working/train/corpus/si.vcb /home/anuadmin/moses/working/train/corpus/si-ta-int-train.snt > /home/anuadmin/moses/working/train/giza.si-ta/si-ta.cooc
/home/anuadmin/moses/mosesdecoder/tools/snt2cooc.out /home/anuadmin/moses/working/train/corpus/ta.vcb /home/anuadmin/moses/working/train/corpus/si.vcb /home/anuadmin/moses/working/train/corpus/si-ta-int-train.snt > /home/anuadmin/moses/working/train/giza.si-ta/si-ta.cooc
line 1000
line 2000
line 3000
line 4000
line 5000
line 6000
line 7000
line 8000
line 9000
line 10000
line 11000
line 12000
line 13000
line 14000
line 15000
line 16000
line 17000
line 18000
line 19000
line 20000
line 21000
line 22000
line 23000
line 24000
line 25000
line 26000
line 27000
line 28000
line 29000
line 30000
line 31000
line 32000
line 33000
line 34000
line 35000
line 36000
line 37000
line 38000
line 39000
line 40000
line 41000
line 42000
line 43000
line 44000
line 45000
line 46000
line 47000
line 48000
line 49000
line 50000
line 51000
line 52000
line 53000
line 54000
line 55000
line 56000
line 57000
line 58000
line 59000
line 60000
line 61000
line 62000
line 63000
line 64000
line 65000
line 66000
line 67000
line 68000
line 69000
line 70000
line 71000
line 72000
line 73000
line 74000
line 75000
line 76000
line 77000
line 78000
line 79000
line 80000
line 81000
line 82000
line 83000
line 84000
line 85000
END.
(2.1b) running giza si-ta @ Tue Nov  5 13:07:29 +0530 2019
/home/anuadmin/moses/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/anuadmin/moses/working/train/giza.si-ta/si-ta.cooc -c /home/anuadmin/moses/working/train/corpus/si-ta-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/anuadmin/moses/working/train/giza.si-ta/si-ta -onlyaldumps 1 -p0 0.999 -s /home/anuadmin/moses/working/train/corpus/ta.vcb -t /home/anuadmin/moses/working/train/corpus/si.vcb
Executing: /home/anuadmin/moses/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/anuadmin/moses/working/train/giza.si-ta/si-ta.cooc -c /home/anuadmin/moses/working/train/corpus/si-ta-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/anuadmin/moses/working/train/giza.si-ta/si-ta -onlyaldumps 1 -p0 0.999 -s /home/anuadmin/moses/working/train/corpus/ta.vcb -t /home/anuadmin/moses/working/train/corpus/si.vcb
/home/anuadmin/moses/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/anuadmin/moses/working/train/giza.si-ta/si-ta.cooc -c /home/anuadmin/moses/working/train/corpus/si-ta-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/anuadmin/moses/working/train/giza.si-ta/si-ta -onlyaldumps 1 -p0 0.999 -s /home/anuadmin/moses/working/train/corpus/ta.vcb -t /home/anuadmin/moses/working/train/corpus/si.vcb
Parameter 'coocurrencefile' changed from '' to '/home/anuadmin/moses/working/train/giza.si-ta/si-ta.cooc'
Parameter 'c' changed from '' to '/home/anuadmin/moses/working/train/corpus/si-ta-int-train.snt'
Parameter 'm3' changed from '5' to '3'
Parameter 'm4' changed from '5' to '3'
Parameter 'model1dumpfrequency' changed from '0' to '1'
Parameter 'model4smoothfactor' changed from '0.2' to '0.4'
Parameter 'nodumps' changed from '0' to '1'
Parameter 'nsmooth' changed from '64' to '4'
Parameter 'o' changed from '119-11-05.130729.anuadmin' to '/home/anuadmin/moses/working/train/giza.si-ta/si-ta'
Parameter 'onlyaldumps' changed from '0' to '1'
Parameter 'p0' changed from '-1' to '0.999'
Parameter 's' changed from '' to '/home/anuadmin/moses/working/train/corpus/ta.vcb'
Parameter 't' changed from '' to '/home/anuadmin/moses/working/train/corpus/si.vcb'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 119-11-05.130729.anuadmin.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/anuadmin/moses/working/train/giza.si-ta/si-ta  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/anuadmin/moses/working/train/corpus/si-ta-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/anuadmin/moses/working/train/corpus/ta.vcb  (source vocabulary file name)
t = /home/anuadmin/moses/working/train/corpus/si.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 119-11-05.130729.anuadmin.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/anuadmin/moses/working/train/giza.si-ta/si-ta  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/anuadmin/moses/working/train/corpus/si-ta-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/anuadmin/moses/working/train/corpus/ta.vcb  (source vocabulary file name)
t = /home/anuadmin/moses/working/train/corpus/si.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Reading vocabulary file from:/home/anuadmin/moses/working/train/corpus/ta.vcb
Reading vocabulary file from:/home/anuadmin/moses/working/train/corpus/si.vcb
Source vocabulary list has 269 unique tokens 
Target vocabulary list has 340 unique tokens 
Calculating vocabulary frequencies from corpus /home/anuadmin/moses/working/train/corpus/si-ta-int-train.snt
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
 Train total # sentence pairs (weighted): 85000
Size of source portion of the training corpus: 512174 tokens
Size of the target portion of the training corpus: 463044 tokens 
In source portion of the training corpus, only 268 unique tokens appeared
In target portion of the training corpus, only 338 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 463044/(597174-85000)== 0.904076
There are 21772 21772 entries in table
==========================================================
Model1 Training Started at: Tue Nov  5 13:07:29 2019

-----------
Model1: Iteration 1
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (1) TRAIN CROSS-ENTROPY 8.93841 PERPLEXITY 490.603
Model1: (1) VITERBI TRAIN CROSS-ENTROPY 11.7724 PERPLEXITY 3498.19
Model 1 Iteration: 1 took: 1 seconds
-----------
Model1: Iteration 2
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (2) TRAIN CROSS-ENTROPY 5.44768 PERPLEXITY 43.6431
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 6.58085 PERPLEXITY 95.7269
Model 1 Iteration: 2 took: 0 seconds
-----------
Model1: Iteration 3
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (3) TRAIN CROSS-ENTROPY 4.49653 PERPLEXITY 22.5731
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 5.14696 PERPLEXITY 35.4314
Model 1 Iteration: 3 took: 1 seconds
-----------
Model1: Iteration 4
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (4) TRAIN CROSS-ENTROPY 4.22712 PERPLEXITY 18.728
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 4.72388 PERPLEXITY 26.4258
Model 1 Iteration: 4 took: 1 seconds
-----------
Model1: Iteration 5
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (5) TRAIN CROSS-ENTROPY 4.15174 PERPLEXITY 17.7745
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 4.58314 PERPLEXITY 23.9697
Model 1 Iteration: 5 took: 0 seconds
Entire Model1 Training took: 3 seconds
NOTE: I am doing iterations with the HMM model!
Read classes: #words: 268  #classes: 51
Read classes: #words: 339  #classes: 51

==========================================================
Hmm Training Started at: Tue Nov  5 13:07:32 2019

-----------
Hmm: Iteration 1
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 2068 parameters.
Hmm: (1) TRAIN CROSS-ENTROPY 4.12382 PERPLEXITY 17.4339
Hmm: (1) VITERBI TRAIN CROSS-ENTROPY 4.51998 PERPLEXITY 22.9429

Hmm Iteration: 1 took: 2 seconds

-----------
Hmm: Iteration 2
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 2068 parameters.
Hmm: (2) TRAIN CROSS-ENTROPY 2.40833 PERPLEXITY 5.3086
Hmm: (2) VITERBI TRAIN CROSS-ENTROPY 2.60881 PERPLEXITY 6.09999

Hmm Iteration: 2 took: 3 seconds

-----------
Hmm: Iteration 3
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 2068 parameters.
Hmm: (3) TRAIN CROSS-ENTROPY 2.06759 PERPLEXITY 4.19185
Hmm: (3) VITERBI TRAIN CROSS-ENTROPY 2.19694 PERPLEXITY 4.58507

Hmm Iteration: 3 took: 3 seconds

-----------
Hmm: Iteration 4
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 2068 parameters.
Hmm: (4) TRAIN CROSS-ENTROPY 1.9571 PERPLEXITY 3.8828
Hmm: (4) VITERBI TRAIN CROSS-ENTROPY 2.04654 PERPLEXITY 4.13115

Hmm Iteration: 4 took: 2 seconds

-----------
Hmm: Iteration 5
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 2068 parameters.
Hmm: (5) TRAIN CROSS-ENTROPY 1.9109 PERPLEXITY 3.76043
Hmm: (5) VITERBI TRAIN CROSS-ENTROPY 1.97614 PERPLEXITY 3.9344

Hmm Iteration: 5 took: 3 seconds

Entire Hmm Training took: 13 seconds
==========================================================
Read classes: #words: 268  #classes: 51
Read classes: #words: 339  #classes: 51
Read classes: #words: 268  #classes: 51
Read classes: #words: 339  #classes: 51

==========================================================
Starting H333444:  Viterbi Training
 H333444 Training Started at: Tue Nov  5 13:07:45 2019


---------------------
THTo3: Iteration 1
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
60000
70000
80000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 48.4952 #alsophisticatedcountcollection: 0 #hcsteps: 0
#peggingImprovements: 0
A/D table contains 2068 parameters.
A/D table contains 1827 parameters.
NTable contains 2690 parameter.
p0_count is 436563 and p1 is 13239.6; p0 is 0.999 p1: 0.001
THTo3: TRAIN CROSS-ENTROPY 1.3604 PERPLEXITY 2.56755
THTo3: (1) TRAIN VITERBI CROSS-ENTROPY 1.40747 PERPLEXITY 2.65272

THTo3 Viterbi Iteration : 1 took: 2 seconds

---------------------
Model3: Iteration 2
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
60000
70000
80000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 48.4145 #alsophisticatedcountcollection: 0 #hcsteps: 1.13013
#peggingImprovements: 0
A/D table contains 2068 parameters.
A/D table contains 1827 parameters.
NTable contains 2690 parameter.
p0_count is 455592 and p1 is 3725.99; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 1.97676 PERPLEXITY 3.93609
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 1.99256 PERPLEXITY 3.97943

Model3 Viterbi Iteration : 2 took: 3 seconds

---------------------
Model3: Iteration 3
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
60000
70000
80000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 48.4097 #alsophisticatedcountcollection: 0 #hcsteps: 1.12728
#peggingImprovements: 0
A/D table contains 2068 parameters.
A/D table contains 1827 parameters.
NTable contains 2690 parameter.
p0_count is 460117 and p1 is 1463.7; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 1.89298 PERPLEXITY 3.71401
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 1.89708 PERPLEXITY 3.72458

Model3 Viterbi Iteration : 3 took: 3 seconds

---------------------
T3To4: Iteration 4
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
60000
70000
80000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 48.4114 #alsophisticatedcountcollection: 2.73667 #hcsteps: 1.06452
#peggingImprovements: 0
D4 table contains 433811 parameters.
A/D table contains 2068 parameters.
A/D table contains 1827 parameters.
NTable contains 2690 parameter.
p0_count is 460595 and p1 is 1224.48; p0 is 0.999 p1: 0.001
T3To4: TRAIN CROSS-ENTROPY 1.87711 PERPLEXITY 3.67339
T3To4: (4) TRAIN VITERBI CROSS-ENTROPY 1.88032 PERPLEXITY 3.68156

T3To4 Viterbi Iteration : 4 took: 3 seconds

---------------------
Model4: Iteration 5
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
60000
70000
80000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 48.4092 #alsophisticatedcountcollection: 2.39731 #hcsteps: 1.05965
#peggingImprovements: 0
D4 table contains 433811 parameters.
A/D table contains 2068 parameters.
A/D table contains 1827 parameters.
NTable contains 2690 parameter.
p0_count is 461343 and p1 is 850.49; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 2.08013 PERPLEXITY 4.22845
Model4: (5) TRAIN VITERBI CROSS-ENTROPY 2.08395 PERPLEXITY 4.23966

Model4 Viterbi Iteration : 5 took: 5 seconds

---------------------
Model4: Iteration 6
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
60000
70000
80000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 48.4092 #alsophisticatedcountcollection: 2.37438 #hcsteps: 1.05995
#peggingImprovements: 0
D4 table contains 433811 parameters.
A/D table contains 2068 parameters.
A/D table contains 1827 parameters.
NTable contains 2690 parameter.
p0_count is 461496 and p1 is 774.169; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 2.07142 PERPLEXITY 4.203
Model4: (6) TRAIN VITERBI CROSS-ENTROPY 2.07547 PERPLEXITY 4.21481

Model4 Viterbi Iteration : 6 took: 6 seconds
H333444 Training Finished at: Tue Nov  5 13:08:07 2019


Entire Viterbi H333444 Training took: 22 seconds
==========================================================

Entire Training took: 38 seconds
Program Finished at: Tue Nov  5 13:08:07 2019

==========================================================
Executing: rm -f /home/anuadmin/moses/working/train/giza.si-ta/si-ta.A3.final.gz
Executing: gzip /home/anuadmin/moses/working/train/giza.si-ta/si-ta.A3.final
(3) generate word alignment @ Tue Nov  5 13:08:08 +0530 2019
Combining forward and inverted alignment from files:
  /home/anuadmin/moses/working/train/giza.ta-si/ta-si.A3.final.{bz2,gz}
  /home/anuadmin/moses/working/train/giza.si-ta/si-ta.A3.final.{bz2,gz}
Executing: mkdir -p /home/anuadmin/moses/working/train/model
Executing: /home/anuadmin/moses/mosesdecoder/scripts/training/giza2bal.pl -d "gzip -cd /home/anuadmin/moses/working/train/giza.si-ta/si-ta.A3.final.gz" -i "gzip -cd /home/anuadmin/moses/working/train/giza.ta-si/ta-si.A3.final.gz" |/home/anuadmin/moses/mosesdecoder/scripts/../bin/symal -alignment="grow" -diagonal="yes" -final="yes" -both="yes" > /home/anuadmin/moses/working/train/model/aligned.grow-diag-final-and
symal: computing grow alignment: diagonal (1) final (1)both-uncovered (1)
skip=<0> counts=<85000>
(4) generate lexical translation table 0-0 @ Tue Nov  5 13:08:15 +0530 2019
(/home/anuadmin/moses/corpus/muslim-ta-si/data.ta-si.clean.ta,/home/anuadmin/moses/corpus/muslim-ta-si/data.ta-si.clean.si,/home/anuadmin/moses/working/train/model/lex)
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
Saved: /home/anuadmin/moses/working/train/model/lex.f2e and /home/anuadmin/moses/working/train/model/lex.e2f
FILE: /home/anuadmin/moses/corpus/muslim-ta-si/data.ta-si.clean.si
FILE: /home/anuadmin/moses/corpus/muslim-ta-si/data.ta-si.clean.ta
FILE: /home/anuadmin/moses/working/train/model/aligned.grow-diag-final-and
(5) extract phrases @ Tue Nov  5 13:08:18 +0530 2019
/home/anuadmin/moses/mosesdecoder/scripts/generic/extract-parallel.perl 2 split "sort    " /home/anuadmin/moses/mosesdecoder/scripts/../bin/extract /home/anuadmin/moses/corpus/muslim-ta-si/data.ta-si.clean.si /home/anuadmin/moses/corpus/muslim-ta-si/data.ta-si.clean.ta /home/anuadmin/moses/working/train/model/aligned.grow-diag-final-and /home/anuadmin/moses/working/train/model/extract 7 orientation --model wbe-msd --GZOutput 
Executing: /home/anuadmin/moses/mosesdecoder/scripts/generic/extract-parallel.perl 2 split "sort    " /home/anuadmin/moses/mosesdecoder/scripts/../bin/extract /home/anuadmin/moses/corpus/muslim-ta-si/data.ta-si.clean.si /home/anuadmin/moses/corpus/muslim-ta-si/data.ta-si.clean.ta /home/anuadmin/moses/working/train/model/aligned.grow-diag-final-and /home/anuadmin/moses/working/train/model/extract 7 orientation --model wbe-msd --GZOutput 
MAX 7 1 0
Started Tue Nov  5 13:08:18 2019
using gzip 
isBSDSplit=0 
Executing: mkdir -p /home/anuadmin/moses/working/train/model/tmp.26075; ls -l /home/anuadmin/moses/working/train/model/tmp.26075 
total=85000 line-per-split=42501 
split -d -l 42501 -a 7 /home/anuadmin/moses/corpus/muslim-ta-si/data.ta-si.clean.si /home/anuadmin/moses/working/train/model/tmp.26075/target.split -d -l 42501 -a 7 /home/anuadmin/moses/corpus/muslim-ta-si/data.ta-si.clean.ta /home/anuadmin/moses/working/train/model/tmp.26075/source.split -d -l 42501 -a 7 /home/anuadmin/moses/working/train/model/aligned.grow-diag-final-and /home/anuadmin/moses/working/train/model/tmp.26075/align.merging extract / extract.inv
gunzip -c /home/anuadmin/moses/working/train/model/tmp.26075/extract.0000000.gz /home/anuadmin/moses/working/train/model/tmp.26075/extract.0000001.gz  | LC_ALL=C sort     -T /home/anuadmin/moses/working/train/model/tmp.26075 2>> /dev/stderr | gzip -c > /home/anuadmin/moses/working/train/model/extract.sorted.gz 2>> /dev/stderr 
gunzip -c /home/anuadmin/moses/working/train/model/tmp.26075/extract.0000000.inv.gz /home/anuadmin/moses/working/train/model/tmp.26075/extract.0000001.inv.gz  | LC_ALL=C sort     -T /home/anuadmin/moses/working/train/model/tmp.26075 2>> /dev/stderr | gzip -c > /home/anuadmin/moses/working/train/model/extract.inv.sorted.gz 2>> /dev/stderr 
gunzip -c /home/anuadmin/moses/working/train/model/tmp.26075/extract.0000000.o.gz /home/anuadmin/moses/working/train/model/tmp.26075/extract.0000001.o.gz  | LC_ALL=C sort     -T /home/anuadmin/moses/working/train/model/tmp.26075 2>> /dev/stderr | gzip -c > /home/anuadmin/moses/working/train/model/extract.o.sorted.gz 2>> /dev/stderr 
Finished Tue Nov  5 13:08:32 2019
(6) score phrases @ Tue Nov  5 13:08:32 +0530 2019
(6.1)  creating table half /home/anuadmin/moses/working/train/model/phrase-table.half.f2e @ Tue Nov  5 13:08:32 +0530 2019
/home/anuadmin/moses/mosesdecoder/scripts/generic/score-parallel.perl 2 "sort    " /home/anuadmin/moses/mosesdecoder/scripts/../bin/score /home/anuadmin/moses/working/train/model/extract.sorted.gz /home/anuadmin/moses/working/train/model/lex.f2e /home/anuadmin/moses/working/train/model/phrase-table.half.f2e.gz  0 
Executing: /home/anuadmin/moses/mosesdecoder/scripts/generic/score-parallel.perl 2 "sort    " /home/anuadmin/moses/mosesdecoder/scripts/../bin/score /home/anuadmin/moses/working/train/model/extract.sorted.gz /home/anuadmin/moses/working/train/model/lex.f2e /home/anuadmin/moses/working/train/model/phrase-table.half.f2e.gz  0 
using gzip 
Started Tue Nov  5 13:08:32 2019
/home/anuadmin/moses/mosesdecoder/scripts/../bin/score /home/anuadmin/moses/working/train/model/tmp.26118/extract.0.gz /home/anuadmin/moses/working/train/model/lex.f2e /home/anuadmin/moses/working/train/model/tmp.26118/phrase-table.half.0000000.gz  2>> /dev/stderr 
/home/anuadmin/moses/working/train/model/tmp.26118/run.0.sh/home/anuadmin/moses/working/train/model/tmp.26118/run.1.shmv /home/anuadmin/moses/working/train/model/tmp.26118/phrase-table.half.0000000.gz /home/anuadmin/moses/working/train/model/phrase-table.half.f2e.gzrm -rf /home/anuadmin/moses/working/train/model/tmp.26118 
Finished Tue Nov  5 13:08:38 2019
(6.3)  creating table half /home/anuadmin/moses/working/train/model/phrase-table.half.e2f @ Tue Nov  5 13:08:38 +0530 2019
/home/anuadmin/moses/mosesdecoder/scripts/generic/score-parallel.perl 2 "sort    " /home/anuadmin/moses/mosesdecoder/scripts/../bin/score /home/anuadmin/moses/working/train/model/extract.inv.sorted.gz /home/anuadmin/moses/working/train/model/lex.e2f /home/anuadmin/moses/working/train/model/phrase-table.half.e2f.gz --Inverse 1 
Executing: /home/anuadmin/moses/mosesdecoder/scripts/generic/score-parallel.perl 2 "sort    " /home/anuadmin/moses/mosesdecoder/scripts/../bin/score /home/anuadmin/moses/working/train/model/extract.inv.sorted.gz /home/anuadmin/moses/working/train/model/lex.e2f /home/anuadmin/moses/working/train/model/phrase-table.half.e2f.gz --Inverse 1 
using gzip 
Started Tue Nov  5 13:08:38 2019
/home/anuadmin/moses/mosesdecoder/scripts/../bin/score /home/anuadmin/moses/working/train/model/tmp.26137/extract.0.gz /home/anuadmin/moses/working/train/model/lex.e2f /home/anuadmin/moses/working/train/model/tmp.26137/phrase-table.half.0000000.gz --Inverse  2>> /dev/stderr 
/home/anuadmin/moses/working/train/model/tmp.26137/run.0.sh/home/anuadmin/moses/working/train/model/tmp.26137/run.1.shgunzip -c /home/anuadmin/moses/working/train/model/tmp.26137/phrase-table.half.*.gz 2>> /dev/stderr| LC_ALL=C sort     -T /home/anuadmin/moses/working/train/model/tmp.26137  | gzip -c > /home/anuadmin/moses/working/train/model/phrase-table.half.e2f.gz  2>> /dev/stderr rm -rf /home/anuadmin/moses/working/train/model/tmp.26137 
Finished Tue Nov  5 13:08:44 2019
(6.6) consolidating the two halves @ Tue Nov  5 13:08:44 +0530 2019
Executing: /home/anuadmin/moses/mosesdecoder/scripts/../bin/consolidate /home/anuadmin/moses/working/train/model/phrase-table.half.f2e.gz /home/anuadmin/moses/working/train/model/phrase-table.half.e2f.gz /dev/stdout | gzip -c > /home/anuadmin/moses/working/train/model/phrase-table.gz
Consolidate v2.0 written by Philipp Koehn
consolidating direct and indirect rule tables
..
Executing: rm -f /home/anuadmin/moses/working/train/model/phrase-table.half.*
(7) learn reordering model @ Tue Nov  5 13:08:46 +0530 2019
(7.1) [no factors] learn reordering model @ Tue Nov  5 13:08:46 +0530 2019
(7.2) building tables @ Tue Nov  5 13:08:46 +0530 2019
Executing: /home/anuadmin/moses/mosesdecoder/scripts/../bin/lexical-reordering-score /home/anuadmin/moses/working/train/model/extract.o.sorted.gz 0.5 /home/anuadmin/moses/working/train/model/reordering-table. --model "wbe msd wbe-msd-bidirectional-fe"
Lexical Reordering Scorer
scores lexical reordering models of several types (hierarchical, phrase-based and word-based-extraction
(8) learn generation model @ Tue Nov  5 13:08:48 +0530 2019
  no generation model requested, skipping step
(9) create moses.ini @ Tue Nov  5 13:08:48 +0530 2019
