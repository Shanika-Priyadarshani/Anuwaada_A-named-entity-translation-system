nohup: ignoring input
Using SCRIPTS_ROOTDIR: /home/shanika/mosesdecoder/scripts
Using single-thread GIZA
using gzip 
(1) preparing corpus @ Wed Dec 11 11:02:53 +0530 2019
Executing: mkdir -p /home/shanika/working/train/corpus
(1.0) selecting factors @ Wed Dec 11 11:02:53 +0530 2019
(1.1) running mkcls  @ Wed Dec 11 11:02:53 +0530 2019
/home/shanika/mosesdecoder/tools/mkcls -c50 -n2 -p/home/shanika/corpus/data.si-en.clean.si -V/home/shanika/working/train/corpus/si.vcb.classes opt
Executing: /home/shanika/mosesdecoder/tools/mkcls -c50 -n2 -p/home/shanika/corpus/data.si-en.clean.si -V/home/shanika/working/train/corpus/si.vcb.classes opt

***** 2 runs. (algorithm:TA)*****
;KategProblem:cats: 50   words: 556

start-costs: MEAN: 4.18855e+06 (4.17684e+06-4.20026e+06)  SIGMA:11709.4   
  end-costs: MEAN: 4.01065e+06 (4.00986e+06-4.01143e+06)  SIGMA:782.707   
   start-pp: MEAN: 42.9449 (41.4806-44.4091)  SIGMA:1.46421   
     end-pp: MEAN: 25.5623 (25.504-25.6206)  SIGMA:0.0582804   
 iterations: MEAN: 15318.5 (15304-15333)  SIGMA:14.5   
       time: MEAN: 0.59551 (0.584226-0.606794)  SIGMA:0.011284   
(1.1) running mkcls  @ Wed Dec 11 11:02:55 +0530 2019
/home/shanika/mosesdecoder/tools/mkcls -c50 -n2 -p/home/shanika/corpus/data.si-en.clean.en -V/home/shanika/working/train/corpus/en.vcb.classes opt
Executing: /home/shanika/mosesdecoder/tools/mkcls -c50 -n2 -p/home/shanika/corpus/data.si-en.clean.en -V/home/shanika/working/train/corpus/en.vcb.classes opt
WARNING: StatVar.cc
WARNING: StatVar.cc

***** 2 runs. (algorithm:TA)*****
;KategProblem:cats: 50   words: 29

start-costs: MEAN: 7.67467e+06 (7.61737e+06-7.73197e+06)  SIGMA:57298.5   
  end-costs: MEAN: 7.59744e+06 (7.59744e+06-7.59744e+06)  SIGMA:0   
   start-pp: MEAN: 9.62406 (8.70812-10.54)  SIGMA:0.915935   
     end-pp: MEAN: 8.42367 (8.42367-8.42367)  SIGMA:1.19209e-07   
 iterations: MEAN: 50047.5 (50046-50049)  SIGMA:1.5   
       time: MEAN: 2.2029 (2.19624-2.20956)  SIGMA:0.0066625   
(1.2) creating vcb file /home/shanika/working/train/corpus/si.vcb @ Wed Dec 11 11:02:59 +0530 2019
(1.2) creating vcb file /home/shanika/working/train/corpus/en.vcb @ Wed Dec 11 11:02:59 +0530 2019
(1.3) numberizing corpus /home/shanika/working/train/corpus/si-en-int-train.snt @ Wed Dec 11 11:02:59 +0530 2019
(1.3) numberizing corpus /home/shanika/working/train/corpus/en-si-int-train.snt @ Wed Dec 11 11:03:00 +0530 2019
(2) running giza @ Wed Dec 11 11:03:00 +0530 2019
(2.1a) running snt2cooc si-en @ Wed Dec 11 11:03:00 +0530 2019

Executing: mkdir -p /home/shanika/working/train/giza.si-en
Executing: /home/shanika/mosesdecoder/tools/snt2cooc.out /home/shanika/working/train/corpus/en.vcb /home/shanika/working/train/corpus/si.vcb /home/shanika/working/train/corpus/si-en-int-train.snt > /home/shanika/working/train/giza.si-en/si-en.cooc
/home/shanika/mosesdecoder/tools/snt2cooc.out /home/shanika/working/train/corpus/en.vcb /home/shanika/working/train/corpus/si.vcb /home/shanika/working/train/corpus/si-en-int-train.snt > /home/shanika/working/train/giza.si-en/si-en.cooc
line 1000
line 2000
line 3000
line 4000
line 5000
line 6000
line 7000
line 8000
line 9000
line 10000
line 11000
line 12000
line 13000
line 14000
line 15000
line 16000
line 17000
line 18000
line 19000
line 20000
line 21000
line 22000
line 23000
line 24000
line 25000
line 26000
line 27000
line 28000
line 29000
line 30000
line 31000
line 32000
line 33000
line 34000
line 35000
line 36000
line 37000
line 38000
line 39000
line 40000
line 41000
line 42000
line 43000
line 44000
line 45000
line 46000
line 47000
line 48000
line 49000
line 50000
line 51000
line 52000
line 53000
line 54000
line 55000
line 56000
line 57000
line 58000
line 59000
line 60000
line 61000
line 62000
line 63000
END.
(2.1b) running giza si-en @ Wed Dec 11 11:03:00 +0530 2019
/home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.si-en/si-en.cooc -c /home/shanika/working/train/corpus/si-en-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.si-en/si-en -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/en.vcb -t /home/shanika/working/train/corpus/si.vcb
Executing: /home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.si-en/si-en.cooc -c /home/shanika/working/train/corpus/si-en-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.si-en/si-en -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/en.vcb -t /home/shanika/working/train/corpus/si.vcb
/home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.si-en/si-en.cooc -c /home/shanika/working/train/corpus/si-en-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.si-en/si-en -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/en.vcb -t /home/shanika/working/train/corpus/si.vcb
Parameter 'coocurrencefile' changed from '' to '/home/shanika/working/train/giza.si-en/si-en.cooc'
Parameter 'c' changed from '' to '/home/shanika/working/train/corpus/si-en-int-train.snt'
Parameter 'm3' changed from '5' to '3'
Parameter 'm4' changed from '5' to '3'
Parameter 'model1dumpfrequency' changed from '0' to '1'
Parameter 'model4smoothfactor' changed from '0.2' to '0.4'
Parameter 'nodumps' changed from '0' to '1'
Parameter 'nsmooth' changed from '64' to '4'
Parameter 'o' changed from '119-12-11.110300.shanika' to '/home/shanika/working/train/giza.si-en/si-en'
Parameter 'onlyaldumps' changed from '0' to '1'
Parameter 'p0' changed from '-1' to '0.999'
Parameter 's' changed from '' to '/home/shanika/working/train/corpus/en.vcb'
Parameter 't' changed from '' to '/home/shanika/working/train/corpus/si.vcb'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 119-12-11.110300.shanika.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/shanika/working/train/giza.si-en/si-en  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/shanika/working/train/corpus/si-en-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/shanika/working/train/corpus/en.vcb  (source vocabulary file name)
t = /home/shanika/working/train/corpus/si.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 119-12-11.110300.shanika.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/shanika/working/train/giza.si-en/si-en  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/shanika/working/train/corpus/si-en-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/shanika/working/train/corpus/en.vcb  (source vocabulary file name)
t = /home/shanika/working/train/corpus/si.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Reading vocabulary file from:/home/shanika/working/train/corpus/en.vcb
Reading vocabulary file from:/home/shanika/working/train/corpus/si.vcb
Source vocabulary list has 30 unique tokens 
Target vocabulary list has 557 unique tokens 
Calculating vocabulary frequencies from corpus /home/shanika/working/train/corpus/si-en-int-train.snt
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
 Train total # sentence pairs (weighted): 63300
Size of source portion of the training corpus: 536934 tokens
Size of the target portion of the training corpus: 280002 tokens 
In source portion of the training corpus, only 29 unique tokens appeared
In target portion of the training corpus, only 555 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 280002/(600234-63300)== 0.521483
There are 7984 7984 entries in table
==========================================================
Model1 Training Started at: Wed Dec 11 11:03:01 2019

-----------
Model1: Iteration 1
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (1) TRAIN CROSS-ENTROPY 10.1805 PERPLEXITY 1160.51
Model1: (1) VITERBI TRAIN CROSS-ENTROPY 13.4923 PERPLEXITY 11523.2
Model 1 Iteration: 1 took: 0 seconds
-----------
Model1: Iteration 2
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (2) TRAIN CROSS-ENTROPY 7.23236 PERPLEXITY 150.369
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 9.19247 PERPLEXITY 585.071
Model 1 Iteration: 2 took: 0 seconds
-----------
Model1: Iteration 3
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (3) TRAIN CROSS-ENTROPY 6.81044 PERPLEXITY 112.24
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 8.18196 PERPLEXITY 290.412
Model 1 Iteration: 3 took: 0 seconds
-----------
Model1: Iteration 4
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (4) TRAIN CROSS-ENTROPY 6.59568 PERPLEXITY 96.7159
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 7.70531 PERPLEXITY 208.703
Model 1 Iteration: 4 took: 1 seconds
-----------
Model1: Iteration 5
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (5) TRAIN CROSS-ENTROPY 6.49908 PERPLEXITY 90.452
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 7.48052 PERPLEXITY 178.591
Model 1 Iteration: 5 took: 0 seconds
Entire Model1 Training took: 1 seconds
NOTE: I am doing iterations with the HMM model!
Read classes: #words: 29  #classes: 29
Read classes: #words: 556  #classes: 51

==========================================================
Hmm Training Started at: Wed Dec 11 11:03:02 2019

-----------
Hmm: Iteration 1
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 1879 parameters.
Hmm: (1) TRAIN CROSS-ENTROPY 6.45392 PERPLEXITY 87.6647
Hmm: (1) VITERBI TRAIN CROSS-ENTROPY 7.36506 PERPLEXITY 164.856

Hmm Iteration: 1 took: 2 seconds

-----------
Hmm: Iteration 2
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 1879 parameters.
Hmm: (2) TRAIN CROSS-ENTROPY 5.18341 PERPLEXITY 36.338
Hmm: (2) VITERBI TRAIN CROSS-ENTROPY 5.51697 PERPLEXITY 45.7902

Hmm Iteration: 2 took: 2 seconds

-----------
Hmm: Iteration 3
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 1879 parameters.
Hmm: (3) TRAIN CROSS-ENTROPY 4.5192 PERPLEXITY 22.9306
Hmm: (3) VITERBI TRAIN CROSS-ENTROPY 4.71769 PERPLEXITY 26.3127

Hmm Iteration: 3 took: 2 seconds

-----------
Hmm: Iteration 4
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 1879 parameters.
Hmm: (4) TRAIN CROSS-ENTROPY 4.33528 PERPLEXITY 20.186
Hmm: (4) VITERBI TRAIN CROSS-ENTROPY 4.48705 PERPLEXITY 22.4253

Hmm Iteration: 4 took: 2 seconds

-----------
Hmm: Iteration 5
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 1879 parameters.
Hmm: (5) TRAIN CROSS-ENTROPY 4.26778 PERPLEXITY 19.2633
Hmm: (5) VITERBI TRAIN CROSS-ENTROPY 4.39698 PERPLEXITY 21.068

Hmm Iteration: 5 took: 1 seconds

Entire Hmm Training took: 9 seconds
==========================================================
Read classes: #words: 29  #classes: 29
Read classes: #words: 556  #classes: 51
Read classes: #words: 29  #classes: 29
Read classes: #words: 556  #classes: 51

==========================================================
Starting H333444:  Viterbi Training
 H333444 Training Started at: Wed Dec 11 11:03:11 2019


---------------------
THTo3: Iteration 1
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
60000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 50.7662 #alsophisticatedcountcollection: 0 #hcsteps: 0
#peggingImprovements: 0
A/D table contains 1879 parameters.
A/D table contains 966 parameters.
NTable contains 300 parameter.
p0_count is 267966 and p1 is 5993.33; p0 is 0.999 p1: 0.001
THTo3: TRAIN CROSS-ENTROPY 3.19356 PERPLEXITY 9.14865
THTo3: (1) TRAIN VITERBI CROSS-ENTROPY 3.29221 PERPLEXITY 9.79615

THTo3 Viterbi Iteration : 1 took: 2 seconds

---------------------
Model3: Iteration 2
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
60000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 50.7787 #alsophisticatedcountcollection: 0 #hcsteps: 1.41469
#peggingImprovements: 0
A/D table contains 1879 parameters.
A/D table contains 966 parameters.
NTable contains 300 parameter.
p0_count is 278379 and p1 is 811.404; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 4.12699 PERPLEXITY 17.4722
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 4.18253 PERPLEXITY 18.158

Model3 Viterbi Iteration : 2 took: 2 seconds

---------------------
Model3: Iteration 3
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
60000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 50.7796 #alsophisticatedcountcollection: 0 #hcsteps: 1.32701
#peggingImprovements: 0
A/D table contains 1879 parameters.
A/D table contains 966 parameters.
NTable contains 300 parameter.
p0_count is 279448 and p1 is 277.237; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 3.86576 PERPLEXITY 14.5783
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 3.88374 PERPLEXITY 14.7612

Model3 Viterbi Iteration : 3 took: 1 seconds

---------------------
T3To4: Iteration 4
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
60000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 50.7807 #alsophisticatedcountcollection: 4.70588 #hcsteps: 1.20957
#peggingImprovements: 0
D4 table contains 238728 parameters.
A/D table contains 1879 parameters.
A/D table contains 966 parameters.
NTable contains 300 parameter.
p0_count is 279419 and p1 is 291.568; p0 is 0.999 p1: 0.001
T3To4: TRAIN CROSS-ENTROPY 3.78022 PERPLEXITY 13.7391
T3To4: (4) TRAIN VITERBI CROSS-ENTROPY 3.79061 PERPLEXITY 13.8384

T3To4 Viterbi Iteration : 4 took: 2 seconds

---------------------
Model4: Iteration 5
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
60000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 50.7799 #alsophisticatedcountcollection: 3.37583 #hcsteps: 1.09735
#peggingImprovements: 0
D4 table contains 238728 parameters.
A/D table contains 1879 parameters.
A/D table contains 969 parameters.
NTable contains 300 parameter.
p0_count is 279498 and p1 is 251.88; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 4.00172 PERPLEXITY 16.0191
Model4: (5) TRAIN VITERBI CROSS-ENTROPY 4.01469 PERPLEXITY 16.1638

Model4 Viterbi Iteration : 5 took: 3 seconds

---------------------
Model4: Iteration 6
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
60000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 50.7798 #alsophisticatedcountcollection: 3.12351 #hcsteps: 1.06275
#peggingImprovements: 0
D4 table contains 238728 parameters.
A/D table contains 1879 parameters.
A/D table contains 971 parameters.
NTable contains 300 parameter.
p0_count is 279514 and p1 is 244.108; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 3.99104 PERPLEXITY 15.901
Model4: (6) TRAIN VITERBI CROSS-ENTROPY 4.00422 PERPLEXITY 16.0469

Model4 Viterbi Iteration : 6 took: 3 seconds
H333444 Training Finished at: Wed Dec 11 11:03:24 2019


Entire Viterbi H333444 Training took: 13 seconds
==========================================================

Entire Training took: 24 seconds
Program Finished at: Wed Dec 11 11:03:24 2019

==========================================================
Executing: rm -f /home/shanika/working/train/giza.si-en/si-en.A3.final.gz
Executing: gzip /home/shanika/working/train/giza.si-en/si-en.A3.final
(2.1a) running snt2cooc en-si @ Wed Dec 11 11:03:24 +0530 2019

Executing: mkdir -p /home/shanika/working/train/giza.en-si
Executing: /home/shanika/mosesdecoder/tools/snt2cooc.out /home/shanika/working/train/corpus/si.vcb /home/shanika/working/train/corpus/en.vcb /home/shanika/working/train/corpus/en-si-int-train.snt > /home/shanika/working/train/giza.en-si/en-si.cooc
/home/shanika/mosesdecoder/tools/snt2cooc.out /home/shanika/working/train/corpus/si.vcb /home/shanika/working/train/corpus/en.vcb /home/shanika/working/train/corpus/en-si-int-train.snt > /home/shanika/working/train/giza.en-si/en-si.cooc
line 1000
line 2000
line 3000
line 4000
line 5000
line 6000
line 7000
line 8000
line 9000
line 10000
line 11000
line 12000
line 13000
line 14000
line 15000
line 16000
line 17000
line 18000
line 19000
line 20000
line 21000
line 22000
line 23000
line 24000
line 25000
line 26000
line 27000
line 28000
line 29000
line 30000
line 31000
line 32000
line 33000
line 34000
line 35000
line 36000
line 37000
line 38000
line 39000
line 40000
line 41000
line 42000
line 43000
line 44000
line 45000
line 46000
line 47000
line 48000
line 49000
line 50000
line 51000
line 52000
line 53000
line 54000
line 55000
line 56000
line 57000
line 58000
line 59000
line 60000
line 61000
line 62000
line 63000
END.
(2.1b) running giza en-si @ Wed Dec 11 11:03:24 +0530 2019
/home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.en-si/en-si.cooc -c /home/shanika/working/train/corpus/en-si-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.en-si/en-si -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/si.vcb -t /home/shanika/working/train/corpus/en.vcb
Executing: /home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.en-si/en-si.cooc -c /home/shanika/working/train/corpus/en-si-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.en-si/en-si -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/si.vcb -t /home/shanika/working/train/corpus/en.vcb
/home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.en-si/en-si.cooc -c /home/shanika/working/train/corpus/en-si-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.en-si/en-si -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/si.vcb -t /home/shanika/working/train/corpus/en.vcb
Parameter 'coocurrencefile' changed from '' to '/home/shanika/working/train/giza.en-si/en-si.cooc'
Parameter 'c' changed from '' to '/home/shanika/working/train/corpus/en-si-int-train.snt'
Parameter 'm3' changed from '5' to '3'
Parameter 'm4' changed from '5' to '3'
Parameter 'model1dumpfrequency' changed from '0' to '1'
Parameter 'model4smoothfactor' changed from '0.2' to '0.4'
Parameter 'nodumps' changed from '0' to '1'
Parameter 'nsmooth' changed from '64' to '4'
Parameter 'o' changed from '119-12-11.110324.shanika' to '/home/shanika/working/train/giza.en-si/en-si'
Parameter 'onlyaldumps' changed from '0' to '1'
Parameter 'p0' changed from '-1' to '0.999'
Parameter 's' changed from '' to '/home/shanika/working/train/corpus/si.vcb'
Parameter 't' changed from '' to '/home/shanika/working/train/corpus/en.vcb'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 119-12-11.110324.shanika.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/shanika/working/train/giza.en-si/en-si  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/shanika/working/train/corpus/en-si-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/shanika/working/train/corpus/si.vcb  (source vocabulary file name)
t = /home/shanika/working/train/corpus/en.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 119-12-11.110324.shanika.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/shanika/working/train/giza.en-si/en-si  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/shanika/working/train/corpus/en-si-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/shanika/working/train/corpus/si.vcb  (source vocabulary file name)
t = /home/shanika/working/train/corpus/en.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Reading vocabulary file from:/home/shanika/working/train/corpus/si.vcb
Reading vocabulary file from:/home/shanika/working/train/corpus/en.vcb
Source vocabulary list has 557 unique tokens 
Target vocabulary list has 30 unique tokens 
Calculating vocabulary frequencies from corpus /home/shanika/working/train/corpus/en-si-int-train.snt
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
 Train total # sentence pairs (weighted): 63300
Size of source portion of the training corpus: 280002 tokens
Size of the target portion of the training corpus: 536934 tokens 
In source portion of the training corpus, only 556 unique tokens appeared
In target portion of the training corpus, only 28 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 536934/(343302-63300)== 1.91761
There are 7457 7457 entries in table
==========================================================
Model1 Training Started at: Wed Dec 11 11:03:24 2019

-----------
Model1: Iteration 1
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (1) TRAIN CROSS-ENTROPY 5.44398 PERPLEXITY 43.5314
Model1: (1) VITERBI TRAIN CROSS-ENTROPY 7.9519 PERPLEXITY 247.606
Model 1 Iteration: 1 took: 1 seconds
-----------
Model1: Iteration 2
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (2) TRAIN CROSS-ENTROPY 3.98052 PERPLEXITY 15.7854
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 5.60836 PERPLEXITY 48.7849
Model 1 Iteration: 2 took: 0 seconds
-----------
Model1: Iteration 3
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (3) TRAIN CROSS-ENTROPY 3.67305 PERPLEXITY 12.7555
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 4.90715 PERPLEXITY 30.0054
Model 1 Iteration: 3 took: 0 seconds
-----------
Model1: Iteration 4
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (4) TRAIN CROSS-ENTROPY 3.5123 PERPLEXITY 11.4105
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 4.51572 PERPLEXITY 22.8753
Model 1 Iteration: 4 took: 0 seconds
-----------
Model1: Iteration 5
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (5) TRAIN CROSS-ENTROPY 3.4321 PERPLEXITY 10.7936
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 4.27114 PERPLEXITY 19.3081
Model 1 Iteration: 5 took: 1 seconds
Entire Model1 Training took: 2 seconds
NOTE: I am doing iterations with the HMM model!
Read classes: #words: 556  #classes: 51
Read classes: #words: 29  #classes: 29

==========================================================
Hmm Training Started at: Wed Dec 11 11:03:26 2019

-----------
Hmm: Iteration 1
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 1079 parameters.
Hmm: (1) TRAIN CROSS-ENTROPY 3.39041 PERPLEXITY 10.4862
Hmm: (1) VITERBI TRAIN CROSS-ENTROPY 4.1257 PERPLEXITY 17.4566

Hmm Iteration: 1 took: 1 seconds

-----------
Hmm: Iteration 2
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 1079 parameters.
Hmm: (2) TRAIN CROSS-ENTROPY 2.78421 PERPLEXITY 6.88861
Hmm: (2) VITERBI TRAIN CROSS-ENTROPY 3.15014 PERPLEXITY 8.87743

Hmm Iteration: 2 took: 1 seconds

-----------
Hmm: Iteration 3
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 1079 parameters.
Hmm: (3) TRAIN CROSS-ENTROPY 2.41231 PERPLEXITY 5.32328
Hmm: (3) VITERBI TRAIN CROSS-ENTROPY 2.65973 PERPLEXITY 6.31913

Hmm Iteration: 3 took: 1 seconds

-----------
Hmm: Iteration 4
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 1079 parameters.
Hmm: (4) TRAIN CROSS-ENTROPY 2.27915 PERPLEXITY 4.85393
Hmm: (4) VITERBI TRAIN CROSS-ENTROPY 2.48675 PERPLEXITY 5.60514

Hmm Iteration: 4 took: 1 seconds

-----------
Hmm: Iteration 5
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 1079 parameters.
Hmm: (5) TRAIN CROSS-ENTROPY 2.24714 PERPLEXITY 4.74741
Hmm: (5) VITERBI TRAIN CROSS-ENTROPY 2.4365 PERPLEXITY 5.41329

Hmm Iteration: 5 took: 1 seconds

Entire Hmm Training took: 5 seconds
==========================================================
Read classes: #words: 556  #classes: 51
Read classes: #words: 29  #classes: 29
Read classes: #words: 556  #classes: 51
Read classes: #words: 29  #classes: 29

==========================================================
Starting H333444:  Viterbi Training
 H333444 Training Started at: Wed Dec 11 11:03:31 2019


---------------------
THTo3: Iteration 1
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
60000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 73.6913 #alsophisticatedcountcollection: 0 #hcsteps: 0
#peggingImprovements: 0
A/D table contains 1079 parameters.
A/D table contains 1951 parameters.
NTable contains 5570 parameter.
p0_count is 343587 and p1 is 96634.4; p0 is 0.999 p1: 0.001
THTo3: TRAIN CROSS-ENTROPY 1.74628 PERPLEXITY 3.35493
THTo3: (1) TRAIN VITERBI CROSS-ENTROPY 1.87879 PERPLEXITY 3.67766

THTo3 Viterbi Iteration : 1 took: 1 seconds

---------------------
Model3: Iteration 2
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
60000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 73.5126 #alsophisticatedcountcollection: 0 #hcsteps: 2.52673
#peggingImprovements: 0
A/D table contains 1079 parameters.
A/D table contains 1950 parameters.
NTable contains 5570 parameter.
p0_count is 482165 and p1 is 27384.4; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 3.01209 PERPLEXITY 8.06733
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 3.10456 PERPLEXITY 8.60131

Model3 Viterbi Iteration : 2 took: 1 seconds

---------------------
Model3: Iteration 3
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
60000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 73.3851 #alsophisticatedcountcollection: 0 #hcsteps: 2.52844
#peggingImprovements: 0
A/D table contains 1079 parameters.
A/D table contains 1950 parameters.
NTable contains 5570 parameter.
p0_count is 512907 and p1 is 12013.6; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 2.60192 PERPLEXITY 6.07093
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 2.64415 PERPLEXITY 6.25127

Model3 Viterbi Iteration : 3 took: 2 seconds

---------------------
T3To4: Iteration 4
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
60000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 73.4101 #alsophisticatedcountcollection: 8.94351 #hcsteps: 2.59336
#peggingImprovements: 0
D4 table contains 210105 parameters.
A/D table contains 1079 parameters.
A/D table contains 1950 parameters.
NTable contains 5570 parameter.
p0_count is 518812 and p1 is 9061.15; p0 is 0.999 p1: 0.001
T3To4: TRAIN CROSS-ENTROPY 2.49792 PERPLEXITY 5.64869
T3To4: (4) TRAIN VITERBI CROSS-ENTROPY 2.52675 PERPLEXITY 5.76272

T3To4 Viterbi Iteration : 4 took: 1 seconds

---------------------
Model4: Iteration 5
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
60000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 73.3587 #alsophisticatedcountcollection: 7.10227 #hcsteps: 2.66995
#peggingImprovements: 0
D4 table contains 210105 parameters.
A/D table contains 1079 parameters.
A/D table contains 1950 parameters.
NTable contains 5570 parameter.
p0_count is 522785 and p1 is 7074.29; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 2.16311 PERPLEXITY 4.47878
Model4: (5) TRAIN VITERBI CROSS-ENTROPY 2.17961 PERPLEXITY 4.53031

Model4 Viterbi Iteration : 5 took: 5 seconds

---------------------
Model4: Iteration 6
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
60000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 73.3545 #alsophisticatedcountcollection: 5.87886 #hcsteps: 2.65801
#peggingImprovements: 0
D4 table contains 210105 parameters.
A/D table contains 1079 parameters.
A/D table contains 1950 parameters.
NTable contains 5570 parameter.
p0_count is 523666 and p1 is 6633.96; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 2.12343 PERPLEXITY 4.3573
Model4: (6) TRAIN VITERBI CROSS-ENTROPY 2.13646 PERPLEXITY 4.39682

Model4 Viterbi Iteration : 6 took: 4 seconds
H333444 Training Finished at: Wed Dec 11 11:03:45 2019


Entire Viterbi H333444 Training took: 14 seconds
==========================================================

Entire Training took: 21 seconds
Program Finished at: Wed Dec 11 11:03:45 2019

==========================================================
Executing: rm -f /home/shanika/working/train/giza.en-si/en-si.A3.final.gz
Executing: gzip /home/shanika/working/train/giza.en-si/en-si.A3.final
(3) generate word alignment @ Wed Dec 11 11:03:46 +0530 2019
Combining forward and inverted alignment from files:
  /home/shanika/working/train/giza.si-en/si-en.A3.final.{bz2,gz}
  /home/shanika/working/train/giza.en-si/en-si.A3.final.{bz2,gz}
Executing: mkdir -p /home/shanika/working/train/model
Executing: /home/shanika/mosesdecoder/scripts/training/giza2bal.pl -d "gzip -cd /home/shanika/working/train/giza.en-si/en-si.A3.final.gz" -i "gzip -cd /home/shanika/working/train/giza.si-en/si-en.A3.final.gz" |/home/shanika/mosesdecoder/scripts/../bin/symal -alignment="grow" -diagonal="yes" -final="yes" -both="yes" > /home/shanika/working/train/model/aligned.grow-diag-final-and
symal: computing grow alignment: diagonal (1) final (1)both-uncovered (1)
skip=<0> counts=<63300>
(4) generate lexical translation table 0-0 @ Wed Dec 11 11:03:48 +0530 2019
(/home/shanika/corpus/data.si-en.clean.si,/home/shanika/corpus/data.si-en.clean.en,/home/shanika/working/train/model/lex)
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
Saved: /home/shanika/working/train/model/lex.f2e and /home/shanika/working/train/model/lex.e2f
FILE: /home/shanika/corpus/data.si-en.clean.en
FILE: /home/shanika/corpus/data.si-en.clean.si
FILE: /home/shanika/working/train/model/aligned.grow-diag-final-and
(5) extract phrases @ Wed Dec 11 11:03:49 +0530 2019
/home/shanika/mosesdecoder/scripts/generic/extract-parallel.perl 4 split "sort    " /home/shanika/mosesdecoder/scripts/../bin/extract /home/shanika/corpus/data.si-en.clean.en /home/shanika/corpus/data.si-en.clean.si /home/shanika/working/train/model/aligned.grow-diag-final-and /home/shanika/working/train/model/extract 7 orientation --model wbe-msd --GZOutput 
Executing: /home/shanika/mosesdecoder/scripts/generic/extract-parallel.perl 4 split "sort    " /home/shanika/mosesdecoder/scripts/../bin/extract /home/shanika/corpus/data.si-en.clean.en /home/shanika/corpus/data.si-en.clean.si /home/shanika/working/train/model/aligned.grow-diag-final-and /home/shanika/working/train/model/extract 7 orientation --model wbe-msd --GZOutput 
MAX 7 1 0
Started Wed Dec 11 11:03:50 2019
using gzip 
isBSDSplit=0 
Executing: mkdir -p /home/shanika/working/train/model/tmp.6709; ls -l /home/shanika/working/train/model/tmp.6709 
total=63300 line-per-split=15826 
split -d -l 15826 -a 7 /home/shanika/corpus/data.si-en.clean.si /home/shanika/working/train/model/tmp.6709/source.split -d -l 15826 -a 7 /home/shanika/working/train/model/aligned.grow-diag-final-and /home/shanika/working/train/model/tmp.6709/align.split -d -l 15826 -a 7 /home/shanika/corpus/data.si-en.clean.en /home/shanika/working/train/model/tmp.6709/target.merging extract / extract.inv
gunzip -c /home/shanika/working/train/model/tmp.6709/extract.0000000.gz /home/shanika/working/train/model/tmp.6709/extract.0000001.gz /home/shanika/working/train/model/tmp.6709/extract.0000002.gz /home/shanika/working/train/model/tmp.6709/extract.0000003.gz  | LC_ALL=C sort     -T /home/shanika/working/train/model/tmp.6709 2>> /dev/stderr | gzip -c > /home/shanika/working/train/model/extract.sorted.gz 2>> /dev/stderr 
gunzip -c /home/shanika/working/train/model/tmp.6709/extract.0000000.inv.gz /home/shanika/working/train/model/tmp.6709/extract.0000001.inv.gz /home/shanika/working/train/model/tmp.6709/extract.0000002.inv.gz /home/shanika/working/train/model/tmp.6709/extract.0000003.inv.gz  | LC_ALL=C sort     -T /home/shanika/working/train/model/tmp.6709 2>> /dev/stderr | gzip -c > /home/shanika/working/train/model/extract.inv.sorted.gz 2>> /dev/stderr 
gunzip -c /home/shanika/working/train/model/tmp.6709/extract.0000000.o.gz /home/shanika/working/train/model/tmp.6709/extract.0000001.o.gz /home/shanika/working/train/model/tmp.6709/extract.0000002.o.gz /home/shanika/working/train/model/tmp.6709/extract.0000003.o.gz  | LC_ALL=C sort     -T /home/shanika/working/train/model/tmp.6709 2>> /dev/stderr | gzip -c > /home/shanika/working/train/model/extract.o.sorted.gz 2>> /dev/stderr 
Finished Wed Dec 11 11:03:54 2019
(6) score phrases @ Wed Dec 11 11:03:54 +0530 2019
(6.1)  creating table half /home/shanika/working/train/model/phrase-table.half.f2e @ Wed Dec 11 11:03:54 +0530 2019
/home/shanika/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/extract.sorted.gz /home/shanika/working/train/model/lex.f2e /home/shanika/working/train/model/phrase-table.half.f2e.gz  0 
Executing: /home/shanika/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/extract.sorted.gz /home/shanika/working/train/model/lex.f2e /home/shanika/working/train/model/phrase-table.half.f2e.gz  0 
using gzip 
Started Wed Dec 11 11:03:54 2019
/home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/tmp.6761/extract.0.gz /home/shanika/working/train/model/lex.f2e /home/shanika/working/train/model/tmp.6761/phrase-table.half.0000000.gz  2>> /dev/stderr 
/home/shanika/working/train/model/tmp.6761/run.0.sh/home/shanika/working/train/model/tmp.6761/run.1.sh/home/shanika/working/train/model/tmp.6761/run.2.sh/home/shanika/working/train/model/tmp.6761/run.3.shmv /home/shanika/working/train/model/tmp.6761/phrase-table.half.0000000.gz /home/shanika/working/train/model/phrase-table.half.f2e.gzrm -rf /home/shanika/working/train/model/tmp.6761 
Finished Wed Dec 11 11:03:55 2019
(6.3)  creating table half /home/shanika/working/train/model/phrase-table.half.e2f @ Wed Dec 11 11:03:56 +0530 2019
/home/shanika/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/extract.inv.sorted.gz /home/shanika/working/train/model/lex.e2f /home/shanika/working/train/model/phrase-table.half.e2f.gz --Inverse 1 
Executing: /home/shanika/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/extract.inv.sorted.gz /home/shanika/working/train/model/lex.e2f /home/shanika/working/train/model/phrase-table.half.e2f.gz --Inverse 1 
using gzip 
Started Wed Dec 11 11:03:56 2019
/home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/tmp.6791/extract.0.gz /home/shanika/working/train/model/lex.e2f /home/shanika/working/train/model/tmp.6791/phrase-table.half.0000000.gz --Inverse  2>> /dev/stderr 
/home/shanika/working/train/model/tmp.6791/run.0.sh/home/shanika/working/train/model/tmp.6791/run.1.sh/home/shanika/working/train/model/tmp.6791/run.2.sh/home/shanika/working/train/model/tmp.6791/run.3.shgunzip -c /home/shanika/working/train/model/tmp.6791/phrase-table.half.*.gz 2>> /dev/stderr| LC_ALL=C sort     -T /home/shanika/working/train/model/tmp.6791  | gzip -c > /home/shanika/working/train/model/phrase-table.half.e2f.gz  2>> /dev/stderr rm -rf /home/shanika/working/train/model/tmp.6791 
Finished Wed Dec 11 11:03:56 2019
(6.6) consolidating the two halves @ Wed Dec 11 11:03:56 +0530 2019
Executing: /home/shanika/mosesdecoder/scripts/../bin/consolidate /home/shanika/working/train/model/phrase-table.half.f2e.gz /home/shanika/working/train/model/phrase-table.half.e2f.gz /dev/stdout | gzip -c > /home/shanika/working/train/model/phrase-table.gz
Consolidate v2.0 written by Philipp Koehn
consolidating direct and indirect rule tables

Executing: rm -f /home/shanika/working/train/model/phrase-table.half.*
(7) learn reordering model @ Wed Dec 11 11:03:57 +0530 2019
(7.1) [no factors] learn reordering model @ Wed Dec 11 11:03:57 +0530 2019
(7.2) building tables @ Wed Dec 11 11:03:57 +0530 2019
Executing: /home/shanika/mosesdecoder/scripts/../bin/lexical-reordering-score /home/shanika/working/train/model/extract.o.sorted.gz 0.5 /home/shanika/working/train/model/reordering-table. --model "wbe msd wbe-msd-bidirectional-fe"
Lexical Reordering Scorer
scores lexical reordering models of several types (hierarchical, phrase-based and word-based-extraction
(8) learn generation model @ Wed Dec 11 11:03:57 +0530 2019
  no generation model requested, skipping step
(9) create moses.ini @ Wed Dec 11 11:03:57 +0530 2019
