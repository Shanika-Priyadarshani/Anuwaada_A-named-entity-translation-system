nohup: ignoring input
Using SCRIPTS_ROOTDIR: /home/shanika/mosesdecoder/scripts
Using single-thread GIZA
using gzip 
(1) preparing corpus @ Sun Nov  3 14:09:02 +0530 2019
Executing: mkdir -p /home/shanika/working/train/corpus
(1.0) selecting factors @ Sun Nov  3 14:09:02 +0530 2019
(1.1) running mkcls  @ Sun Nov  3 14:09:02 +0530 2019
/home/shanika/mosesdecoder/tools/mkcls -c50 -n2 -p/home/shanika/corpus/data.si-ta.clean.si -V/home/shanika/working/train/corpus/si.vcb.classes opt
Executing: /home/shanika/mosesdecoder/tools/mkcls -c50 -n2 -p/home/shanika/corpus/data.si-ta.clean.si -V/home/shanika/working/train/corpus/si.vcb.classes opt

***** 2 runs. (algorithm:TA)*****
;KategProblem:cats: 50   words: 354

start-costs: MEAN: 7.3578e+06 (7.35684e+06-7.35875e+06)  SIGMA:955.761   
  end-costs: MEAN: 7.06918e+06 (7.06871e+06-7.06965e+06)  SIGMA:467.97   
   start-pp: MEAN: 32.6624 (32.6089-32.7159)  SIGMA:0.0535367   
     end-pp: MEAN: 19.9105 (19.8945-19.9265)  SIGMA:0.0159792   
 iterations: MEAN: 10267.5 (10151-10384)  SIGMA:116.5   
       time: MEAN: 0.614191 (0.60025-0.628133)  SIGMA:0.0139415   
(1.1) running mkcls  @ Sun Nov  3 14:09:03 +0530 2019
/home/shanika/mosesdecoder/tools/mkcls -c50 -n2 -p/home/shanika/corpus/data.si-ta.clean.ta -V/home/shanika/working/train/corpus/ta.vcb.classes opt
Executing: /home/shanika/mosesdecoder/tools/mkcls -c50 -n2 -p/home/shanika/corpus/data.si-ta.clean.ta -V/home/shanika/working/train/corpus/ta.vcb.classes opt

***** 2 runs. (algorithm:TA)*****
;KategProblem:cats: 50   words: 251

start-costs: MEAN: 7.96344e+06 (7.95899e+06-7.96789e+06)  SIGMA:4453.72   
  end-costs: MEAN: 7.54712e+06 (7.547e+06-7.54725e+06)  SIGMA:124.039   
   start-pp: MEAN: 32.0309 (31.8049-32.2568)  SIGMA:0.225962   
     end-pp: MEAN: 16.564 (16.5608-16.5673)  SIGMA:0.00325443   
 iterations: MEAN: 7819.5 (7675-7964)  SIGMA:144.5   
       time: MEAN: 0.559065 (0.554673-0.563456)  SIGMA:0.0043915   
(1.2) creating vcb file /home/shanika/working/train/corpus/si.vcb @ Sun Nov  3 14:09:04 +0530 2019
(1.2) creating vcb file /home/shanika/working/train/corpus/ta.vcb @ Sun Nov  3 14:09:04 +0530 2019
(1.3) numberizing corpus /home/shanika/working/train/corpus/si-ta-int-train.snt @ Sun Nov  3 14:09:05 +0530 2019
(1.3) numberizing corpus /home/shanika/working/train/corpus/ta-si-int-train.snt @ Sun Nov  3 14:09:05 +0530 2019
(2) running giza @ Sun Nov  3 14:09:06 +0530 2019
(2.1a) running snt2cooc si-ta @ Sun Nov  3 14:09:06 +0530 2019

Executing: mkdir -p /home/shanika/working/train/giza.si-ta
Executing: /home/shanika/mosesdecoder/tools/snt2cooc.out /home/shanika/working/train/corpus/ta.vcb /home/shanika/working/train/corpus/si.vcb /home/shanika/working/train/corpus/si-ta-int-train.snt > /home/shanika/working/train/giza.si-ta/si-ta.cooc
/home/shanika/mosesdecoder/tools/snt2cooc.out /home/shanika/working/train/corpus/ta.vcb /home/shanika/working/train/corpus/si.vcb /home/shanika/working/train/corpus/si-ta-int-train.snt > /home/shanika/working/train/giza.si-ta/si-ta.cooc
line 1000
line 2000
line 3000
line 4000
line 5000
line 6000
line 7000
line 8000
line 9000
line 10000
line 11000
line 12000
line 13000
line 14000
line 15000
line 16000
line 17000
line 18000
line 19000
line 20000
line 21000
line 22000
line 23000
line 24000
line 25000
line 26000
line 27000
line 28000
line 29000
line 30000
line 31000
line 32000
line 33000
line 34000
line 35000
line 36000
line 37000
line 38000
line 39000
line 40000
line 41000
line 42000
line 43000
line 44000
line 45000
line 46000
line 47000
line 48000
line 49000
line 50000
line 51000
line 52000
line 53000
line 54000
line 55000
line 56000
line 57000
line 58000
line 59000
line 60000
line 61000
line 62000
line 63000
line 64000
line 65000
line 66000
line 67000
line 68000
line 69000
line 70000
line 71000
line 72000
line 73000
line 74000
line 75000
END.
(2.1b) running giza si-ta @ Sun Nov  3 14:09:06 +0530 2019
/home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.si-ta/si-ta.cooc -c /home/shanika/working/train/corpus/si-ta-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.si-ta/si-ta -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/ta.vcb -t /home/shanika/working/train/corpus/si.vcb
Executing: /home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.si-ta/si-ta.cooc -c /home/shanika/working/train/corpus/si-ta-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.si-ta/si-ta -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/ta.vcb -t /home/shanika/working/train/corpus/si.vcb
/home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.si-ta/si-ta.cooc -c /home/shanika/working/train/corpus/si-ta-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.si-ta/si-ta -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/ta.vcb -t /home/shanika/working/train/corpus/si.vcb
Parameter 'coocurrencefile' changed from '' to '/home/shanika/working/train/giza.si-ta/si-ta.cooc'
Parameter 'c' changed from '' to '/home/shanika/working/train/corpus/si-ta-int-train.snt'
Parameter 'm3' changed from '5' to '3'
Parameter 'm4' changed from '5' to '3'
Parameter 'model1dumpfrequency' changed from '0' to '1'
Parameter 'model4smoothfactor' changed from '0.2' to '0.4'
Parameter 'nodumps' changed from '0' to '1'
Parameter 'nsmooth' changed from '64' to '4'
Parameter 'o' changed from '119-11-03.140906.shanika' to '/home/shanika/working/train/giza.si-ta/si-ta'
Parameter 'onlyaldumps' changed from '0' to '1'
Parameter 'p0' changed from '-1' to '0.999'
Parameter 's' changed from '' to '/home/shanika/working/train/corpus/ta.vcb'
Parameter 't' changed from '' to '/home/shanika/working/train/corpus/si.vcb'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 119-11-03.140906.shanika.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/shanika/working/train/giza.si-ta/si-ta  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/shanika/working/train/corpus/si-ta-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/shanika/working/train/corpus/ta.vcb  (source vocabulary file name)
t = /home/shanika/working/train/corpus/si.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 119-11-03.140906.shanika.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/shanika/working/train/giza.si-ta/si-ta  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/shanika/working/train/corpus/si-ta-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/shanika/working/train/corpus/ta.vcb  (source vocabulary file name)
t = /home/shanika/working/train/corpus/si.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Reading vocabulary file from:/home/shanika/working/train/corpus/ta.vcb
Reading vocabulary file from:/home/shanika/working/train/corpus/si.vcb
Source vocabulary list has 252 unique tokens 
Target vocabulary list has 355 unique tokens 
Calculating vocabulary frequencies from corpus /home/shanika/working/train/corpus/si-ta-int-train.snt
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
 Train total # sentence pairs (weighted): 75000
Size of source portion of the training corpus: 556318 tokens
Size of the target portion of the training corpus: 508103 tokens 
In source portion of the training corpus, only 251 unique tokens appeared
In target portion of the training corpus, only 353 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 508103/(631318-75000)== 0.913332
There are 24960 24960 entries in table
==========================================================
Model1 Training Started at: Sun Nov  3 14:09:06 2019

-----------
Model1: Iteration 1
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (1) TRAIN CROSS-ENTROPY 8.9185 PERPLEXITY 483.878
Model1: (1) VITERBI TRAIN CROSS-ENTROPY 12.0214 PERPLEXITY 4157.26
Model 1 Iteration: 1 took: 1 seconds
-----------
Model1: Iteration 2
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (2) TRAIN CROSS-ENTROPY 5.56059 PERPLEXITY 47.196
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 6.97529 PERPLEXITY 125.826
Model 1 Iteration: 2 took: 0 seconds
-----------
Model1: Iteration 3
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (3) TRAIN CROSS-ENTROPY 4.67126 PERPLEXITY 25.4794
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 5.48958 PERPLEXITY 44.9293
Model 1 Iteration: 3 took: 0 seconds
-----------
Model1: Iteration 4
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (4) TRAIN CROSS-ENTROPY 4.36014 PERPLEXITY 20.5368
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 4.96222 PERPLEXITY 31.1729
Model 1 Iteration: 4 took: 1 seconds
-----------
Model1: Iteration 5
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (5) TRAIN CROSS-ENTROPY 4.26193 PERPLEXITY 19.1853
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 4.76694 PERPLEXITY 27.2265
Model 1 Iteration: 5 took: 0 seconds
Entire Model1 Training took: 2 seconds
NOTE: I am doing iterations with the HMM model!
Read classes: #words: 251  #classes: 51
Read classes: #words: 354  #classes: 51

==========================================================
Hmm Training Started at: Sun Nov  3 14:09:08 2019

-----------
Hmm: Iteration 1
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 2860 parameters.
Hmm: (1) TRAIN CROSS-ENTROPY 4.22423 PERPLEXITY 18.6905
Hmm: (1) VITERBI TRAIN CROSS-ENTROPY 4.67953 PERPLEXITY 25.6259

Hmm Iteration: 1 took: 2 seconds

-----------
Hmm: Iteration 2
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 2860 parameters.
Hmm: (2) TRAIN CROSS-ENTROPY 2.52886 PERPLEXITY 5.77114
Hmm: (2) VITERBI TRAIN CROSS-ENTROPY 2.70901 PERPLEXITY 6.53873

Hmm Iteration: 2 took: 3 seconds

-----------
Hmm: Iteration 3
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 2860 parameters.
Hmm: (3) TRAIN CROSS-ENTROPY 2.08775 PERPLEXITY 4.25085
Hmm: (3) VITERBI TRAIN CROSS-ENTROPY 2.18889 PERPLEXITY 4.55954

Hmm Iteration: 3 took: 2 seconds

-----------
Hmm: Iteration 4
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 2860 parameters.
Hmm: (4) TRAIN CROSS-ENTROPY 1.96835 PERPLEXITY 3.91321
Hmm: (4) VITERBI TRAIN CROSS-ENTROPY 2.04359 PERPLEXITY 4.12271

Hmm Iteration: 4 took: 2 seconds

-----------
Hmm: Iteration 5
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 2860 parameters.
Hmm: (5) TRAIN CROSS-ENTROPY 1.92313 PERPLEXITY 3.79246
Hmm: (5) VITERBI TRAIN CROSS-ENTROPY 1.98787 PERPLEXITY 3.96652

Hmm Iteration: 5 took: 3 seconds

Entire Hmm Training took: 12 seconds
==========================================================
Read classes: #words: 251  #classes: 51
Read classes: #words: 354  #classes: 51
Read classes: #words: 251  #classes: 51
Read classes: #words: 354  #classes: 51

==========================================================
Starting H333444:  Viterbi Training
 H333444 Training Started at: Sun Nov  3 14:09:20 2019


---------------------
THTo3: Iteration 1
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
60000
70000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 75.0124 #alsophisticatedcountcollection: 0 #hcsteps: 0
#peggingImprovements: 0
A/D table contains 2860 parameters.
A/D table contains 2281 parameters.
NTable contains 2520 parameter.
p0_count is 482967 and p1 is 12406.2; p0 is 0.999 p1: 0.001
THTo3: TRAIN CROSS-ENTROPY 1.45686 PERPLEXITY 2.7451
THTo3: (1) TRAIN VITERBI CROSS-ENTROPY 1.50822 PERPLEXITY 2.84459

THTo3 Viterbi Iteration : 1 took: 3 seconds

---------------------
Model3: Iteration 2
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
60000
70000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 74.9232 #alsophisticatedcountcollection: 0 #hcsteps: 1.2194
#peggingImprovements: 0
A/D table contains 2860 parameters.
A/D table contains 2281 parameters.
NTable contains 2520 parameter.
p0_count is 501080 and p1 is 3511.75; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 2.01577 PERPLEXITY 4.04395
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 2.03912 PERPLEXITY 4.10994

Model3 Viterbi Iteration : 2 took: 3 seconds

---------------------
Model3: Iteration 3
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
60000
70000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 74.9216 #alsophisticatedcountcollection: 0 #hcsteps: 1.20315
#peggingImprovements: 0
A/D table contains 2860 parameters.
A/D table contains 2281 parameters.
NTable contains 2520 parameter.
p0_count is 504672 and p1 is 1715.54; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 1.94361 PERPLEXITY 3.84666
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 1.95422 PERPLEXITY 3.87508

Model3 Viterbi Iteration : 3 took: 2 seconds

---------------------
T3To4: Iteration 4
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
60000
70000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 74.9189 #alsophisticatedcountcollection: 3.59807 #hcsteps: 1.12792
#peggingImprovements: 0
D4 table contains 469539 parameters.
A/D table contains 2860 parameters.
A/D table contains 2281 parameters.
NTable contains 2520 parameter.
p0_count is 505188 and p1 is 1457.32; p0 is 0.999 p1: 0.001
T3To4: TRAIN CROSS-ENTROPY 1.92176 PERPLEXITY 3.78885
T3To4: (4) TRAIN VITERBI CROSS-ENTROPY 1.92731 PERPLEXITY 3.80346

T3To4 Viterbi Iteration : 4 took: 3 seconds

---------------------
Model4: Iteration 5
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
60000
70000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 74.9111 #alsophisticatedcountcollection: 3.02887 #hcsteps: 1.12004
#peggingImprovements: 0
D4 table contains 469539 parameters.
A/D table contains 2860 parameters.
A/D table contains 2281 parameters.
NTable contains 2520 parameter.
p0_count is 506080 and p1 is 1011.55; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 2.0346 PERPLEXITY 4.09709
Model4: (5) TRAIN VITERBI CROSS-ENTROPY 2.03987 PERPLEXITY 4.11209

Model4 Viterbi Iteration : 5 took: 6 seconds

---------------------
Model4: Iteration 6
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
60000
70000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 74.9102 #alsophisticatedcountcollection: 2.85761 #hcsteps: 1.09739
#peggingImprovements: 0
D4 table contains 469539 parameters.
A/D table contains 2860 parameters.
A/D table contains 2281 parameters.
NTable contains 2520 parameter.
p0_count is 506162 and p1 is 970.308; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 2.0196 PERPLEXITY 4.05472
Model4: (6) TRAIN VITERBI CROSS-ENTROPY 2.02483 PERPLEXITY 4.06945

Model4 Viterbi Iteration : 6 took: 6 seconds
H333444 Training Finished at: Sun Nov  3 14:09:43 2019


Entire Viterbi H333444 Training took: 23 seconds
==========================================================

Entire Training took: 37 seconds
Program Finished at: Sun Nov  3 14:09:43 2019

==========================================================
Executing: rm -f /home/shanika/working/train/giza.si-ta/si-ta.A3.final.gz
Executing: gzip /home/shanika/working/train/giza.si-ta/si-ta.A3.final
(2.1a) running snt2cooc ta-si @ Sun Nov  3 14:09:44 +0530 2019

Executing: mkdir -p /home/shanika/working/train/giza.ta-si
Executing: /home/shanika/mosesdecoder/tools/snt2cooc.out /home/shanika/working/train/corpus/si.vcb /home/shanika/working/train/corpus/ta.vcb /home/shanika/working/train/corpus/ta-si-int-train.snt > /home/shanika/working/train/giza.ta-si/ta-si.cooc
/home/shanika/mosesdecoder/tools/snt2cooc.out /home/shanika/working/train/corpus/si.vcb /home/shanika/working/train/corpus/ta.vcb /home/shanika/working/train/corpus/ta-si-int-train.snt > /home/shanika/working/train/giza.ta-si/ta-si.cooc
line 1000
line 2000
line 3000
line 4000
line 5000
line 6000
line 7000
line 8000
line 9000
line 10000
line 11000
line 12000
line 13000
line 14000
line 15000
line 16000
line 17000
line 18000
line 19000
line 20000
line 21000
line 22000
line 23000
line 24000
line 25000
line 26000
line 27000
line 28000
line 29000
line 30000
line 31000
line 32000
line 33000
line 34000
line 35000
line 36000
line 37000
line 38000
line 39000
line 40000
line 41000
line 42000
line 43000
line 44000
line 45000
line 46000
line 47000
line 48000
line 49000
line 50000
line 51000
line 52000
line 53000
line 54000
line 55000
line 56000
line 57000
line 58000
line 59000
line 60000
line 61000
line 62000
line 63000
line 64000
line 65000
line 66000
line 67000
line 68000
line 69000
line 70000
line 71000
line 72000
line 73000
line 74000
line 75000
END.
(2.1b) running giza ta-si @ Sun Nov  3 14:09:44 +0530 2019
/home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.ta-si/ta-si.cooc -c /home/shanika/working/train/corpus/ta-si-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.ta-si/ta-si -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/si.vcb -t /home/shanika/working/train/corpus/ta.vcb
Executing: /home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.ta-si/ta-si.cooc -c /home/shanika/working/train/corpus/ta-si-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.ta-si/ta-si -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/si.vcb -t /home/shanika/working/train/corpus/ta.vcb
/home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.ta-si/ta-si.cooc -c /home/shanika/working/train/corpus/ta-si-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.ta-si/ta-si -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/si.vcb -t /home/shanika/working/train/corpus/ta.vcb
Parameter 'coocurrencefile' changed from '' to '/home/shanika/working/train/giza.ta-si/ta-si.cooc'
Parameter 'c' changed from '' to '/home/shanika/working/train/corpus/ta-si-int-train.snt'
Parameter 'm3' changed from '5' to '3'
Parameter 'm4' changed from '5' to '3'
Parameter 'model1dumpfrequency' changed from '0' to '1'
Parameter 'model4smoothfactor' changed from '0.2' to '0.4'
Parameter 'nodumps' changed from '0' to '1'
Parameter 'nsmooth' changed from '64' to '4'
Parameter 'o' changed from '119-11-03.140944.shanika' to '/home/shanika/working/train/giza.ta-si/ta-si'
Parameter 'onlyaldumps' changed from '0' to '1'
Parameter 'p0' changed from '-1' to '0.999'
Parameter 's' changed from '' to '/home/shanika/working/train/corpus/si.vcb'
Parameter 't' changed from '' to '/home/shanika/working/train/corpus/ta.vcb'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 119-11-03.140944.shanika.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/shanika/working/train/giza.ta-si/ta-si  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/shanika/working/train/corpus/ta-si-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/shanika/working/train/corpus/si.vcb  (source vocabulary file name)
t = /home/shanika/working/train/corpus/ta.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 119-11-03.140944.shanika.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/shanika/working/train/giza.ta-si/ta-si  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/shanika/working/train/corpus/ta-si-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/shanika/working/train/corpus/si.vcb  (source vocabulary file name)
t = /home/shanika/working/train/corpus/ta.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Reading vocabulary file from:/home/shanika/working/train/corpus/si.vcb
Reading vocabulary file from:/home/shanika/working/train/corpus/ta.vcb
Source vocabulary list has 355 unique tokens 
Target vocabulary list has 252 unique tokens 
Calculating vocabulary frequencies from corpus /home/shanika/working/train/corpus/ta-si-int-train.snt
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
 Train total # sentence pairs (weighted): 75000
Size of source portion of the training corpus: 508103 tokens
Size of the target portion of the training corpus: 556318 tokens 
In source portion of the training corpus, only 354 unique tokens appeared
In target portion of the training corpus, only 250 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 556318/(583103-75000)== 1.09489
There are 24857 24857 entries in table
==========================================================
Model1 Training Started at: Sun Nov  3 14:09:45 2019

-----------
Model1: Iteration 1
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (1) TRAIN CROSS-ENTROPY 8.37464 PERPLEXITY 331.908
Model1: (1) VITERBI TRAIN CROSS-ENTROPY 11.367 PERPLEXITY 2641.15
Model 1 Iteration: 1 took: 0 seconds
-----------
Model1: Iteration 2
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (2) TRAIN CROSS-ENTROPY 5.61474 PERPLEXITY 49.0009
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 6.96797 PERPLEXITY 125.189
Model 1 Iteration: 2 took: 1 seconds
-----------
Model1: Iteration 3
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (3) TRAIN CROSS-ENTROPY 4.7685 PERPLEXITY 27.256
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 5.55762 PERPLEXITY 47.099
Model 1 Iteration: 3 took: 0 seconds
-----------
Model1: Iteration 4
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (4) TRAIN CROSS-ENTROPY 4.47728 PERPLEXITY 22.2738
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 5.06682 PERPLEXITY 33.5169
Model 1 Iteration: 4 took: 1 seconds
-----------
Model1: Iteration 5
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (5) TRAIN CROSS-ENTROPY 4.382 PERPLEXITY 20.8504
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 4.87997 PERPLEXITY 29.4454
Model 1 Iteration: 5 took: 0 seconds
Entire Model1 Training took: 2 seconds
NOTE: I am doing iterations with the HMM model!
Read classes: #words: 354  #classes: 51
Read classes: #words: 251  #classes: 51

==========================================================
Hmm Training Started at: Sun Nov  3 14:09:47 2019

-----------
Hmm: Iteration 1
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 2365 parameters.
Hmm: (1) TRAIN CROSS-ENTROPY 4.34394 PERPLEXITY 20.3075
Hmm: (1) VITERBI TRAIN CROSS-ENTROPY 4.79287 PERPLEXITY 27.7203

Hmm Iteration: 1 took: 2 seconds

-----------
Hmm: Iteration 2
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 2365 parameters.
Hmm: (2) TRAIN CROSS-ENTROPY 2.70318 PERPLEXITY 6.51235
Hmm: (2) VITERBI TRAIN CROSS-ENTROPY 2.88981 PERPLEXITY 7.41175

Hmm Iteration: 2 took: 3 seconds

-----------
Hmm: Iteration 3
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 2365 parameters.
Hmm: (3) TRAIN CROSS-ENTROPY 2.26122 PERPLEXITY 4.79395
Hmm: (3) VITERBI TRAIN CROSS-ENTROPY 2.36872 PERPLEXITY 5.16481

Hmm Iteration: 3 took: 3 seconds

-----------
Hmm: Iteration 4
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 2365 parameters.
Hmm: (4) TRAIN CROSS-ENTROPY 2.09339 PERPLEXITY 4.26751
Hmm: (4) VITERBI TRAIN CROSS-ENTROPY 2.17702 PERPLEXITY 4.52218

Hmm Iteration: 4 took: 2 seconds

-----------
Hmm: Iteration 5
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 2365 parameters.
Hmm: (5) TRAIN CROSS-ENTROPY 2.04098 PERPLEXITY 4.11525
Hmm: (5) VITERBI TRAIN CROSS-ENTROPY 2.11332 PERPLEXITY 4.32686

Hmm Iteration: 5 took: 2 seconds

Entire Hmm Training took: 12 seconds
==========================================================
Read classes: #words: 354  #classes: 51
Read classes: #words: 251  #classes: 51
Read classes: #words: 354  #classes: 51
Read classes: #words: 251  #classes: 51

==========================================================
Starting H333444:  Viterbi Training
 H333444 Training Started at: Sun Nov  3 14:09:59 2019


---------------------
THTo3: Iteration 1
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
60000
70000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 78.6461 #alsophisticatedcountcollection: 0 #hcsteps: 0
#peggingImprovements: 0
A/D table contains 2365 parameters.
A/D table contains 2806 parameters.
NTable contains 3550 parameter.
p0_count is 495428 and p1 is 30313.4; p0 is 0.999 p1: 0.001
THTo3: TRAIN CROSS-ENTROPY 1.63003 PERPLEXITY 3.0952
THTo3: (1) TRAIN VITERBI CROSS-ENTROPY 1.68554 PERPLEXITY 3.2166

THTo3 Viterbi Iteration : 1 took: 3 seconds

---------------------
Model3: Iteration 2
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
60000
70000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 78.6131 #alsophisticatedcountcollection: 0 #hcsteps: 1.39175
#peggingImprovements: 0
A/D table contains 2365 parameters.
A/D table contains 2806 parameters.
NTable contains 3550 parameter.
p0_count is 531380 and p1 is 12468.8; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 2.42695 PERPLEXITY 5.37754
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 2.44556 PERPLEXITY 5.44738

Model3 Viterbi Iteration : 2 took: 3 seconds

---------------------
Model3: Iteration 3
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
60000
70000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 78.5426 #alsophisticatedcountcollection: 0 #hcsteps: 1.24921
#peggingImprovements: 0
A/D table contains 2365 parameters.
A/D table contains 2806 parameters.
NTable contains 3550 parameter.
p0_count is 543897 and p1 is 6210.52; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 2.30386 PERPLEXITY 4.93776
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 2.31419 PERPLEXITY 4.97324

Model3 Viterbi Iteration : 3 took: 2 seconds

---------------------
T3To4: Iteration 4
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
60000
70000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 78.4691 #alsophisticatedcountcollection: 3.29653 #hcsteps: 1.16608
#peggingImprovements: 0
D4 table contains 484561 parameters.
A/D table contains 2365 parameters.
A/D table contains 2806 parameters.
NTable contains 3550 parameter.
p0_count is 548548 and p1 is 3884.97; p0 is 0.999 p1: 0.001
T3To4: TRAIN CROSS-ENTROPY 2.26888 PERPLEXITY 4.81949
T3To4: (4) TRAIN VITERBI CROSS-ENTROPY 2.27769 PERPLEXITY 4.84901

T3To4 Viterbi Iteration : 4 took: 4 seconds

---------------------
Model4: Iteration 5
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
60000
70000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 78.4423 #alsophisticatedcountcollection: 3.00048 #hcsteps: 1.17228
#peggingImprovements: 0
D4 table contains 484561 parameters.
A/D table contains 2365 parameters.
A/D table contains 2806 parameters.
NTable contains 3550 parameter.
p0_count is 551346 and p1 is 2485.84; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 2.26562 PERPLEXITY 4.80861
Model4: (5) TRAIN VITERBI CROSS-ENTROPY 2.27047 PERPLEXITY 4.82481

Model4 Viterbi Iteration : 5 took: 5 seconds

---------------------
Model4: Iteration 6
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
60000
70000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 78.4417 #alsophisticatedcountcollection: 2.77593 #hcsteps: 1.05904
#peggingImprovements: 0
D4 table contains 484561 parameters.
A/D table contains 2365 parameters.
A/D table contains 2806 parameters.
NTable contains 3550 parameter.
p0_count is 551871 and p1 is 2223.53; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 2.25416 PERPLEXITY 4.77057
Model4: (6) TRAIN VITERBI CROSS-ENTROPY 2.2577 PERPLEXITY 4.78227

Model4 Viterbi Iteration : 6 took: 6 seconds
H333444 Training Finished at: Sun Nov  3 14:10:22 2019


Entire Viterbi H333444 Training took: 23 seconds
==========================================================

Entire Training took: 38 seconds
Program Finished at: Sun Nov  3 14:10:22 2019

==========================================================
Executing: rm -f /home/shanika/working/train/giza.ta-si/ta-si.A3.final.gz
Executing: gzip /home/shanika/working/train/giza.ta-si/ta-si.A3.final
(3) generate word alignment @ Sun Nov  3 14:10:23 +0530 2019
Combining forward and inverted alignment from files:
  /home/shanika/working/train/giza.si-ta/si-ta.A3.final.{bz2,gz}
  /home/shanika/working/train/giza.ta-si/ta-si.A3.final.{bz2,gz}
Executing: mkdir -p /home/shanika/working/train/model
Executing: /home/shanika/mosesdecoder/scripts/training/giza2bal.pl -d "gzip -cd /home/shanika/working/train/giza.ta-si/ta-si.A3.final.gz" -i "gzip -cd /home/shanika/working/train/giza.si-ta/si-ta.A3.final.gz" |/home/shanika/mosesdecoder/scripts/../bin/symal -alignment="grow" -diagonal="yes" -final="yes" -both="yes" > /home/shanika/working/train/model/aligned.grow-diag-final-and
symal: computing grow alignment: diagonal (1) final (1)both-uncovered (1)
skip=<0> counts=<75000>
(4) generate lexical translation table 0-0 @ Sun Nov  3 14:10:28 +0530 2019
(/home/shanika/corpus/data.si-ta.clean.si,/home/shanika/corpus/data.si-ta.clean.ta,/home/shanika/working/train/model/lex)
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
Saved: /home/shanika/working/train/model/lex.f2e and /home/shanika/working/train/model/lex.e2f
FILE: /home/shanika/corpus/data.si-ta.clean.ta
FILE: /home/shanika/corpus/data.si-ta.clean.si
FILE: /home/shanika/working/train/model/aligned.grow-diag-final-and
(5) extract phrases @ Sun Nov  3 14:10:31 +0530 2019
/home/shanika/mosesdecoder/scripts/generic/extract-parallel.perl 4 split "sort    " /home/shanika/mosesdecoder/scripts/../bin/extract /home/shanika/corpus/data.si-ta.clean.ta /home/shanika/corpus/data.si-ta.clean.si /home/shanika/working/train/model/aligned.grow-diag-final-and /home/shanika/working/train/model/extract 7 orientation --model wbe-msd --GZOutput 
Executing: /home/shanika/mosesdecoder/scripts/generic/extract-parallel.perl 4 split "sort    " /home/shanika/mosesdecoder/scripts/../bin/extract /home/shanika/corpus/data.si-ta.clean.ta /home/shanika/corpus/data.si-ta.clean.si /home/shanika/working/train/model/aligned.grow-diag-final-and /home/shanika/working/train/model/extract 7 orientation --model wbe-msd --GZOutput 
MAX 7 1 0
Started Sun Nov  3 14:10:31 2019
using gzip 
isBSDSplit=0 
Executing: mkdir -p /home/shanika/working/train/model/tmp.17700; ls -l /home/shanika/working/train/model/tmp.17700 
total=75000 line-per-split=18751 
split -d -l 18751 -a 7 /home/shanika/corpus/data.si-ta.clean.ta /home/shanika/working/train/model/tmp.17700/target.split -d -l 18751 -a 7 /home/shanika/corpus/data.si-ta.clean.si /home/shanika/working/train/model/tmp.17700/source.split -d -l 18751 -a 7 /home/shanika/working/train/model/aligned.grow-diag-final-and /home/shanika/working/train/model/tmp.17700/align.merging extract / extract.inv
gunzip -c /home/shanika/working/train/model/tmp.17700/extract.0000000.gz /home/shanika/working/train/model/tmp.17700/extract.0000001.gz /home/shanika/working/train/model/tmp.17700/extract.0000002.gz /home/shanika/working/train/model/tmp.17700/extract.0000003.gz  | LC_ALL=C sort     -T /home/shanika/working/train/model/tmp.17700 2>> /dev/stderr | gzip -c > /home/shanika/working/train/model/extract.sorted.gz 2>> /dev/stderr 
gunzip -c /home/shanika/working/train/model/tmp.17700/extract.0000000.inv.gz /home/shanika/working/train/model/tmp.17700/extract.0000001.inv.gz /home/shanika/working/train/model/tmp.17700/extract.0000002.inv.gz /home/shanika/working/train/model/tmp.17700/extract.0000003.inv.gz  | LC_ALL=C sort     -T /home/shanika/working/train/model/tmp.17700 2>> /dev/stderr | gzip -c > /home/shanika/working/train/model/extract.inv.sorted.gz 2>> /dev/stderr 
gunzip -c /home/shanika/working/train/model/tmp.17700/extract.0000000.o.gz /home/shanika/working/train/model/tmp.17700/extract.0000001.o.gz /home/shanika/working/train/model/tmp.17700/extract.0000002.o.gz /home/shanika/working/train/model/tmp.17700/extract.0000003.o.gz  | LC_ALL=C sort     -T /home/shanika/working/train/model/tmp.17700 2>> /dev/stderr | gzip -c > /home/shanika/working/train/model/extract.o.sorted.gz 2>> /dev/stderr 
Finished Sun Nov  3 14:10:53 2019
(6) score phrases @ Sun Nov  3 14:10:53 +0530 2019
(6.1)  creating table half /home/shanika/working/train/model/phrase-table.half.f2e @ Sun Nov  3 14:10:53 +0530 2019
/home/shanika/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/extract.sorted.gz /home/shanika/working/train/model/lex.f2e /home/shanika/working/train/model/phrase-table.half.f2e.gz  0 
Executing: /home/shanika/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/extract.sorted.gz /home/shanika/working/train/model/lex.f2e /home/shanika/working/train/model/phrase-table.half.f2e.gz  0 
using gzip 
Started Sun Nov  3 14:10:53 2019
/home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/tmp.17751/extract.0.gz /home/shanika/working/train/model/lex.f2e /home/shanika/working/train/model/tmp.17751/phrase-table.half.0000000.gz  2>> /dev/stderr 
/home/shanika/working/train/model/tmp.17751/run.0.sh/home/shanika/working/train/model/tmp.17751/run.1.sh/home/shanika/working/train/model/tmp.17751/run.2.sh/home/shanika/working/train/model/tmp.17751/run.3.shmv /home/shanika/working/train/model/tmp.17751/phrase-table.half.0000000.gz /home/shanika/working/train/model/phrase-table.half.f2e.gzrm -rf /home/shanika/working/train/model/tmp.17751 
Finished Sun Nov  3 14:10:59 2019
(6.3)  creating table half /home/shanika/working/train/model/phrase-table.half.e2f @ Sun Nov  3 14:10:59 +0530 2019
/home/shanika/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/extract.inv.sorted.gz /home/shanika/working/train/model/lex.e2f /home/shanika/working/train/model/phrase-table.half.e2f.gz --Inverse 1 
Executing: /home/shanika/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/extract.inv.sorted.gz /home/shanika/working/train/model/lex.e2f /home/shanika/working/train/model/phrase-table.half.e2f.gz --Inverse 1 
using gzip 
Started Sun Nov  3 14:10:59 2019
/home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/tmp.17778/extract.0.gz /home/shanika/working/train/model/lex.e2f /home/shanika/working/train/model/tmp.17778/phrase-table.half.0000000.gz --Inverse  2>> /dev/stderr 
/home/shanika/working/train/model/tmp.17778/run.0.sh/home/shanika/working/train/model/tmp.17778/run.1.sh/home/shanika/working/train/model/tmp.17778/run.2.sh/home/shanika/working/train/model/tmp.17778/run.3.shgunzip -c /home/shanika/working/train/model/tmp.17778/phrase-table.half.*.gz 2>> /dev/stderr| LC_ALL=C sort     -T /home/shanika/working/train/model/tmp.17778  | gzip -c > /home/shanika/working/train/model/phrase-table.half.e2f.gz  2>> /dev/stderr rm -rf /home/shanika/working/train/model/tmp.17778 
Finished Sun Nov  3 14:11:05 2019
(6.6) consolidating the two halves @ Sun Nov  3 14:11:05 +0530 2019
Executing: /home/shanika/mosesdecoder/scripts/../bin/consolidate /home/shanika/working/train/model/phrase-table.half.f2e.gz /home/shanika/working/train/model/phrase-table.half.e2f.gz /dev/stdout | gzip -c > /home/shanika/working/train/model/phrase-table.gz
Consolidate v2.0 written by Philipp Koehn
consolidating direct and indirect rule tables
..
Executing: rm -f /home/shanika/working/train/model/phrase-table.half.*
(7) learn reordering model @ Sun Nov  3 14:11:07 +0530 2019
(7.1) [no factors] learn reordering model @ Sun Nov  3 14:11:07 +0530 2019
(7.2) building tables @ Sun Nov  3 14:11:07 +0530 2019
Executing: /home/shanika/mosesdecoder/scripts/../bin/lexical-reordering-score /home/shanika/working/train/model/extract.o.sorted.gz 0.5 /home/shanika/working/train/model/reordering-table. --model "wbe msd wbe-msd-bidirectional-fe"
Lexical Reordering Scorer
scores lexical reordering models of several types (hierarchical, phrase-based and word-based-extraction
(8) learn generation model @ Sun Nov  3 14:11:10 +0530 2019
  no generation model requested, skipping step
(9) create moses.ini @ Sun Nov  3 14:11:10 +0530 2019
