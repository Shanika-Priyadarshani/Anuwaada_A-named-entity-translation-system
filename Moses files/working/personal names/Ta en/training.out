nohup: ignoring input
Using SCRIPTS_ROOTDIR: /home/shanika/mosesdecoder/scripts
Using single-thread GIZA
using gzip 
(1) preparing corpus @ Wed Dec 11 12:25:42 +0530 2019
Executing: mkdir -p /home/shanika/working/train/corpus
(1.0) selecting factors @ Wed Dec 11 12:25:42 +0530 2019
(1.1) running mkcls  @ Wed Dec 11 12:25:42 +0530 2019
/home/shanika/mosesdecoder/tools/mkcls -c50 -n2 -p/home/shanika/corpus/data.ta-en.clean.ta -V/home/shanika/working/train/corpus/ta.vcb.classes opt
Executing: /home/shanika/mosesdecoder/tools/mkcls -c50 -n2 -p/home/shanika/corpus/data.ta-en.clean.ta -V/home/shanika/working/train/corpus/ta.vcb.classes opt

***** 2 runs. (algorithm:TA)*****
;KategProblem:cats: 50   words: 241

start-costs: MEAN: 1.93777e+06 (1.93743e+06-1.93811e+06)  SIGMA:341.098   
  end-costs: MEAN: 1.85743e+06 (1.85738e+06-1.85748e+06)  SIGMA:46.7726   
   start-pp: MEAN: 33.1764 (33.1094-33.2433)  SIGMA:0.0669423   
     end-pp: MEAN: 20.6267 (20.6209-20.6324)  SIGMA:0.00570707   
 iterations: MEAN: 7828 (7777-7879)  SIGMA:51   
       time: MEAN: 0.448917 (0.440818-0.457016)  SIGMA:0.008099   
(1.1) running mkcls  @ Wed Dec 11 12:25:43 +0530 2019
/home/shanika/mosesdecoder/tools/mkcls -c50 -n2 -p/home/shanika/corpus/data.ta-en.clean.en -V/home/shanika/working/train/corpus/en.vcb.classes opt
Executing: /home/shanika/mosesdecoder/tools/mkcls -c50 -n2 -p/home/shanika/corpus/data.ta-en.clean.en -V/home/shanika/working/train/corpus/en.vcb.classes opt
WARNING: StatVar.cc
WARNING: StatVar.cc

***** 2 runs. (algorithm:TA)*****
;KategProblem:cats: 50   words: 38

start-costs: MEAN: 3.50232e+06 (3.4773e+06-3.52735e+06)  SIGMA:25021.6   
  end-costs: MEAN: 3.46504e+06 (3.46504e+06-3.46504e+06)  SIGMA:0.0441942   
   start-pp: MEAN: 9.2981 (8.50172-10.0945)  SIGMA:0.79638   
     end-pp: MEAN: 8.15153 (8.15153-8.15153)  SIGMA:1.19209e-07   
 iterations: MEAN: 50066 (50063-50069)  SIGMA:3   
       time: MEAN: 2.04548 (2.00073-2.09023)  SIGMA:0.0447535   
(1.2) creating vcb file /home/shanika/working/train/corpus/ta.vcb @ Wed Dec 11 12:25:47 +0530 2019
(1.2) creating vcb file /home/shanika/working/train/corpus/en.vcb @ Wed Dec 11 12:25:47 +0530 2019
(1.3) numberizing corpus /home/shanika/working/train/corpus/ta-en-int-train.snt @ Wed Dec 11 12:25:47 +0530 2019
(1.3) numberizing corpus /home/shanika/working/train/corpus/en-ta-int-train.snt @ Wed Dec 11 12:25:48 +0530 2019
(2) running giza @ Wed Dec 11 12:25:48 +0530 2019
(2.1a) running snt2cooc ta-en @ Wed Dec 11 12:25:48 +0530 2019

Executing: mkdir -p /home/shanika/working/train/giza.ta-en
Executing: /home/shanika/mosesdecoder/tools/snt2cooc.out /home/shanika/working/train/corpus/en.vcb /home/shanika/working/train/corpus/ta.vcb /home/shanika/working/train/corpus/ta-en-int-train.snt > /home/shanika/working/train/giza.ta-en/ta-en.cooc
/home/shanika/mosesdecoder/tools/snt2cooc.out /home/shanika/working/train/corpus/en.vcb /home/shanika/working/train/corpus/ta.vcb /home/shanika/working/train/corpus/ta-en-int-train.snt > /home/shanika/working/train/giza.ta-en/ta-en.cooc
line 1000
line 2000
line 3000
line 4000
line 5000
line 6000
line 7000
line 8000
line 9000
line 10000
line 11000
line 12000
line 13000
line 14000
line 15000
line 16000
line 17000
line 18000
line 19000
line 20000
line 21000
line 22000
line 23000
line 24000
line 25000
line 26000
line 27000
line 28000
line 29000
line 30000
END.
(2.1b) running giza ta-en @ Wed Dec 11 12:25:48 +0530 2019
/home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.ta-en/ta-en.cooc -c /home/shanika/working/train/corpus/ta-en-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.ta-en/ta-en -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/en.vcb -t /home/shanika/working/train/corpus/ta.vcb
Executing: /home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.ta-en/ta-en.cooc -c /home/shanika/working/train/corpus/ta-en-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.ta-en/ta-en -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/en.vcb -t /home/shanika/working/train/corpus/ta.vcb
/home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.ta-en/ta-en.cooc -c /home/shanika/working/train/corpus/ta-en-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.ta-en/ta-en -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/en.vcb -t /home/shanika/working/train/corpus/ta.vcb
Parameter 'coocurrencefile' changed from '' to '/home/shanika/working/train/giza.ta-en/ta-en.cooc'
Parameter 'c' changed from '' to '/home/shanika/working/train/corpus/ta-en-int-train.snt'
Parameter 'm3' changed from '5' to '3'
Parameter 'm4' changed from '5' to '3'
Parameter 'model1dumpfrequency' changed from '0' to '1'
Parameter 'model4smoothfactor' changed from '0.2' to '0.4'
Parameter 'nodumps' changed from '0' to '1'
Parameter 'nsmooth' changed from '64' to '4'
Parameter 'o' changed from '119-12-11.122548.shanika' to '/home/shanika/working/train/giza.ta-en/ta-en'
Parameter 'onlyaldumps' changed from '0' to '1'
Parameter 'p0' changed from '-1' to '0.999'
Parameter 's' changed from '' to '/home/shanika/working/train/corpus/en.vcb'
Parameter 't' changed from '' to '/home/shanika/working/train/corpus/ta.vcb'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 119-12-11.122548.shanika.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/shanika/working/train/giza.ta-en/ta-en  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/shanika/working/train/corpus/ta-en-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/shanika/working/train/corpus/en.vcb  (source vocabulary file name)
t = /home/shanika/working/train/corpus/ta.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 119-12-11.122548.shanika.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/shanika/working/train/giza.ta-en/ta-en  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/shanika/working/train/corpus/ta-en-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/shanika/working/train/corpus/en.vcb  (source vocabulary file name)
t = /home/shanika/working/train/corpus/ta.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Reading vocabulary file from:/home/shanika/working/train/corpus/en.vcb
Reading vocabulary file from:/home/shanika/working/train/corpus/ta.vcb
Source vocabulary list has 39 unique tokens 
Target vocabulary list has 242 unique tokens 
Calculating vocabulary frequencies from corpus /home/shanika/working/train/corpus/ta-en-int-train.snt
Reading more sentence pairs into memory ... 
Corpus fits in memory, corpus has: 30906 sentence pairs.
 Train total # sentence pairs (weighted): 30906
Size of source portion of the training corpus: 260517 tokens
Size of the target portion of the training corpus: 138141 tokens 
In source portion of the training corpus, only 38 unique tokens appeared
In target portion of the training corpus, only 240 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 138141/(291423-30906)== 0.530257
There are 4494 4494 entries in table
==========================================================
Model1 Training Started at: Wed Dec 11 12:25:48 2019

-----------
Model1: Iteration 1
Model1: (1) TRAIN CROSS-ENTROPY 8.94992 PERPLEXITY 494.533
Model1: (1) VITERBI TRAIN CROSS-ENTROPY 12.2505 PERPLEXITY 4872.76
Model 1 Iteration: 1 took: 0 seconds
-----------
Model1: Iteration 2
Model1: (2) TRAIN CROSS-ENTROPY 6.88664 PERPLEXITY 118.327
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 8.96092 PERPLEXITY 498.317
Model 1 Iteration: 2 took: 0 seconds
-----------
Model1: Iteration 3
Model1: (3) TRAIN CROSS-ENTROPY 6.52846 PERPLEXITY 92.3129
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 8.03192 PERPLEXITY 261.728
Model 1 Iteration: 3 took: 0 seconds
-----------
Model1: Iteration 4
Model1: (4) TRAIN CROSS-ENTROPY 6.33377 PERPLEXITY 80.6596
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 7.55278 PERPLEXITY 187.764
Model 1 Iteration: 4 took: 0 seconds
-----------
Model1: Iteration 5
Model1: (5) TRAIN CROSS-ENTROPY 6.23831 PERPLEXITY 75.495
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 7.31102 PERPLEXITY 158.794
Model 1 Iteration: 5 took: 0 seconds
Entire Model1 Training took: 0 seconds
NOTE: I am doing iterations with the HMM model!
Read classes: #words: 38  #classes: 38
Read classes: #words: 241  #classes: 51

==========================================================
Hmm Training Started at: Wed Dec 11 12:25:48 2019

-----------
Hmm: Iteration 1
A/D table contains 1958 parameters.
Hmm: (1) TRAIN CROSS-ENTROPY 6.19047 PERPLEXITY 73.0328
Hmm: (1) VITERBI TRAIN CROSS-ENTROPY 7.18933 PERPLEXITY 145.95

Hmm Iteration: 1 took: 1 seconds

-----------
Hmm: Iteration 2
A/D table contains 1958 parameters.
Hmm: (2) TRAIN CROSS-ENTROPY 5.07279 PERPLEXITY 33.6559
Hmm: (2) VITERBI TRAIN CROSS-ENTROPY 5.49754 PERPLEXITY 45.1777

Hmm Iteration: 2 took: 1 seconds

-----------
Hmm: Iteration 3
A/D table contains 1958 parameters.
Hmm: (3) TRAIN CROSS-ENTROPY 4.41 PERPLEXITY 21.259
Hmm: (3) VITERBI TRAIN CROSS-ENTROPY 4.66711 PERPLEXITY 25.4063

Hmm Iteration: 3 took: 1 seconds

-----------
Hmm: Iteration 4
A/D table contains 1958 parameters.
Hmm: (4) TRAIN CROSS-ENTROPY 4.20282 PERPLEXITY 18.4151
Hmm: (4) VITERBI TRAIN CROSS-ENTROPY 4.39805 PERPLEXITY 21.0836

Hmm Iteration: 4 took: 1 seconds

-----------
Hmm: Iteration 5
A/D table contains 1958 parameters.
Hmm: (5) TRAIN CROSS-ENTROPY 4.1348 PERPLEXITY 17.567
Hmm: (5) VITERBI TRAIN CROSS-ENTROPY 4.30418 PERPLEXITY 19.7555

Hmm Iteration: 5 took: 0 seconds

Entire Hmm Training took: 4 seconds
==========================================================
Read classes: #words: 38  #classes: 38
Read classes: #words: 241  #classes: 51
Read classes: #words: 38  #classes: 38
Read classes: #words: 241  #classes: 51

==========================================================
Starting H333444:  Viterbi Training
 H333444 Training Started at: Wed Dec 11 12:25:52 2019


---------------------
THTo3: Iteration 1
10000
20000
30000
#centers(pre/hillclimbed/real): 1 1 1  #al: 51.0588 #alsophisticatedcountcollection: 0 #hcsteps: 0
#peggingImprovements: 0
A/D table contains 1958 parameters.
A/D table contains 1135 parameters.
NTable contains 390 parameter.
p0_count is 131661 and p1 is 3200.64; p0 is 0.999 p1: 0.001
THTo3: TRAIN CROSS-ENTROPY 3.10605 PERPLEXITY 8.61021
THTo3: (1) TRAIN VITERBI CROSS-ENTROPY 3.23375 PERPLEXITY 9.40711

THTo3 Viterbi Iteration : 1 took: 1 seconds

---------------------
Model3: Iteration 2
10000
20000
30000
#centers(pre/hillclimbed/real): 1 1 1  #al: 51.077 #alsophisticatedcountcollection: 0 #hcsteps: 1.4436
#peggingImprovements: 0
A/D table contains 1958 parameters.
A/D table contains 1135 parameters.
NTable contains 390 parameter.
p0_count is 137529 and p1 is 306.014; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 4.13441 PERPLEXITY 17.5623
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 4.22044 PERPLEXITY 18.6414

Model3 Viterbi Iteration : 2 took: 1 seconds

---------------------
Model3: Iteration 3
10000
20000
30000
#centers(pre/hillclimbed/real): 1 1 1  #al: 51.076 #alsophisticatedcountcollection: 0 #hcsteps: 1.49282
#peggingImprovements: 0
A/D table contains 1958 parameters.
A/D table contains 1135 parameters.
NTable contains 390 parameter.
p0_count is 137485 and p1 is 328.211; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 3.91569 PERPLEXITY 15.0918
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 3.96737 PERPLEXITY 15.6422

Model3 Viterbi Iteration : 3 took: 0 seconds

---------------------
T3To4: Iteration 4
10000
20000
30000
#centers(pre/hillclimbed/real): 1 1 1  #al: 51.0754 #alsophisticatedcountcollection: 6.34071 #hcsteps: 1.4106
#peggingImprovements: 0
D4 table contains 251720 parameters.
A/D table contains 1958 parameters.
A/D table contains 1135 parameters.
NTable contains 390 parameter.
p0_count is 137448 and p1 is 346.518; p0 is 0.999 p1: 0.001
T3To4: TRAIN CROSS-ENTROPY 3.79974 PERPLEXITY 13.9263
T3To4: (4) TRAIN VITERBI CROSS-ENTROPY 3.82451 PERPLEXITY 14.1674

T3To4 Viterbi Iteration : 4 took: 1 seconds

---------------------
Model4: Iteration 5
10000
20000
30000
#centers(pre/hillclimbed/real): 1 1 1  #al: 51.0729 #alsophisticatedcountcollection: 5.65793 #hcsteps: 1.36488
#peggingImprovements: 0
D4 table contains 251720 parameters.
A/D table contains 1958 parameters.
A/D table contains 1145 parameters.
NTable contains 390 parameter.
p0_count is 137703 and p1 is 219.08; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 3.94269 PERPLEXITY 15.3769
Model4: (5) TRAIN VITERBI CROSS-ENTROPY 3.9671 PERPLEXITY 15.6393

Model4 Viterbi Iteration : 5 took: 2 seconds

---------------------
Model4: Iteration 6
10000
20000
30000
#centers(pre/hillclimbed/real): 1 1 1  #al: 51.0719 #alsophisticatedcountcollection: 5.13852 #hcsteps: 1.34207
#peggingImprovements: 0
D4 table contains 251720 parameters.
A/D table contains 1958 parameters.
A/D table contains 1145 parameters.
NTable contains 390 parameter.
p0_count is 137783 and p1 is 179.212; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 3.91178 PERPLEXITY 15.0509
Model4: (6) TRAIN VITERBI CROSS-ENTROPY 3.93555 PERPLEXITY 15.301

Model4 Viterbi Iteration : 6 took: 1 seconds
H333444 Training Finished at: Wed Dec 11 12:25:58 2019


Entire Viterbi H333444 Training took: 6 seconds
==========================================================

Entire Training took: 10 seconds
Program Finished at: Wed Dec 11 12:25:58 2019

==========================================================
Executing: rm -f /home/shanika/working/train/giza.ta-en/ta-en.A3.final.gz
Executing: gzip /home/shanika/working/train/giza.ta-en/ta-en.A3.final
(2.1a) running snt2cooc en-ta @ Wed Dec 11 12:25:58 +0530 2019

Executing: mkdir -p /home/shanika/working/train/giza.en-ta
Executing: /home/shanika/mosesdecoder/tools/snt2cooc.out /home/shanika/working/train/corpus/ta.vcb /home/shanika/working/train/corpus/en.vcb /home/shanika/working/train/corpus/en-ta-int-train.snt > /home/shanika/working/train/giza.en-ta/en-ta.cooc
/home/shanika/mosesdecoder/tools/snt2cooc.out /home/shanika/working/train/corpus/ta.vcb /home/shanika/working/train/corpus/en.vcb /home/shanika/working/train/corpus/en-ta-int-train.snt > /home/shanika/working/train/giza.en-ta/en-ta.cooc
line 1000
line 2000
line 3000
line 4000
line 5000
line 6000
line 7000
line 8000
line 9000
line 10000
line 11000
line 12000
line 13000
line 14000
line 15000
line 16000
line 17000
line 18000
line 19000
line 20000
line 21000
line 22000
line 23000
line 24000
line 25000
line 26000
line 27000
line 28000
line 29000
line 30000
END.
(2.1b) running giza en-ta @ Wed Dec 11 12:25:58 +0530 2019
/home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.en-ta/en-ta.cooc -c /home/shanika/working/train/corpus/en-ta-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.en-ta/en-ta -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/ta.vcb -t /home/shanika/working/train/corpus/en.vcb
Executing: /home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.en-ta/en-ta.cooc -c /home/shanika/working/train/corpus/en-ta-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.en-ta/en-ta -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/ta.vcb -t /home/shanika/working/train/corpus/en.vcb
/home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.en-ta/en-ta.cooc -c /home/shanika/working/train/corpus/en-ta-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.en-ta/en-ta -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/ta.vcb -t /home/shanika/working/train/corpus/en.vcb
Parameter 'coocurrencefile' changed from '' to '/home/shanika/working/train/giza.en-ta/en-ta.cooc'
Parameter 'c' changed from '' to '/home/shanika/working/train/corpus/en-ta-int-train.snt'
Parameter 'm3' changed from '5' to '3'
Parameter 'm4' changed from '5' to '3'
Parameter 'model1dumpfrequency' changed from '0' to '1'
Parameter 'model4smoothfactor' changed from '0.2' to '0.4'
Parameter 'nodumps' changed from '0' to '1'
Parameter 'nsmooth' changed from '64' to '4'
Parameter 'o' changed from '119-12-11.122558.shanika' to '/home/shanika/working/train/giza.en-ta/en-ta'
Parameter 'onlyaldumps' changed from '0' to '1'
Parameter 'p0' changed from '-1' to '0.999'
Parameter 's' changed from '' to '/home/shanika/working/train/corpus/ta.vcb'
Parameter 't' changed from '' to '/home/shanika/working/train/corpus/en.vcb'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 119-12-11.122558.shanika.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/shanika/working/train/giza.en-ta/en-ta  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/shanika/working/train/corpus/en-ta-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/shanika/working/train/corpus/ta.vcb  (source vocabulary file name)
t = /home/shanika/working/train/corpus/en.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 119-12-11.122558.shanika.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/shanika/working/train/giza.en-ta/en-ta  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/shanika/working/train/corpus/en-ta-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/shanika/working/train/corpus/ta.vcb  (source vocabulary file name)
t = /home/shanika/working/train/corpus/en.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Reading vocabulary file from:/home/shanika/working/train/corpus/ta.vcb
Reading vocabulary file from:/home/shanika/working/train/corpus/en.vcb
Source vocabulary list has 242 unique tokens 
Target vocabulary list has 39 unique tokens 
Calculating vocabulary frequencies from corpus /home/shanika/working/train/corpus/en-ta-int-train.snt
Reading more sentence pairs into memory ... 
Corpus fits in memory, corpus has: 30906 sentence pairs.
 Train total # sentence pairs (weighted): 30906
Size of source portion of the training corpus: 138141 tokens
Size of the target portion of the training corpus: 260517 tokens 
In source portion of the training corpus, only 241 unique tokens appeared
In target portion of the training corpus, only 37 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 260517/(169047-30906)== 1.88588
There are 4291 4291 entries in table
==========================================================
Model1 Training Started at: Wed Dec 11 12:25:58 2019

-----------
Model1: Iteration 1
Model1: (1) TRAIN CROSS-ENTROPY 5.81565 PERPLEXITY 56.3228
Model1: (1) VITERBI TRAIN CROSS-ENTROPY 8.32889 PERPLEXITY 321.547
Model 1 Iteration: 1 took: 0 seconds
-----------
Model1: Iteration 2
Model1: (2) TRAIN CROSS-ENTROPY 4.06948 PERPLEXITY 16.7894
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 5.74347 PERPLEXITY 53.5742
Model 1 Iteration: 2 took: 1 seconds
-----------
Model1: Iteration 3
Model1: (3) TRAIN CROSS-ENTROPY 3.80598 PERPLEXITY 13.9867
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 5.07275 PERPLEXITY 33.655
Model 1 Iteration: 3 took: 0 seconds
-----------
Model1: Iteration 4
Model1: (4) TRAIN CROSS-ENTROPY 3.66247 PERPLEXITY 12.6623
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 4.69918 PERPLEXITY 25.9773
Model 1 Iteration: 4 took: 0 seconds
-----------
Model1: Iteration 5
Model1: (5) TRAIN CROSS-ENTROPY 3.58693 PERPLEXITY 12.0164
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 4.46736 PERPLEXITY 22.1212
Model 1 Iteration: 5 took: 0 seconds
Entire Model1 Training took: 1 seconds
NOTE: I am doing iterations with the HMM model!
Read classes: #words: 241  #classes: 51
Read classes: #words: 38  #classes: 38

==========================================================
Hmm Training Started at: Wed Dec 11 12:25:59 2019

-----------
Hmm: Iteration 1
A/D table contains 1229 parameters.
Hmm: (1) TRAIN CROSS-ENTROPY 3.54531 PERPLEXITY 11.6747
Hmm: (1) VITERBI TRAIN CROSS-ENTROPY 4.31419 PERPLEXITY 19.893

Hmm Iteration: 1 took: 0 seconds

-----------
Hmm: Iteration 2
A/D table contains 1229 parameters.
Hmm: (2) TRAIN CROSS-ENTROPY 2.95126 PERPLEXITY 7.73424
Hmm: (2) VITERBI TRAIN CROSS-ENTROPY 3.32582 PERPLEXITY 10.027

Hmm Iteration: 2 took: 0 seconds

-----------
Hmm: Iteration 3
A/D table contains 1229 parameters.
Hmm: (3) TRAIN CROSS-ENTROPY 2.54933 PERPLEXITY 5.85364
Hmm: (3) VITERBI TRAIN CROSS-ENTROPY 2.77618 PERPLEXITY 6.85035

Hmm Iteration: 3 took: 1 seconds

-----------
Hmm: Iteration 4
A/D table contains 1229 parameters.
Hmm: (4) TRAIN CROSS-ENTROPY 2.3995 PERPLEXITY 5.27621
Hmm: (4) VITERBI TRAIN CROSS-ENTROPY 2.57493 PERPLEXITY 5.95841

Hmm Iteration: 4 took: 0 seconds

-----------
Hmm: Iteration 5
A/D table contains 1229 parameters.
Hmm: (5) TRAIN CROSS-ENTROPY 2.36146 PERPLEXITY 5.13891
Hmm: (5) VITERBI TRAIN CROSS-ENTROPY 2.51537 PERPLEXITY 5.71744

Hmm Iteration: 5 took: 1 seconds

Entire Hmm Training took: 2 seconds
==========================================================
Read classes: #words: 241  #classes: 51
Read classes: #words: 38  #classes: 38
Read classes: #words: 241  #classes: 51
Read classes: #words: 38  #classes: 38

==========================================================
Starting H333444:  Viterbi Training
 H333444 Training Started at: Wed Dec 11 12:26:01 2019


---------------------
THTo3: Iteration 1
10000
20000
30000
#centers(pre/hillclimbed/real): 1 1 1  #al: 73.333 #alsophisticatedcountcollection: 0 #hcsteps: 0
#peggingImprovements: 0
A/D table contains 1229 parameters.
A/D table contains 2026 parameters.
NTable contains 2420 parameter.
p0_count is 174089 and p1 is 43205; p0 is 0.999 p1: 0.001
THTo3: TRAIN CROSS-ENTROPY 1.85081 PERPLEXITY 3.60702
THTo3: (1) TRAIN VITERBI CROSS-ENTROPY 1.96118 PERPLEXITY 3.89379

THTo3 Viterbi Iteration : 1 took: 0 seconds

---------------------
Model3: Iteration 2
10000
20000
30000
#centers(pre/hillclimbed/real): 1 1 1  #al: 73.09 #alsophisticatedcountcollection: 0 #hcsteps: 2.36789
#peggingImprovements: 0
A/D table contains 1229 parameters.
A/D table contains 2026 parameters.
NTable contains 2420 parameter.
p0_count is 236379 and p1 is 12069.2; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 3.10834 PERPLEXITY 8.6239
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 3.19912 PERPLEXITY 9.18398

Model3 Viterbi Iteration : 2 took: 1 seconds

---------------------
Model3: Iteration 3
10000
20000
30000
#centers(pre/hillclimbed/real): 1 1 1  #al: 72.8606 #alsophisticatedcountcollection: 0 #hcsteps: 2.62202
#peggingImprovements: 0
A/D table contains 1229 parameters.
A/D table contains 2026 parameters.
NTable contains 2420 parameter.
p0_count is 253472 and p1 is 3522.74; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 2.7226 PERPLEXITY 6.60063
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 2.75923 PERPLEXITY 6.77035

Model3 Viterbi Iteration : 3 took: 1 seconds

---------------------
T3To4: Iteration 4
10000
20000
30000
#centers(pre/hillclimbed/real): 1 1 1  #al: 72.8764 #alsophisticatedcountcollection: 8.58406 #hcsteps: 2.65715
#peggingImprovements: 0
D4 table contains 225330 parameters.
A/D table contains 1229 parameters.
A/D table contains 2026 parameters.
NTable contains 2420 parameter.
p0_count is 255518 and p1 is 2499.65; p0 is 0.999 p1: 0.001
T3To4: TRAIN CROSS-ENTROPY 2.6065 PERPLEXITY 6.09024
T3To4: (4) TRAIN VITERBI CROSS-ENTROPY 2.62836 PERPLEXITY 6.18325

T3To4 Viterbi Iteration : 4 took: 0 seconds

---------------------
Model4: Iteration 5
10000
20000
30000
#centers(pre/hillclimbed/real): 1 1 1  #al: 72.8564 #alsophisticatedcountcollection: 7.05164 #hcsteps: 2.69074
#peggingImprovements: 0
D4 table contains 225330 parameters.
A/D table contains 1229 parameters.
A/D table contains 2026 parameters.
NTable contains 2420 parameter.
p0_count is 257118 and p1 is 1699.59; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 2.28742 PERPLEXITY 4.88181
Model4: (5) TRAIN VITERBI CROSS-ENTROPY 2.30223 PERPLEXITY 4.93221

Model4 Viterbi Iteration : 5 took: 2 seconds

---------------------
Model4: Iteration 6
10000
20000
30000
#centers(pre/hillclimbed/real): 1 1 1  #al: 72.8157 #alsophisticatedcountcollection: 5.87274 #hcsteps: 2.7319
#peggingImprovements: 0
D4 table contains 225330 parameters.
A/D table contains 1229 parameters.
A/D table contains 2026 parameters.
NTable contains 2420 parameter.
p0_count is 257770 and p1 is 1373.31; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 2.24971 PERPLEXITY 4.75589
Model4: (6) TRAIN VITERBI CROSS-ENTROPY 2.26156 PERPLEXITY 4.7951

Model4 Viterbi Iteration : 6 took: 3 seconds
H333444 Training Finished at: Wed Dec 11 12:26:08 2019


Entire Viterbi H333444 Training took: 7 seconds
==========================================================

Entire Training took: 10 seconds
Program Finished at: Wed Dec 11 12:26:08 2019

==========================================================
Executing: rm -f /home/shanika/working/train/giza.en-ta/en-ta.A3.final.gz
Executing: gzip /home/shanika/working/train/giza.en-ta/en-ta.A3.final
(3) generate word alignment @ Wed Dec 11 12:26:08 +0530 2019
Combining forward and inverted alignment from files:
  /home/shanika/working/train/giza.ta-en/ta-en.A3.final.{bz2,gz}
  /home/shanika/working/train/giza.en-ta/en-ta.A3.final.{bz2,gz}
Executing: mkdir -p /home/shanika/working/train/model
Executing: /home/shanika/mosesdecoder/scripts/training/giza2bal.pl -d "gzip -cd /home/shanika/working/train/giza.en-ta/en-ta.A3.final.gz" -i "gzip -cd /home/shanika/working/train/giza.ta-en/ta-en.A3.final.gz" |/home/shanika/mosesdecoder/scripts/../bin/symal -alignment="grow" -diagonal="yes" -final="yes" -both="yes" > /home/shanika/working/train/model/aligned.grow-diag-final-and
symal: computing grow alignment: diagonal (1) final (1)both-uncovered (1)
skip=<0> counts=<30906>
(4) generate lexical translation table 0-0 @ Wed Dec 11 12:26:09 +0530 2019
(/home/shanika/corpus/data.ta-en.clean.ta,/home/shanika/corpus/data.ta-en.clean.en,/home/shanika/working/train/model/lex)
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
Saved: /home/shanika/working/train/model/lex.f2e and /home/shanika/working/train/model/lex.e2f
FILE: /home/shanika/corpus/data.ta-en.clean.en
FILE: /home/shanika/corpus/data.ta-en.clean.ta
FILE: /home/shanika/working/train/model/aligned.grow-diag-final-and
(5) extract phrases @ Wed Dec 11 12:26:10 +0530 2019
/home/shanika/mosesdecoder/scripts/generic/extract-parallel.perl 4 split "sort    " /home/shanika/mosesdecoder/scripts/../bin/extract /home/shanika/corpus/data.ta-en.clean.en /home/shanika/corpus/data.ta-en.clean.ta /home/shanika/working/train/model/aligned.grow-diag-final-and /home/shanika/working/train/model/extract 7 orientation --model wbe-msd --GZOutput 
Executing: /home/shanika/mosesdecoder/scripts/generic/extract-parallel.perl 4 split "sort    " /home/shanika/mosesdecoder/scripts/../bin/extract /home/shanika/corpus/data.ta-en.clean.en /home/shanika/corpus/data.ta-en.clean.ta /home/shanika/working/train/model/aligned.grow-diag-final-and /home/shanika/working/train/model/extract 7 orientation --model wbe-msd --GZOutput 
MAX 7 1 0
Started Wed Dec 11 12:26:10 2019
using gzip 
isBSDSplit=0 
Executing: mkdir -p /home/shanika/working/train/model/tmp.8929; ls -l /home/shanika/working/train/model/tmp.8929 
total=30906 line-per-split=7727 
split -d -l 7727 -a 7 /home/shanika/corpus/data.ta-en.clean.en /home/shanika/working/train/model/tmp.8929/target.split -d -l 7727 -a 7 /home/shanika/corpus/data.ta-en.clean.ta /home/shanika/working/train/model/tmp.8929/source.split -d -l 7727 -a 7 /home/shanika/working/train/model/aligned.grow-diag-final-and /home/shanika/working/train/model/tmp.8929/align.merging extract / extract.inv
gunzip -c /home/shanika/working/train/model/tmp.8929/extract.0000000.inv.gz /home/shanika/working/train/model/tmp.8929/extract.0000001.inv.gz /home/shanika/working/train/model/tmp.8929/extract.0000002.inv.gz /home/shanika/working/train/model/tmp.8929/extract.0000003.inv.gz  | LC_ALL=C sort     -T /home/shanika/working/train/model/tmp.8929 2>> /dev/stderr | gzip -c > /home/shanika/working/train/model/extract.inv.sorted.gz 2>> /dev/stderr 
gunzip -c /home/shanika/working/train/model/tmp.8929/extract.0000000.o.gz /home/shanika/working/train/model/tmp.8929/extract.0000001.o.gz /home/shanika/working/train/model/tmp.8929/extract.0000002.o.gz /home/shanika/working/train/model/tmp.8929/extract.0000003.o.gz  | LC_ALL=C sort     -T /home/shanika/working/train/model/tmp.8929 2>> /dev/stderr | gzip -c > /home/shanika/working/train/model/extract.o.sorted.gz 2>> /dev/stderr 
gunzip -c /home/shanika/working/train/model/tmp.8929/extract.0000000.gz /home/shanika/working/train/model/tmp.8929/extract.0000001.gz /home/shanika/working/train/model/tmp.8929/extract.0000002.gz /home/shanika/working/train/model/tmp.8929/extract.0000003.gz  | LC_ALL=C sort     -T /home/shanika/working/train/model/tmp.8929 2>> /dev/stderr | gzip -c > /home/shanika/working/train/model/extract.sorted.gz 2>> /dev/stderr 
Finished Wed Dec 11 12:26:11 2019
(6) score phrases @ Wed Dec 11 12:26:11 +0530 2019
(6.1)  creating table half /home/shanika/working/train/model/phrase-table.half.f2e @ Wed Dec 11 12:26:11 +0530 2019
/home/shanika/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/extract.sorted.gz /home/shanika/working/train/model/lex.f2e /home/shanika/working/train/model/phrase-table.half.f2e.gz  0 
Executing: /home/shanika/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/extract.sorted.gz /home/shanika/working/train/model/lex.f2e /home/shanika/working/train/model/phrase-table.half.f2e.gz  0 
using gzip 
Started Wed Dec 11 12:26:11 2019
/home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/tmp.8978/extract.0.gz /home/shanika/working/train/model/lex.f2e /home/shanika/working/train/model/tmp.8978/phrase-table.half.0000000.gz  2>> /dev/stderr 
/home/shanika/working/train/model/tmp.8978/run.0.sh/home/shanika/working/train/model/tmp.8978/run.1.sh/home/shanika/working/train/model/tmp.8978/run.2.sh/home/shanika/working/train/model/tmp.8978/run.3.shmv /home/shanika/working/train/model/tmp.8978/phrase-table.half.0000000.gz /home/shanika/working/train/model/phrase-table.half.f2e.gzrm -rf /home/shanika/working/train/model/tmp.8978 
Finished Wed Dec 11 12:26:11 2019
(6.3)  creating table half /home/shanika/working/train/model/phrase-table.half.e2f @ Wed Dec 11 12:26:11 +0530 2019
/home/shanika/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/extract.inv.sorted.gz /home/shanika/working/train/model/lex.e2f /home/shanika/working/train/model/phrase-table.half.e2f.gz --Inverse 1 
Executing: /home/shanika/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/extract.inv.sorted.gz /home/shanika/working/train/model/lex.e2f /home/shanika/working/train/model/phrase-table.half.e2f.gz --Inverse 1 
using gzip 
Started Wed Dec 11 12:26:11 2019
/home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/tmp.9004/extract.0.gz /home/shanika/working/train/model/lex.e2f /home/shanika/working/train/model/tmp.9004/phrase-table.half.0000000.gz --Inverse  2>> /dev/stderr 
/home/shanika/working/train/model/tmp.9004/run.0.sh/home/shanika/working/train/model/tmp.9004/run.1.sh/home/shanika/working/train/model/tmp.9004/run.2.sh/home/shanika/working/train/model/tmp.9004/run.3.shgunzip -c /home/shanika/working/train/model/tmp.9004/phrase-table.half.*.gz 2>> /dev/stderr| LC_ALL=C sort     -T /home/shanika/working/train/model/tmp.9004  | gzip -c > /home/shanika/working/train/model/phrase-table.half.e2f.gz  2>> /dev/stderr rm -rf /home/shanika/working/train/model/tmp.9004 
Finished Wed Dec 11 12:26:12 2019
(6.6) consolidating the two halves @ Wed Dec 11 12:26:12 +0530 2019
Executing: /home/shanika/mosesdecoder/scripts/../bin/consolidate /home/shanika/working/train/model/phrase-table.half.f2e.gz /home/shanika/working/train/model/phrase-table.half.e2f.gz /dev/stdout | gzip -c > /home/shanika/working/train/model/phrase-table.gz
Consolidate v2.0 written by Philipp Koehn
consolidating direct and indirect rule tables

Executing: rm -f /home/shanika/working/train/model/phrase-table.half.*
(7) learn reordering model @ Wed Dec 11 12:26:12 +0530 2019
(7.1) [no factors] learn reordering model @ Wed Dec 11 12:26:12 +0530 2019
(7.2) building tables @ Wed Dec 11 12:26:12 +0530 2019
Executing: /home/shanika/mosesdecoder/scripts/../bin/lexical-reordering-score /home/shanika/working/train/model/extract.o.sorted.gz 0.5 /home/shanika/working/train/model/reordering-table. --model "wbe msd wbe-msd-bidirectional-fe"
Lexical Reordering Scorer
scores lexical reordering models of several types (hierarchical, phrase-based and word-based-extraction
(8) learn generation model @ Wed Dec 11 12:26:12 +0530 2019
  no generation model requested, skipping step
(9) create moses.ini @ Wed Dec 11 12:26:12 +0530 2019
