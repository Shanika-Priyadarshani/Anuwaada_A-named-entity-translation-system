nohup: ignoring input
Using SCRIPTS_ROOTDIR: /home/shanika/mosesdecoder/scripts
Using single-thread GIZA
using gzip 
(1) preparing corpus @ Wed Dec 11 01:42:38 +0530 2019
Executing: mkdir -p /home/shanika/working/train/corpus
(1.0) selecting factors @ Wed Dec 11 01:42:38 +0530 2019
(1.1) running mkcls  @ Wed Dec 11 01:42:38 +0530 2019
/home/shanika/mosesdecoder/tools/mkcls -c50 -n2 -p/home/shanika/corpus/data.en-si.clean.en -V/home/shanika/working/train/corpus/en.vcb.classes opt
Executing: /home/shanika/mosesdecoder/tools/mkcls -c50 -n2 -p/home/shanika/corpus/data.en-si.clean.en -V/home/shanika/working/train/corpus/en.vcb.classes opt
WARNING: StatVar.cc
WARNING: StatVar.cc

***** 2 runs. (algorithm:TA)*****
;KategProblem:cats: 50   words: 29

start-costs: MEAN: 3.62931e+06 (3.60066e+06-3.65796e+06)  SIGMA:28649.3   
  end-costs: MEAN: 3.59069e+06 (3.59069e+06-3.59069e+06)  SIGMA:0.0441942   
   start-pp: MEAN: 9.62406 (8.70812-10.54)  SIGMA:0.915935   
     end-pp: MEAN: 8.42367 (8.42367-8.42367)  SIGMA:0   
 iterations: MEAN: 50047.5 (50046-50049)  SIGMA:1.5   
       time: MEAN: 2.31654 (2.18873-2.44435)  SIGMA:0.127811   
(1.1) running mkcls  @ Wed Dec 11 01:42:42 +0530 2019
/home/shanika/mosesdecoder/tools/mkcls -c50 -n2 -p/home/shanika/corpus/data.en-si.clean.si -V/home/shanika/working/train/corpus/si.vcb.classes opt
Executing: /home/shanika/mosesdecoder/tools/mkcls -c50 -n2 -p/home/shanika/corpus/data.en-si.clean.si -V/home/shanika/working/train/corpus/si.vcb.classes opt

***** 2 runs. (algorithm:TA)*****
;KategProblem:cats: 50   words: 556

start-costs: MEAN: 1.9753e+06 (1.96944e+06-1.98115e+06)  SIGMA:5854.72   
  end-costs: MEAN: 1.88634e+06 (1.88595e+06-1.88674e+06)  SIGMA:391.354   
   start-pp: MEAN: 42.9449 (41.4806-44.4091)  SIGMA:1.46421   
     end-pp: MEAN: 25.5623 (25.504-25.6206)  SIGMA:0.0582804   
 iterations: MEAN: 15318.5 (15304-15333)  SIGMA:14.5   
       time: MEAN: 0.600845 (0.59844-0.60325)  SIGMA:0.002405   
(1.2) creating vcb file /home/shanika/working/train/corpus/en.vcb @ Wed Dec 11 01:42:44 +0530 2019
(1.2) creating vcb file /home/shanika/working/train/corpus/si.vcb @ Wed Dec 11 01:42:44 +0530 2019
(1.3) numberizing corpus /home/shanika/working/train/corpus/en-si-int-train.snt @ Wed Dec 11 01:42:44 +0530 2019
(1.3) numberizing corpus /home/shanika/working/train/corpus/si-en-int-train.snt @ Wed Dec 11 01:42:44 +0530 2019
(2) running giza @ Wed Dec 11 01:42:44 +0530 2019
(2.1a) running snt2cooc en-si @ Wed Dec 11 01:42:44 +0530 2019

Executing: mkdir -p /home/shanika/working/train/giza.en-si
Executing: /home/shanika/mosesdecoder/tools/snt2cooc.out /home/shanika/working/train/corpus/si.vcb /home/shanika/working/train/corpus/en.vcb /home/shanika/working/train/corpus/en-si-int-train.snt > /home/shanika/working/train/giza.en-si/en-si.cooc
/home/shanika/mosesdecoder/tools/snt2cooc.out /home/shanika/working/train/corpus/si.vcb /home/shanika/working/train/corpus/en.vcb /home/shanika/working/train/corpus/en-si-int-train.snt > /home/shanika/working/train/giza.en-si/en-si.cooc
line 1000
line 2000
line 3000
line 4000
line 5000
line 6000
line 7000
line 8000
line 9000
line 10000
line 11000
line 12000
line 13000
line 14000
line 15000
line 16000
line 17000
line 18000
line 19000
line 20000
line 21000
line 22000
line 23000
line 24000
line 25000
line 26000
line 27000
line 28000
line 29000
line 30000
line 31000
END.
(2.1b) running giza en-si @ Wed Dec 11 01:42:44 +0530 2019
/home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.en-si/en-si.cooc -c /home/shanika/working/train/corpus/en-si-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.en-si/en-si -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/si.vcb -t /home/shanika/working/train/corpus/en.vcb
Executing: /home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.en-si/en-si.cooc -c /home/shanika/working/train/corpus/en-si-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.en-si/en-si -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/si.vcb -t /home/shanika/working/train/corpus/en.vcb
/home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.en-si/en-si.cooc -c /home/shanika/working/train/corpus/en-si-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.en-si/en-si -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/si.vcb -t /home/shanika/working/train/corpus/en.vcb
Parameter 'coocurrencefile' changed from '' to '/home/shanika/working/train/giza.en-si/en-si.cooc'
Parameter 'c' changed from '' to '/home/shanika/working/train/corpus/en-si-int-train.snt'
Parameter 'm3' changed from '5' to '3'
Parameter 'm4' changed from '5' to '3'
Parameter 'model1dumpfrequency' changed from '0' to '1'
Parameter 'model4smoothfactor' changed from '0.2' to '0.4'
Parameter 'nodumps' changed from '0' to '1'
Parameter 'nsmooth' changed from '64' to '4'
Parameter 'o' changed from '119-12-11.014245.shanika' to '/home/shanika/working/train/giza.en-si/en-si'
Parameter 'onlyaldumps' changed from '0' to '1'
Parameter 'p0' changed from '-1' to '0.999'
Parameter 's' changed from '' to '/home/shanika/working/train/corpus/si.vcb'
Parameter 't' changed from '' to '/home/shanika/working/train/corpus/en.vcb'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 119-12-11.014245.shanika.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/shanika/working/train/giza.en-si/en-si  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/shanika/working/train/corpus/en-si-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/shanika/working/train/corpus/si.vcb  (source vocabulary file name)
t = /home/shanika/working/train/corpus/en.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 119-12-11.014245.shanika.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/shanika/working/train/giza.en-si/en-si  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/shanika/working/train/corpus/en-si-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/shanika/working/train/corpus/si.vcb  (source vocabulary file name)
t = /home/shanika/working/train/corpus/en.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Reading vocabulary file from:/home/shanika/working/train/corpus/si.vcb
Reading vocabulary file from:/home/shanika/working/train/corpus/en.vcb
Source vocabulary list has 557 unique tokens 
Target vocabulary list has 30 unique tokens 
Calculating vocabulary frequencies from corpus /home/shanika/working/train/corpus/en-si-int-train.snt
Reading more sentence pairs into memory ... 
Corpus fits in memory, corpus has: 31650 sentence pairs.
 Train total # sentence pairs (weighted): 31650
Size of source portion of the training corpus: 140001 tokens
Size of the target portion of the training corpus: 268467 tokens 
In source portion of the training corpus, only 556 unique tokens appeared
In target portion of the training corpus, only 28 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 268467/(171651-31650)== 1.91761
There are 7457 7457 entries in table
==========================================================
Model1 Training Started at: Wed Dec 11 01:42:45 2019

-----------
Model1: Iteration 1
Model1: (1) TRAIN CROSS-ENTROPY 5.44398 PERPLEXITY 43.5314
Model1: (1) VITERBI TRAIN CROSS-ENTROPY 7.9519 PERPLEXITY 247.606
Model 1 Iteration: 1 took: 0 seconds
-----------
Model1: Iteration 2
Model1: (2) TRAIN CROSS-ENTROPY 3.98051 PERPLEXITY 15.7853
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 5.60835 PERPLEXITY 48.7846
Model 1 Iteration: 2 took: 0 seconds
-----------
Model1: Iteration 3
Model1: (3) TRAIN CROSS-ENTROPY 3.67303 PERPLEXITY 12.7554
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 4.90714 PERPLEXITY 30.0052
Model 1 Iteration: 3 took: 0 seconds
-----------
Model1: Iteration 4
Model1: (4) TRAIN CROSS-ENTROPY 3.51228 PERPLEXITY 11.4104
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 4.51558 PERPLEXITY 22.8731
Model 1 Iteration: 4 took: 0 seconds
-----------
Model1: Iteration 5
Model1: (5) TRAIN CROSS-ENTROPY 3.43209 PERPLEXITY 10.7935
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 4.27102 PERPLEXITY 19.3066
Model 1 Iteration: 5 took: 0 seconds
Entire Model1 Training took: 0 seconds
NOTE: I am doing iterations with the HMM model!
Read classes: #words: 556  #classes: 51
Read classes: #words: 29  #classes: 29

==========================================================
Hmm Training Started at: Wed Dec 11 01:42:45 2019

-----------
Hmm: Iteration 1
A/D table contains 1079 parameters.
Hmm: (1) TRAIN CROSS-ENTROPY 3.3904 PERPLEXITY 10.4861
Hmm: (1) VITERBI TRAIN CROSS-ENTROPY 4.12561 PERPLEXITY 17.4555

Hmm Iteration: 1 took: 1 seconds

-----------
Hmm: Iteration 2
A/D table contains 1079 parameters.
Hmm: (2) TRAIN CROSS-ENTROPY 2.78417 PERPLEXITY 6.88842
Hmm: (2) VITERBI TRAIN CROSS-ENTROPY 3.15001 PERPLEXITY 8.87663

Hmm Iteration: 2 took: 0 seconds

-----------
Hmm: Iteration 3
A/D table contains 1079 parameters.
Hmm: (3) TRAIN CROSS-ENTROPY 2.41227 PERPLEXITY 5.32312
Hmm: (3) VITERBI TRAIN CROSS-ENTROPY 2.65961 PERPLEXITY 6.31864

Hmm Iteration: 3 took: 1 seconds

-----------
Hmm: Iteration 4
A/D table contains 1079 parameters.
Hmm: (4) TRAIN CROSS-ENTROPY 2.27913 PERPLEXITY 4.85384
Hmm: (4) VITERBI TRAIN CROSS-ENTROPY 2.48666 PERPLEXITY 5.6048

Hmm Iteration: 4 took: 0 seconds

-----------
Hmm: Iteration 5
A/D table contains 1079 parameters.
Hmm: (5) TRAIN CROSS-ENTROPY 2.24712 PERPLEXITY 4.74735
Hmm: (5) VITERBI TRAIN CROSS-ENTROPY 2.43645 PERPLEXITY 5.41308

Hmm Iteration: 5 took: 1 seconds

Entire Hmm Training took: 3 seconds
==========================================================
Read classes: #words: 556  #classes: 51
Read classes: #words: 29  #classes: 29
Read classes: #words: 556  #classes: 51
Read classes: #words: 29  #classes: 29

==========================================================
Starting H333444:  Viterbi Training
 H333444 Training Started at: Wed Dec 11 01:42:48 2019


---------------------
THTo3: Iteration 1
10000
20000
30000
#centers(pre/hillclimbed/real): 1 1 1  #al: 73.6913 #alsophisticatedcountcollection: 0 #hcsteps: 0
#peggingImprovements: 0
A/D table contains 1079 parameters.
A/D table contains 1951 parameters.
NTable contains 5570 parameter.
p0_count is 171786 and p1 is 48320.9; p0 is 0.999 p1: 0.001
THTo3: TRAIN CROSS-ENTROPY 1.74626 PERPLEXITY 3.35488
THTo3: (1) TRAIN VITERBI CROSS-ENTROPY 1.87876 PERPLEXITY 3.67759

THTo3 Viterbi Iteration : 1 took: 1 seconds

---------------------
Model3: Iteration 2
10000
20000
30000
#centers(pre/hillclimbed/real): 1 1 1  #al: 73.5147 #alsophisticatedcountcollection: 0 #hcsteps: 2.53166
#peggingImprovements: 0
A/D table contains 1079 parameters.
A/D table contains 1950 parameters.
NTable contains 5570 parameter.
p0_count is 241172 and p1 is 13647.7; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 3.01299 PERPLEXITY 8.07236
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 3.10568 PERPLEXITY 8.608

Model3 Viterbi Iteration : 2 took: 1 seconds

---------------------
Model3: Iteration 3
10000
20000
30000
#centers(pre/hillclimbed/real): 1 1 1  #al: 73.3842 #alsophisticatedcountcollection: 0 #hcsteps: 2.53024
#peggingImprovements: 0
A/D table contains 1079 parameters.
A/D table contains 1950 parameters.
NTable contains 5570 parameter.
p0_count is 256556 and p1 is 5955.71; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 2.60208 PERPLEXITY 6.0716
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 2.64439 PERPLEXITY 6.2523

Model3 Viterbi Iteration : 3 took: 0 seconds

---------------------
T3To4: Iteration 4
10000
20000
30000
#centers(pre/hillclimbed/real): 1 1 1  #al: 73.4097 #alsophisticatedcountcollection: 9.03716 #hcsteps: 2.5945
#peggingImprovements: 0
D4 table contains 210511 parameters.
A/D table contains 1079 parameters.
A/D table contains 1950 parameters.
NTable contains 5570 parameter.
p0_count is 259496 and p1 is 4485.36; p0 is 0.999 p1: 0.001
T3To4: TRAIN CROSS-ENTROPY 2.49851 PERPLEXITY 5.65102
T3To4: (4) TRAIN VITERBI CROSS-ENTROPY 2.52746 PERPLEXITY 5.76557

T3To4 Viterbi Iteration : 4 took: 2 seconds

---------------------
Model4: Iteration 5
10000
20000
30000
#centers(pre/hillclimbed/real): 1 1 1  #al: 73.3535 #alsophisticatedcountcollection: 7.22303 #hcsteps: 2.66995
#peggingImprovements: 0
D4 table contains 210511 parameters.
A/D table contains 1079 parameters.
A/D table contains 1950 parameters.
NTable contains 5570 parameter.
p0_count is 261495 and p1 is 3486.1; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 2.16519 PERPLEXITY 4.48525
Model4: (5) TRAIN VITERBI CROSS-ENTROPY 2.18189 PERPLEXITY 4.53749

Model4 Viterbi Iteration : 5 took: 2 seconds

---------------------
Model4: Iteration 6
10000
20000
30000
#centers(pre/hillclimbed/real): 1 1 1  #al: 73.3503 #alsophisticatedcountcollection: 6.00834 #hcsteps: 2.65668
#peggingImprovements: 0
D4 table contains 210511 parameters.
A/D table contains 1079 parameters.
A/D table contains 1950 parameters.
NTable contains 5570 parameter.
p0_count is 262058 and p1 is 3204.72; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 2.12146 PERPLEXITY 4.35133
Model4: (6) TRAIN VITERBI CROSS-ENTROPY 2.13455 PERPLEXITY 4.39099

Model4 Viterbi Iteration : 6 took: 3 seconds
H333444 Training Finished at: Wed Dec 11 01:42:57 2019


Entire Viterbi H333444 Training took: 9 seconds
==========================================================

Entire Training took: 12 seconds
Program Finished at: Wed Dec 11 01:42:57 2019

==========================================================
Executing: rm -f /home/shanika/working/train/giza.en-si/en-si.A3.final.gz
Executing: gzip /home/shanika/working/train/giza.en-si/en-si.A3.final
(2.1a) running snt2cooc si-en @ Wed Dec 11 01:42:57 +0530 2019

Executing: mkdir -p /home/shanika/working/train/giza.si-en
Executing: /home/shanika/mosesdecoder/tools/snt2cooc.out /home/shanika/working/train/corpus/en.vcb /home/shanika/working/train/corpus/si.vcb /home/shanika/working/train/corpus/si-en-int-train.snt > /home/shanika/working/train/giza.si-en/si-en.cooc
/home/shanika/mosesdecoder/tools/snt2cooc.out /home/shanika/working/train/corpus/en.vcb /home/shanika/working/train/corpus/si.vcb /home/shanika/working/train/corpus/si-en-int-train.snt > /home/shanika/working/train/giza.si-en/si-en.cooc
line 1000
line 2000
line 3000
line 4000
line 5000
line 6000
line 7000
line 8000
line 9000
line 10000
line 11000
line 12000
line 13000
line 14000
line 15000
line 16000
line 17000
line 18000
line 19000
line 20000
line 21000
line 22000
line 23000
line 24000
line 25000
line 26000
line 27000
line 28000
line 29000
line 30000
line 31000
END.
(2.1b) running giza si-en @ Wed Dec 11 01:42:57 +0530 2019
/home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.si-en/si-en.cooc -c /home/shanika/working/train/corpus/si-en-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.si-en/si-en -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/en.vcb -t /home/shanika/working/train/corpus/si.vcb
Executing: /home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.si-en/si-en.cooc -c /home/shanika/working/train/corpus/si-en-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.si-en/si-en -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/en.vcb -t /home/shanika/working/train/corpus/si.vcb
/home/shanika/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/shanika/working/train/giza.si-en/si-en.cooc -c /home/shanika/working/train/corpus/si-en-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/shanika/working/train/giza.si-en/si-en -onlyaldumps 1 -p0 0.999 -s /home/shanika/working/train/corpus/en.vcb -t /home/shanika/working/train/corpus/si.vcb
Parameter 'coocurrencefile' changed from '' to '/home/shanika/working/train/giza.si-en/si-en.cooc'
Parameter 'c' changed from '' to '/home/shanika/working/train/corpus/si-en-int-train.snt'
Parameter 'm3' changed from '5' to '3'
Parameter 'm4' changed from '5' to '3'
Parameter 'model1dumpfrequency' changed from '0' to '1'
Parameter 'model4smoothfactor' changed from '0.2' to '0.4'
Parameter 'nodumps' changed from '0' to '1'
Parameter 'nsmooth' changed from '64' to '4'
Parameter 'o' changed from '119-12-11.014257.shanika' to '/home/shanika/working/train/giza.si-en/si-en'
Parameter 'onlyaldumps' changed from '0' to '1'
Parameter 'p0' changed from '-1' to '0.999'
Parameter 's' changed from '' to '/home/shanika/working/train/corpus/en.vcb'
Parameter 't' changed from '' to '/home/shanika/working/train/corpus/si.vcb'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 119-12-11.014257.shanika.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/shanika/working/train/giza.si-en/si-en  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/shanika/working/train/corpus/si-en-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/shanika/working/train/corpus/en.vcb  (source vocabulary file name)
t = /home/shanika/working/train/corpus/si.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 119-12-11.014257.shanika.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/shanika/working/train/giza.si-en/si-en  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/shanika/working/train/corpus/si-en-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/shanika/working/train/corpus/en.vcb  (source vocabulary file name)
t = /home/shanika/working/train/corpus/si.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Reading vocabulary file from:/home/shanika/working/train/corpus/en.vcb
Reading vocabulary file from:/home/shanika/working/train/corpus/si.vcb
Source vocabulary list has 30 unique tokens 
Target vocabulary list has 557 unique tokens 
Calculating vocabulary frequencies from corpus /home/shanika/working/train/corpus/si-en-int-train.snt
Reading more sentence pairs into memory ... 
Corpus fits in memory, corpus has: 31650 sentence pairs.
 Train total # sentence pairs (weighted): 31650
Size of source portion of the training corpus: 268467 tokens
Size of the target portion of the training corpus: 140001 tokens 
In source portion of the training corpus, only 29 unique tokens appeared
In target portion of the training corpus, only 555 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 140001/(300117-31650)== 0.521483
There are 7984 7984 entries in table
==========================================================
Model1 Training Started at: Wed Dec 11 01:42:57 2019

-----------
Model1: Iteration 1
Model1: (1) TRAIN CROSS-ENTROPY 10.1805 PERPLEXITY 1160.51
Model1: (1) VITERBI TRAIN CROSS-ENTROPY 13.4923 PERPLEXITY 11523.2
Model 1 Iteration: 1 took: 1 seconds
-----------
Model1: Iteration 2
Model1: (2) TRAIN CROSS-ENTROPY 7.23236 PERPLEXITY 150.368
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 9.19245 PERPLEXITY 585.065
Model 1 Iteration: 2 took: 0 seconds
-----------
Model1: Iteration 3
Model1: (3) TRAIN CROSS-ENTROPY 6.81043 PERPLEXITY 112.239
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 8.18195 PERPLEXITY 290.411
Model 1 Iteration: 3 took: 0 seconds
-----------
Model1: Iteration 4
Model1: (4) TRAIN CROSS-ENTROPY 6.59568 PERPLEXITY 96.7158
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 7.70531 PERPLEXITY 208.703
Model 1 Iteration: 4 took: 0 seconds
-----------
Model1: Iteration 5
Model1: (5) TRAIN CROSS-ENTROPY 6.49908 PERPLEXITY 90.452
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 7.48052 PERPLEXITY 178.591
Model 1 Iteration: 5 took: 0 seconds
Entire Model1 Training took: 1 seconds
NOTE: I am doing iterations with the HMM model!
Read classes: #words: 29  #classes: 29
Read classes: #words: 556  #classes: 51

==========================================================
Hmm Training Started at: Wed Dec 11 01:42:58 2019

-----------
Hmm: Iteration 1
A/D table contains 1879 parameters.
Hmm: (1) TRAIN CROSS-ENTROPY 6.45392 PERPLEXITY 87.6646
Hmm: (1) VITERBI TRAIN CROSS-ENTROPY 7.36506 PERPLEXITY 164.856

Hmm Iteration: 1 took: 1 seconds

-----------
Hmm: Iteration 2
A/D table contains 1879 parameters.
Hmm: (2) TRAIN CROSS-ENTROPY 5.18341 PERPLEXITY 36.338
Hmm: (2) VITERBI TRAIN CROSS-ENTROPY 5.51697 PERPLEXITY 45.7902

Hmm Iteration: 2 took: 1 seconds

-----------
Hmm: Iteration 3
A/D table contains 1879 parameters.
Hmm: (3) TRAIN CROSS-ENTROPY 4.5192 PERPLEXITY 22.9306
Hmm: (3) VITERBI TRAIN CROSS-ENTROPY 4.71769 PERPLEXITY 26.3127

Hmm Iteration: 3 took: 1 seconds

-----------
Hmm: Iteration 4
A/D table contains 1879 parameters.
Hmm: (4) TRAIN CROSS-ENTROPY 4.33529 PERPLEXITY 20.186
Hmm: (4) VITERBI TRAIN CROSS-ENTROPY 4.48705 PERPLEXITY 22.4253

Hmm Iteration: 4 took: 1 seconds

-----------
Hmm: Iteration 5
A/D table contains 1879 parameters.
Hmm: (5) TRAIN CROSS-ENTROPY 4.26778 PERPLEXITY 19.2633
Hmm: (5) VITERBI TRAIN CROSS-ENTROPY 4.39698 PERPLEXITY 21.068

Hmm Iteration: 5 took: 1 seconds

Entire Hmm Training took: 5 seconds
==========================================================
Read classes: #words: 29  #classes: 29
Read classes: #words: 556  #classes: 51
Read classes: #words: 29  #classes: 29
Read classes: #words: 556  #classes: 51

==========================================================
Starting H333444:  Viterbi Training
 H333444 Training Started at: Wed Dec 11 01:43:03 2019


---------------------
THTo3: Iteration 1
10000
20000
30000
#centers(pre/hillclimbed/real): 1 1 1  #al: 50.7662 #alsophisticatedcountcollection: 0 #hcsteps: 0
#peggingImprovements: 0
A/D table contains 1879 parameters.
A/D table contains 966 parameters.
NTable contains 300 parameter.
p0_count is 133983 and p1 is 2996.67; p0 is 0.999 p1: 0.001
THTo3: TRAIN CROSS-ENTROPY 3.19356 PERPLEXITY 9.14865
THTo3: (1) TRAIN VITERBI CROSS-ENTROPY 3.29222 PERPLEXITY 9.79615

THTo3 Viterbi Iteration : 1 took: 1 seconds

---------------------
Model3: Iteration 2
10000
20000
30000
#centers(pre/hillclimbed/real): 1 1 1  #al: 50.7787 #alsophisticatedcountcollection: 0 #hcsteps: 1.41469
#peggingImprovements: 0
A/D table contains 1879 parameters.
A/D table contains 966 parameters.
NTable contains 300 parameter.
p0_count is 139190 and p1 is 405.748; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 4.12712 PERPLEXITY 17.4738
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 4.18267 PERPLEXITY 18.1597

Model3 Viterbi Iteration : 2 took: 1 seconds

---------------------
Model3: Iteration 3
10000
20000
30000
#centers(pre/hillclimbed/real): 1 1 1  #al: 50.7796 #alsophisticatedcountcollection: 0 #hcsteps: 1.32711
#peggingImprovements: 0
A/D table contains 1879 parameters.
A/D table contains 966 parameters.
NTable contains 300 parameter.
p0_count is 139724 and p1 is 138.638; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 3.86589 PERPLEXITY 14.5798
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 3.88389 PERPLEXITY 14.7627

Model3 Viterbi Iteration : 3 took: 1 seconds

---------------------
T3To4: Iteration 4
10000
20000
30000
#centers(pre/hillclimbed/real): 1 1 1  #al: 50.7807 #alsophisticatedcountcollection: 4.70976 #hcsteps: 1.20938
#peggingImprovements: 0
D4 table contains 238728 parameters.
A/D table contains 1879 parameters.
A/D table contains 966 parameters.
NTable contains 300 parameter.
p0_count is 139709 and p1 is 145.784; p0 is 0.999 p1: 0.001
T3To4: TRAIN CROSS-ENTROPY 3.78034 PERPLEXITY 13.7403
T3To4: (4) TRAIN VITERBI CROSS-ENTROPY 3.79074 PERPLEXITY 13.8397

T3To4 Viterbi Iteration : 4 took: 1 seconds

---------------------
Model4: Iteration 5
10000
20000
30000
#centers(pre/hillclimbed/real): 1 1 1  #al: 50.7799 #alsophisticatedcountcollection: 3.37763 #hcsteps: 1.09735
#peggingImprovements: 0
D4 table contains 238728 parameters.
A/D table contains 1879 parameters.
A/D table contains 969 parameters.
NTable contains 300 parameter.
p0_count is 139749 and p1 is 125.94; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 4.0018 PERPLEXITY 16.02
Model4: (5) TRAIN VITERBI CROSS-ENTROPY 4.01478 PERPLEXITY 16.1648

Model4 Viterbi Iteration : 5 took: 2 seconds

---------------------
Model4: Iteration 6
10000
20000
30000
#centers(pre/hillclimbed/real): 1 1 1  #al: 50.7798 #alsophisticatedcountcollection: 3.13422 #hcsteps: 1.06284
#peggingImprovements: 0
D4 table contains 238728 parameters.
A/D table contains 1879 parameters.
A/D table contains 971 parameters.
NTable contains 300 parameter.
p0_count is 139757 and p1 is 122.046; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 3.99114 PERPLEXITY 15.902
Model4: (6) TRAIN VITERBI CROSS-ENTROPY 4.00435 PERPLEXITY 16.0484

Model4 Viterbi Iteration : 6 took: 2 seconds
H333444 Training Finished at: Wed Dec 11 01:43:11 2019


Entire Viterbi H333444 Training took: 8 seconds
==========================================================

Entire Training took: 14 seconds
Program Finished at: Wed Dec 11 01:43:11 2019

==========================================================
Executing: rm -f /home/shanika/working/train/giza.si-en/si-en.A3.final.gz
Executing: gzip /home/shanika/working/train/giza.si-en/si-en.A3.final
(3) generate word alignment @ Wed Dec 11 01:43:11 +0530 2019
Combining forward and inverted alignment from files:
  /home/shanika/working/train/giza.en-si/en-si.A3.final.{bz2,gz}
  /home/shanika/working/train/giza.si-en/si-en.A3.final.{bz2,gz}
Executing: mkdir -p /home/shanika/working/train/model
Executing: /home/shanika/mosesdecoder/scripts/training/giza2bal.pl -d "gzip -cd /home/shanika/working/train/giza.si-en/si-en.A3.final.gz" -i "gzip -cd /home/shanika/working/train/giza.en-si/en-si.A3.final.gz" |/home/shanika/mosesdecoder/scripts/../bin/symal -alignment="grow" -diagonal="yes" -final="yes" -both="yes" > /home/shanika/working/train/model/aligned.grow-diag-final-and
symal: computing grow alignment: diagonal (1) final (1)both-uncovered (1)
skip=<0> counts=<31650>
(4) generate lexical translation table 0-0 @ Wed Dec 11 01:43:12 +0530 2019
(/home/shanika/corpus/data.en-si.clean.en,/home/shanika/corpus/data.en-si.clean.si,/home/shanika/working/train/model/lex)
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
Saved: /home/shanika/working/train/model/lex.f2e and /home/shanika/working/train/model/lex.e2f
FILE: /home/shanika/corpus/data.en-si.clean.si
FILE: /home/shanika/corpus/data.en-si.clean.en
FILE: /home/shanika/working/train/model/aligned.grow-diag-final-and
(5) extract phrases @ Wed Dec 11 01:43:13 +0530 2019
/home/shanika/mosesdecoder/scripts/generic/extract-parallel.perl 4 split "sort    " /home/shanika/mosesdecoder/scripts/../bin/extract /home/shanika/corpus/data.en-si.clean.si /home/shanika/corpus/data.en-si.clean.en /home/shanika/working/train/model/aligned.grow-diag-final-and /home/shanika/working/train/model/extract 7 orientation --model wbe-msd --GZOutput 
Executing: /home/shanika/mosesdecoder/scripts/generic/extract-parallel.perl 4 split "sort    " /home/shanika/mosesdecoder/scripts/../bin/extract /home/shanika/corpus/data.en-si.clean.si /home/shanika/corpus/data.en-si.clean.en /home/shanika/working/train/model/aligned.grow-diag-final-and /home/shanika/working/train/model/extract 7 orientation --model wbe-msd --GZOutput 
MAX 7 1 0
Started Wed Dec 11 01:43:13 2019
using gzip 
isBSDSplit=0 
Executing: mkdir -p /home/shanika/working/train/model/tmp.16059; ls -l /home/shanika/working/train/model/tmp.16059 
total=31650 line-per-split=7913 
split -d -l 7913 -a 7 /home/shanika/corpus/data.en-si.clean.si /home/shanika/working/train/model/tmp.16059/target.split -d -l 7913 -a 7 /home/shanika/corpus/data.en-si.clean.en /home/shanika/working/train/model/tmp.16059/source.split -d -l 7913 -a 7 /home/shanika/working/train/model/aligned.grow-diag-final-and /home/shanika/working/train/model/tmp.16059/align.merging extract / extract.inv
gunzip -c /home/shanika/working/train/model/tmp.16059/extract.0000000.gz /home/shanika/working/train/model/tmp.16059/extract.0000001.gz /home/shanika/working/train/model/tmp.16059/extract.0000002.gz /home/shanika/working/train/model/tmp.16059/extract.0000003.gz  | LC_ALL=C sort     -T /home/shanika/working/train/model/tmp.16059 2>> /dev/stderr | gzip -c > /home/shanika/working/train/model/extract.sorted.gz 2>> /dev/stderr 
gunzip -c /home/shanika/working/train/model/tmp.16059/extract.0000000.inv.gz /home/shanika/working/train/model/tmp.16059/extract.0000001.inv.gz /home/shanika/working/train/model/tmp.16059/extract.0000002.inv.gz /home/shanika/working/train/model/tmp.16059/extract.0000003.inv.gz  | LC_ALL=C sort     -T /home/shanika/working/train/model/tmp.16059 2>> /dev/stderr | gzip -c > /home/shanika/working/train/model/extract.inv.sorted.gz 2>> /dev/stderr 
gunzip -c /home/shanika/working/train/model/tmp.16059/extract.0000000.o.gz /home/shanika/working/train/model/tmp.16059/extract.0000001.o.gz /home/shanika/working/train/model/tmp.16059/extract.0000002.o.gz /home/shanika/working/train/model/tmp.16059/extract.0000003.o.gz  | LC_ALL=C sort     -T /home/shanika/working/train/model/tmp.16059 2>> /dev/stderr | gzip -c > /home/shanika/working/train/model/extract.o.sorted.gz 2>> /dev/stderr 
Finished Wed Dec 11 01:43:16 2019
(6) score phrases @ Wed Dec 11 01:43:16 +0530 2019
(6.1)  creating table half /home/shanika/working/train/model/phrase-table.half.f2e @ Wed Dec 11 01:43:16 +0530 2019
/home/shanika/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/extract.sorted.gz /home/shanika/working/train/model/lex.f2e /home/shanika/working/train/model/phrase-table.half.f2e.gz  0 
Executing: /home/shanika/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/extract.sorted.gz /home/shanika/working/train/model/lex.f2e /home/shanika/working/train/model/phrase-table.half.f2e.gz  0 
using gzip 
Started Wed Dec 11 01:43:16 2019
/home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/tmp.16110/extract.0.gz /home/shanika/working/train/model/lex.f2e /home/shanika/working/train/model/tmp.16110/phrase-table.half.0000000.gz  2>> /dev/stderr 
/home/shanika/working/train/model/tmp.16110/run.0.sh/home/shanika/working/train/model/tmp.16110/run.1.sh/home/shanika/working/train/model/tmp.16110/run.2.sh/home/shanika/working/train/model/tmp.16110/run.3.shmv /home/shanika/working/train/model/tmp.16110/phrase-table.half.0000000.gz /home/shanika/working/train/model/phrase-table.half.f2e.gzrm -rf /home/shanika/working/train/model/tmp.16110 
Finished Wed Dec 11 01:43:17 2019
(6.3)  creating table half /home/shanika/working/train/model/phrase-table.half.e2f @ Wed Dec 11 01:43:17 +0530 2019
/home/shanika/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/extract.inv.sorted.gz /home/shanika/working/train/model/lex.e2f /home/shanika/working/train/model/phrase-table.half.e2f.gz --Inverse 1 
Executing: /home/shanika/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/extract.inv.sorted.gz /home/shanika/working/train/model/lex.e2f /home/shanika/working/train/model/phrase-table.half.e2f.gz --Inverse 1 
using gzip 
Started Wed Dec 11 01:43:17 2019
/home/shanika/mosesdecoder/scripts/../bin/score /home/shanika/working/train/model/tmp.16138/extract.0.gz /home/shanika/working/train/model/lex.e2f /home/shanika/working/train/model/tmp.16138/phrase-table.half.0000000.gz --Inverse  2>> /dev/stderr 
/home/shanika/working/train/model/tmp.16138/run.0.sh/home/shanika/working/train/model/tmp.16138/run.1.sh/home/shanika/working/train/model/tmp.16138/run.2.sh/home/shanika/working/train/model/tmp.16138/run.3.shgunzip -c /home/shanika/working/train/model/tmp.16138/phrase-table.half.*.gz 2>> /dev/stderr| LC_ALL=C sort     -T /home/shanika/working/train/model/tmp.16138  | gzip -c > /home/shanika/working/train/model/phrase-table.half.e2f.gz  2>> /dev/stderr rm -rf /home/shanika/working/train/model/tmp.16138 
Finished Wed Dec 11 01:43:17 2019
(6.6) consolidating the two halves @ Wed Dec 11 01:43:17 +0530 2019
Executing: /home/shanika/mosesdecoder/scripts/../bin/consolidate /home/shanika/working/train/model/phrase-table.half.f2e.gz /home/shanika/working/train/model/phrase-table.half.e2f.gz /dev/stdout | gzip -c > /home/shanika/working/train/model/phrase-table.gz
Consolidate v2.0 written by Philipp Koehn
consolidating direct and indirect rule tables

Executing: rm -f /home/shanika/working/train/model/phrase-table.half.*
(7) learn reordering model @ Wed Dec 11 01:43:17 +0530 2019
(7.1) [no factors] learn reordering model @ Wed Dec 11 01:43:17 +0530 2019
(7.2) building tables @ Wed Dec 11 01:43:17 +0530 2019
Executing: /home/shanika/mosesdecoder/scripts/../bin/lexical-reordering-score /home/shanika/working/train/model/extract.o.sorted.gz 0.5 /home/shanika/working/train/model/reordering-table. --model "wbe msd wbe-msd-bidirectional-fe"
Lexical Reordering Scorer
scores lexical reordering models of several types (hierarchical, phrase-based and word-based-extraction
(8) learn generation model @ Wed Dec 11 01:43:18 +0530 2019
  no generation model requested, skipping step
(9) create moses.ini @ Wed Dec 11 01:43:18 +0530 2019
