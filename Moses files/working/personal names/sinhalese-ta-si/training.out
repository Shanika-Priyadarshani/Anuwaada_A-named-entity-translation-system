nohup: ignoring input
Using SCRIPTS_ROOTDIR: /home/anuadmin/moses/mosesdecoder/scripts
Using single-thread GIZA
using gzip 
(1) preparing corpus @ Sun Nov  3 00:24:39 +0530 2019
Executing: mkdir -p /home/anuadmin/moses/working/train/corpus
(1.0) selecting factors @ Sun Nov  3 00:24:39 +0530 2019
(1.1) running mkcls  @ Sun Nov  3 00:24:39 +0530 2019
/home/anuadmin/moses/mosesdecoder/tools/mkcls -c50 -n2 -p/home/anuadmin/moses/corpus/sinhalese-ta-si/data.ta-si.clean.ta -V/home/anuadmin/moses/working/train/corpus/ta.vcb.classes opt
Executing: /home/anuadmin/moses/mosesdecoder/tools/mkcls -c50 -n2 -p/home/anuadmin/moses/corpus/sinhalese-ta-si/data.ta-si.clean.ta -V/home/anuadmin/moses/working/train/corpus/ta.vcb.classes opt

***** 2 runs. (algorithm:TA)*****
;KategProblem:cats: 50   words: 342

start-costs: MEAN: 4.6731e+07 (4.65462e+07-4.69158e+07)  SIGMA:184785   
  end-costs: MEAN: 4.47689e+07 (4.47672e+07-4.47707e+07)  SIGMA:1756.93   
   start-pp: MEAN: 35.8102 (33.7758-37.8446)  SIGMA:2.03438   
     end-pp: MEAN: 19.5456 (19.535-19.5562)  SIGMA:0.0105689   
 iterations: MEAN: 10393.5 (10229-10558)  SIGMA:164.5   
       time: MEAN: 1.39468 (1.37108-1.41828)  SIGMA:0.0235975   
(1.1) running mkcls  @ Sun Nov  3 00:24:43 +0530 2019
/home/anuadmin/moses/mosesdecoder/tools/mkcls -c50 -n2 -p/home/anuadmin/moses/corpus/sinhalese-ta-si/data.ta-si.clean.si -V/home/anuadmin/moses/working/train/corpus/si.vcb.classes opt
Executing: /home/anuadmin/moses/mosesdecoder/tools/mkcls -c50 -n2 -p/home/anuadmin/moses/corpus/sinhalese-ta-si/data.ta-si.clean.si -V/home/anuadmin/moses/working/train/corpus/si.vcb.classes opt

***** 2 runs. (algorithm:TA)*****
;KategProblem:cats: 50   words: 504

start-costs: MEAN: 4.21646e+07 (4.21605e+07-4.21686e+07)  SIGMA:4008.12   
  end-costs: MEAN: 4.08363e+07 (4.08312e+07-4.08414e+07)  SIGMA:5085.49   
   start-pp: MEAN: 40.8707 (40.8149-40.9264)  SIGMA:0.0557642   
     end-pp: MEAN: 26.0045 (25.9595-26.0495)  SIGMA:0.0450178   
 iterations: MEAN: 13933 (13917-13949)  SIGMA:16   
       time: MEAN: 1.5995 (1.59803-1.60096)  SIGMA:0.0014685   
(1.2) creating vcb file /home/anuadmin/moses/working/train/corpus/ta.vcb @ Sun Nov  3 00:24:47 +0530 2019
(1.2) creating vcb file /home/anuadmin/moses/working/train/corpus/si.vcb @ Sun Nov  3 00:24:48 +0530 2019
(1.3) numberizing corpus /home/anuadmin/moses/working/train/corpus/ta-si-int-train.snt @ Sun Nov  3 00:24:49 +0530 2019
(1.3) numberizing corpus /home/anuadmin/moses/working/train/corpus/si-ta-int-train.snt @ Sun Nov  3 00:24:53 +0530 2019
(2) running giza @ Sun Nov  3 00:24:58 +0530 2019
(2.1a) running snt2cooc ta-si @ Sun Nov  3 00:24:58 +0530 2019

Executing: mkdir -p /home/anuadmin/moses/working/train/giza.ta-si
Executing: /home/anuadmin/moses/mosesdecoder/tools/snt2cooc.out /home/anuadmin/moses/working/train/corpus/si.vcb /home/anuadmin/moses/working/train/corpus/ta.vcb /home/anuadmin/moses/working/train/corpus/ta-si-int-train.snt > /home/anuadmin/moses/working/train/giza.ta-si/ta-si.cooc
/home/anuadmin/moses/mosesdecoder/tools/snt2cooc.out /home/anuadmin/moses/working/train/corpus/si.vcb /home/anuadmin/moses/working/train/corpus/ta.vcb /home/anuadmin/moses/working/train/corpus/ta-si-int-train.snt > /home/anuadmin/moses/working/train/giza.ta-si/ta-si.cooc
line 1000
line 2000
line 3000
line 4000
line 5000
line 6000
line 7000
line 8000
line 9000
line 10000
line 11000
line 12000
line 13000
line 14000
line 15000
line 16000
line 17000
line 18000
line 19000
line 20000
line 21000
line 22000
line 23000
line 24000
line 25000
line 26000
line 27000
line 28000
line 29000
line 30000
line 31000
line 32000
line 33000
line 34000
line 35000
line 36000
line 37000
line 38000
line 39000
line 40000
line 41000
line 42000
line 43000
line 44000
line 45000
line 46000
line 47000
line 48000
line 49000
line 50000
line 51000
line 52000
line 53000
line 54000
line 55000
line 56000
line 57000
line 58000
line 59000
line 60000
line 61000
line 62000
line 63000
line 64000
line 65000
line 66000
line 67000
line 68000
line 69000
line 70000
line 71000
line 72000
line 73000
line 74000
line 75000
line 76000
line 77000
line 78000
line 79000
line 80000
line 81000
line 82000
line 83000
line 84000
line 85000
line 86000
line 87000
line 88000
line 89000
line 90000
line 91000
line 92000
line 93000
line 94000
line 95000
line 96000
line 97000
line 98000
line 99000
line 100000
line 101000
line 102000
line 103000
line 104000
line 105000
line 106000
line 107000
line 108000
line 109000
line 110000
line 111000
line 112000
line 113000
line 114000
line 115000
line 116000
line 117000
line 118000
line 119000
line 120000
line 121000
line 122000
line 123000
line 124000
line 125000
line 126000
line 127000
line 128000
line 129000
line 130000
line 131000
line 132000
line 133000
line 134000
line 135000
line 136000
line 137000
line 138000
line 139000
line 140000
line 141000
line 142000
line 143000
line 144000
line 145000
line 146000
line 147000
line 148000
line 149000
line 150000
line 151000
line 152000
line 153000
line 154000
line 155000
line 156000
line 157000
line 158000
line 159000
line 160000
line 161000
line 162000
line 163000
line 164000
line 165000
line 166000
line 167000
line 168000
line 169000
line 170000
line 171000
line 172000
line 173000
line 174000
line 175000
line 176000
line 177000
line 178000
line 179000
line 180000
line 181000
line 182000
line 183000
line 184000
line 185000
line 186000
line 187000
line 188000
line 189000
line 190000
line 191000
line 192000
line 193000
line 194000
line 195000
line 196000
line 197000
line 198000
line 199000
line 200000
line 201000
line 202000
line 203000
line 204000
line 205000
line 206000
line 207000
line 208000
line 209000
line 210000
line 211000
line 212000
line 213000
line 214000
line 215000
line 216000
line 217000
line 218000
line 219000
line 220000
line 221000
line 222000
line 223000
line 224000
line 225000
line 226000
line 227000
line 228000
line 229000
line 230000
line 231000
line 232000
line 233000
line 234000
line 235000
line 236000
line 237000
line 238000
line 239000
line 240000
line 241000
line 242000
line 243000
line 244000
line 245000
line 246000
line 247000
line 248000
line 249000
line 250000
line 251000
line 252000
line 253000
line 254000
line 255000
line 256000
line 257000
line 258000
line 259000
line 260000
line 261000
line 262000
line 263000
line 264000
line 265000
line 266000
line 267000
line 268000
line 269000
line 270000
line 271000
line 272000
line 273000
line 274000
line 275000
line 276000
line 277000
line 278000
line 279000
line 280000
line 281000
line 282000
line 283000
line 284000
line 285000
line 286000
line 287000
line 288000
line 289000
line 290000
line 291000
line 292000
line 293000
line 294000
line 295000
line 296000
line 297000
line 298000
line 299000
line 300000
line 301000
line 302000
line 303000
line 304000
line 305000
line 306000
line 307000
line 308000
line 309000
line 310000
line 311000
line 312000
line 313000
line 314000
line 315000
line 316000
line 317000
line 318000
line 319000
line 320000
line 321000
line 322000
line 323000
line 324000
line 325000
line 326000
line 327000
line 328000
line 329000
line 330000
line 331000
line 332000
line 333000
line 334000
line 335000
line 336000
line 337000
line 338000
line 339000
line 340000
line 341000
line 342000
line 343000
line 344000
line 345000
line 346000
line 347000
line 348000
line 349000
line 350000
line 351000
line 352000
line 353000
line 354000
line 355000
line 356000
line 357000
line 358000
line 359000
line 360000
line 361000
line 362000
line 363000
line 364000
line 365000
line 366000
line 367000
line 368000
line 369000
line 370000
line 371000
line 372000
line 373000
line 374000
line 375000
line 376000
line 377000
line 378000
line 379000
line 380000
line 381000
line 382000
line 383000
line 384000
line 385000
line 386000
line 387000
line 388000
line 389000
line 390000
line 391000
line 392000
line 393000
line 394000
line 395000
line 396000
line 397000
line 398000
line 399000
line 400000
END.
(2.1b) running giza ta-si @ Sun Nov  3 00:25:01 +0530 2019
/home/anuadmin/moses/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/anuadmin/moses/working/train/giza.ta-si/ta-si.cooc -c /home/anuadmin/moses/working/train/corpus/ta-si-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/anuadmin/moses/working/train/giza.ta-si/ta-si -onlyaldumps 1 -p0 0.999 -s /home/anuadmin/moses/working/train/corpus/si.vcb -t /home/anuadmin/moses/working/train/corpus/ta.vcb
Executing: /home/anuadmin/moses/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/anuadmin/moses/working/train/giza.ta-si/ta-si.cooc -c /home/anuadmin/moses/working/train/corpus/ta-si-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/anuadmin/moses/working/train/giza.ta-si/ta-si -onlyaldumps 1 -p0 0.999 -s /home/anuadmin/moses/working/train/corpus/si.vcb -t /home/anuadmin/moses/working/train/corpus/ta.vcb
/home/anuadmin/moses/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/anuadmin/moses/working/train/giza.ta-si/ta-si.cooc -c /home/anuadmin/moses/working/train/corpus/ta-si-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/anuadmin/moses/working/train/giza.ta-si/ta-si -onlyaldumps 1 -p0 0.999 -s /home/anuadmin/moses/working/train/corpus/si.vcb -t /home/anuadmin/moses/working/train/corpus/ta.vcb
Parameter 'coocurrencefile' changed from '' to '/home/anuadmin/moses/working/train/giza.ta-si/ta-si.cooc'
Parameter 'c' changed from '' to '/home/anuadmin/moses/working/train/corpus/ta-si-int-train.snt'
Parameter 'm3' changed from '5' to '3'
Parameter 'm4' changed from '5' to '3'
Parameter 'model1dumpfrequency' changed from '0' to '1'
Parameter 'model4smoothfactor' changed from '0.2' to '0.4'
Parameter 'nodumps' changed from '0' to '1'
Parameter 'nsmooth' changed from '64' to '4'
Parameter 'o' changed from '119-11-03.002501.anuadmin' to '/home/anuadmin/moses/working/train/giza.ta-si/ta-si'
Parameter 'onlyaldumps' changed from '0' to '1'
Parameter 'p0' changed from '-1' to '0.999'
Parameter 's' changed from '' to '/home/anuadmin/moses/working/train/corpus/si.vcb'
Parameter 't' changed from '' to '/home/anuadmin/moses/working/train/corpus/ta.vcb'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 119-11-03.002501.anuadmin.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/anuadmin/moses/working/train/giza.ta-si/ta-si  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/anuadmin/moses/working/train/corpus/ta-si-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/anuadmin/moses/working/train/corpus/si.vcb  (source vocabulary file name)
t = /home/anuadmin/moses/working/train/corpus/ta.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 119-11-03.002501.anuadmin.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/anuadmin/moses/working/train/giza.ta-si/ta-si  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/anuadmin/moses/working/train/corpus/ta-si-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/anuadmin/moses/working/train/corpus/si.vcb  (source vocabulary file name)
t = /home/anuadmin/moses/working/train/corpus/ta.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Reading vocabulary file from:/home/anuadmin/moses/working/train/corpus/si.vcb
Reading vocabulary file from:/home/anuadmin/moses/working/train/corpus/ta.vcb
Source vocabulary list has 505 unique tokens 
Target vocabulary list has 343 unique tokens 
Calculating vocabulary frequencies from corpus /home/anuadmin/moses/working/train/corpus/ta-si-int-train.snt
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:200000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:300000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:400000]
Reading more sentence pairs into memory ... 
 Train total # sentence pairs (weighted): 400000
Size of source portion of the training corpus: 2.53762e+06 tokens
Size of the target portion of the training corpus: 2.84918e+06 tokens 
In source portion of the training corpus, only 504 unique tokens appeared
In target portion of the training corpus, only 341 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 2.84918e+06/(2.93762e+06-400000)== 1.12277
There are 41714 41714 entries in table
==========================================================
Model1 Training Started at: Sun Nov  3 00:25:02 2019

-----------
Model1: Iteration 1
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:200000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:300000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:400000]
Reading more sentence pairs into memory ... 
Model1: (1) TRAIN CROSS-ENTROPY 8.8351 PERPLEXITY 456.7
Model1: (1) VITERBI TRAIN CROSS-ENTROPY 11.747 PERPLEXITY 3437.14
Model 1 Iteration: 1 took: 3 seconds
-----------
Model1: Iteration 2
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:200000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:300000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:400000]
Reading more sentence pairs into memory ... 
Model1: (2) TRAIN CROSS-ENTROPY 5.57045 PERPLEXITY 47.5196
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 6.83332 PERPLEXITY 114.034
Model 1 Iteration: 2 took: 4 seconds
-----------
Model1: Iteration 3
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:200000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:300000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:400000]
Reading more sentence pairs into memory ... 
Model1: (3) TRAIN CROSS-ENTROPY 4.65857 PERPLEXITY 25.2562
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 5.38715 PERPLEXITY 41.8499
Model 1 Iteration: 3 took: 3 seconds
-----------
Model1: Iteration 4
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:200000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:300000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:400000]
Reading more sentence pairs into memory ... 
Model1: (4) TRAIN CROSS-ENTROPY 4.38036 PERPLEXITY 20.8267
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 4.93861 PERPLEXITY 30.6669
Model 1 Iteration: 4 took: 3 seconds
-----------
Model1: Iteration 5
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:200000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:300000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:400000]
Reading more sentence pairs into memory ... 
Model1: (5) TRAIN CROSS-ENTROPY 4.29693 PERPLEXITY 19.6565
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 4.77922 PERPLEXITY 27.4592
Model 1 Iteration: 5 took: 3 seconds
Entire Model1 Training took: 16 seconds
NOTE: I am doing iterations with the HMM model!
Read classes: #words: 504  #classes: 51
Read classes: #words: 342  #classes: 51

==========================================================
Hmm Training Started at: Sun Nov  3 00:25:18 2019

-----------
Hmm: Iteration 1
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:200000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:300000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:400000]
Reading more sentence pairs into memory ... 
A/D table contains 4151 parameters.
Hmm: (1) TRAIN CROSS-ENTROPY 4.26412 PERPLEXITY 19.2145
Hmm: (1) VITERBI TRAIN CROSS-ENTROPY 4.69953 PERPLEXITY 25.9836

Hmm Iteration: 1 took: 15 seconds

-----------
Hmm: Iteration 2
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:200000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:300000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:400000]
Reading more sentence pairs into memory ... 
A/D table contains 4151 parameters.
Hmm: (2) TRAIN CROSS-ENTROPY 2.61618 PERPLEXITY 6.13124
Hmm: (2) VITERBI TRAIN CROSS-ENTROPY 2.8219 PERPLEXITY 7.07095

Hmm Iteration: 2 took: 15 seconds

-----------
Hmm: Iteration 3
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:200000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:300000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:400000]
Reading more sentence pairs into memory ... 
A/D table contains 4151 parameters.
Hmm: (3) TRAIN CROSS-ENTROPY 2.23019 PERPLEXITY 4.69194
Hmm: (3) VITERBI TRAIN CROSS-ENTROPY 2.3618 PERPLEXITY 5.14011

Hmm Iteration: 3 took: 16 seconds

-----------
Hmm: Iteration 4
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:200000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:300000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:400000]
Reading more sentence pairs into memory ... 
A/D table contains 4151 parameters.
Hmm: (4) TRAIN CROSS-ENTROPY 2.09041 PERPLEXITY 4.25868
Hmm: (4) VITERBI TRAIN CROSS-ENTROPY 2.1956 PERPLEXITY 4.58079

Hmm Iteration: 4 took: 14 seconds

-----------
Hmm: Iteration 5
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:200000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:300000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:400000]
Reading more sentence pairs into memory ... 
A/D table contains 4151 parameters.
Hmm: (5) TRAIN CROSS-ENTROPY 2.04828 PERPLEXITY 4.13612
Hmm: (5) VITERBI TRAIN CROSS-ENTROPY 2.13772 PERPLEXITY 4.40068

Hmm Iteration: 5 took: 15 seconds

Entire Hmm Training took: 75 seconds
==========================================================
Read classes: #words: 504  #classes: 51
Read classes: #words: 342  #classes: 51
Read classes: #words: 504  #classes: 51
Read classes: #words: 342  #classes: 51

==========================================================
Starting H333444:  Viterbi Training
 H333444 Training Started at: Sun Nov  3 00:26:33 2019


---------------------
THTo3: Iteration 1
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
[sent:100000]
60000
70000
80000
90000
100000
Reading more sentence pairs into memory ... 
110000
120000
130000
140000
150000
Reading more sentence pairs into memory ... 
[sent:200000]
160000
170000
180000
190000
200000
Reading more sentence pairs into memory ... 
210000
220000
230000
240000
250000
Reading more sentence pairs into memory ... 
[sent:300000]
260000
270000
280000
290000
300000
Reading more sentence pairs into memory ... 
310000
320000
330000
340000
350000
Reading more sentence pairs into memory ... 
[sent:400000]
360000
370000
380000
390000
400000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 72.1107 #alsophisticatedcountcollection: 0 #hcsteps: 0
#peggingImprovements: 0
A/D table contains 4151 parameters.
A/D table contains 4669 parameters.
NTable contains 5050 parameter.
p0_count is 2.45502e+06 and p1 is 197008; p0 is 0.999 p1: 0.001
THTo3: TRAIN CROSS-ENTROPY 1.63382 PERPLEXITY 3.10333
THTo3: (1) TRAIN VITERBI CROSS-ENTROPY 1.70411 PERPLEXITY 3.25829

THTo3 Viterbi Iteration : 1 took: 16 seconds

---------------------
Model3: Iteration 2
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
[sent:100000]
60000
70000
80000
90000
100000
Reading more sentence pairs into memory ... 
110000
120000
130000
140000
150000
Reading more sentence pairs into memory ... 
[sent:200000]
160000
170000
180000
190000
200000
Reading more sentence pairs into memory ... 
210000
220000
230000
240000
250000
Reading more sentence pairs into memory ... 
[sent:300000]
260000
270000
280000
290000
300000
Reading more sentence pairs into memory ... 
310000
320000
330000
340000
350000
Reading more sentence pairs into memory ... 
[sent:400000]
360000
370000
380000
390000
400000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 71.6838 #alsophisticatedcountcollection: 0 #hcsteps: 1.51814
#peggingImprovements: 0
A/D table contains 4151 parameters.
A/D table contains 4669 parameters.
NTable contains 5050 parameter.
p0_count is 2.73473e+06 and p1 is 57224.8; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 2.4527 PERPLEXITY 5.47438
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 2.48581 PERPLEXITY 5.60147

Model3 Viterbi Iteration : 2 took: 16 seconds

---------------------
Model3: Iteration 3
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
[sent:100000]
60000
70000
80000
90000
100000
Reading more sentence pairs into memory ... 
110000
120000
130000
140000
150000
Reading more sentence pairs into memory ... 
[sent:200000]
160000
170000
180000
190000
200000
Reading more sentence pairs into memory ... 
210000
220000
230000
240000
250000
Reading more sentence pairs into memory ... 
[sent:300000]
260000
270000
280000
290000
300000
Reading more sentence pairs into memory ... 
310000
320000
330000
340000
350000
Reading more sentence pairs into memory ... 
[sent:400000]
360000
370000
380000
390000
400000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 71.7073 #alsophisticatedcountcollection: 0 #hcsteps: 1.24963
#peggingImprovements: 0
A/D table contains 4151 parameters.
A/D table contains 4669 parameters.
NTable contains 5050 parameter.
p0_count is 2.79226e+06 and p1 is 28460.2; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 2.31943 PERPLEXITY 4.99135
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 2.34249 PERPLEXITY 5.07178

Model3 Viterbi Iteration : 3 took: 17 seconds

---------------------
T3To4: Iteration 4
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
[sent:100000]
60000
70000
80000
90000
100000
Reading more sentence pairs into memory ... 
110000
120000
130000
140000
150000
Reading more sentence pairs into memory ... 
[sent:200000]
160000
170000
180000
190000
200000
Reading more sentence pairs into memory ... 
210000
220000
230000
240000
250000
Reading more sentence pairs into memory ... 
[sent:300000]
260000
270000
280000
290000
300000
Reading more sentence pairs into memory ... 
310000
320000
330000
340000
350000
Reading more sentence pairs into memory ... 
[sent:400000]
360000
370000
380000
390000
400000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 71.6195 #alsophisticatedcountcollection: 4.00711 #hcsteps: 1.26245
#peggingImprovements: 0
D4 table contains 512778 parameters.
A/D table contains 4151 parameters.
A/D table contains 4669 parameters.
NTable contains 5050 parameter.
p0_count is 2.81529e+06 and p1 is 16947.4; p0 is 0.999 p1: 0.001
T3To4: TRAIN CROSS-ENTROPY 2.2778 PERPLEXITY 4.84939
T3To4: (4) TRAIN VITERBI CROSS-ENTROPY 2.29036 PERPLEXITY 4.8918

T3To4 Viterbi Iteration : 4 took: 19 seconds

---------------------
Model4: Iteration 5
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
[sent:100000]
60000
70000
80000
90000
100000
Reading more sentence pairs into memory ... 
110000
120000
130000
140000
150000
Reading more sentence pairs into memory ... 
[sent:200000]
160000
170000
180000
190000
200000
Reading more sentence pairs into memory ... 
210000
220000
230000
240000
250000
Reading more sentence pairs into memory ... 
[sent:300000]
260000
270000
280000
290000
300000
Reading more sentence pairs into memory ... 
310000
320000
330000
340000
350000
Reading more sentence pairs into memory ... 
[sent:400000]
360000
370000
380000
390000
400000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 71.5876 #alsophisticatedcountcollection: 3.34913 #hcsteps: 1.11441
#peggingImprovements: 0
D4 table contains 512778 parameters.
A/D table contains 4151 parameters.
A/D table contains 4648 parameters.
NTable contains 5050 parameter.
p0_count is 2.82637e+06 and p1 is 11404.4; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 2.2725 PERPLEXITY 4.83159
Model4: (5) TRAIN VITERBI CROSS-ENTROPY 2.27938 PERPLEXITY 4.85469

Model4 Viterbi Iteration : 5 took: 33 seconds

---------------------
Model4: Iteration 6
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
[sent:100000]
60000
70000
80000
90000
100000
Reading more sentence pairs into memory ... 
110000
120000
130000
140000
150000
Reading more sentence pairs into memory ... 
[sent:200000]
160000
170000
180000
190000
200000
Reading more sentence pairs into memory ... 
210000
220000
230000
240000
250000
Reading more sentence pairs into memory ... 
[sent:300000]
260000
270000
280000
290000
300000
Reading more sentence pairs into memory ... 
310000
320000
330000
340000
350000
Reading more sentence pairs into memory ... 
[sent:400000]
360000
370000
380000
390000
400000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 71.5854 #alsophisticatedcountcollection: 3.10399 #hcsteps: 1.08067
#peggingImprovements: 0
D4 table contains 512778 parameters.
A/D table contains 4151 parameters.
A/D table contains 4648 parameters.
NTable contains 5050 parameter.
p0_count is 2.828e+06 and p1 is 10589.5; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 2.25456 PERPLEXITY 4.77189
Model4: (6) TRAIN VITERBI CROSS-ENTROPY 2.26113 PERPLEXITY 4.79366

Model4 Viterbi Iteration : 6 took: 39 seconds
H333444 Training Finished at: Sun Nov  3 00:28:53 2019


Entire Viterbi H333444 Training took: 140 seconds
==========================================================

Entire Training took: 232 seconds
Program Finished at: Sun Nov  3 00:28:53 2019

==========================================================
Executing: rm -f /home/anuadmin/moses/working/train/giza.ta-si/ta-si.A3.final.gz
Executing: gzip /home/anuadmin/moses/working/train/giza.ta-si/ta-si.A3.final
(2.1a) running snt2cooc si-ta @ Sun Nov  3 00:28:57 +0530 2019

Executing: mkdir -p /home/anuadmin/moses/working/train/giza.si-ta
Executing: /home/anuadmin/moses/mosesdecoder/tools/snt2cooc.out /home/anuadmin/moses/working/train/corpus/ta.vcb /home/anuadmin/moses/working/train/corpus/si.vcb /home/anuadmin/moses/working/train/corpus/si-ta-int-train.snt > /home/anuadmin/moses/working/train/giza.si-ta/si-ta.cooc
/home/anuadmin/moses/mosesdecoder/tools/snt2cooc.out /home/anuadmin/moses/working/train/corpus/ta.vcb /home/anuadmin/moses/working/train/corpus/si.vcb /home/anuadmin/moses/working/train/corpus/si-ta-int-train.snt > /home/anuadmin/moses/working/train/giza.si-ta/si-ta.cooc
line 1000
line 2000
line 3000
line 4000
line 5000
line 6000
line 7000
line 8000
line 9000
line 10000
line 11000
line 12000
line 13000
line 14000
line 15000
line 16000
line 17000
line 18000
line 19000
line 20000
line 21000
line 22000
line 23000
line 24000
line 25000
line 26000
line 27000
line 28000
line 29000
line 30000
line 31000
line 32000
line 33000
line 34000
line 35000
line 36000
line 37000
line 38000
line 39000
line 40000
line 41000
line 42000
line 43000
line 44000
line 45000
line 46000
line 47000
line 48000
line 49000
line 50000
line 51000
line 52000
line 53000
line 54000
line 55000
line 56000
line 57000
line 58000
line 59000
line 60000
line 61000
line 62000
line 63000
line 64000
line 65000
line 66000
line 67000
line 68000
line 69000
line 70000
line 71000
line 72000
line 73000
line 74000
line 75000
line 76000
line 77000
line 78000
line 79000
line 80000
line 81000
line 82000
line 83000
line 84000
line 85000
line 86000
line 87000
line 88000
line 89000
line 90000
line 91000
line 92000
line 93000
line 94000
line 95000
line 96000
line 97000
line 98000
line 99000
line 100000
line 101000
line 102000
line 103000
line 104000
line 105000
line 106000
line 107000
line 108000
line 109000
line 110000
line 111000
line 112000
line 113000
line 114000
line 115000
line 116000
line 117000
line 118000
line 119000
line 120000
line 121000
line 122000
line 123000
line 124000
line 125000
line 126000
line 127000
line 128000
line 129000
line 130000
line 131000
line 132000
line 133000
line 134000
line 135000
line 136000
line 137000
line 138000
line 139000
line 140000
line 141000
line 142000
line 143000
line 144000
line 145000
line 146000
line 147000
line 148000
line 149000
line 150000
line 151000
line 152000
line 153000
line 154000
line 155000
line 156000
line 157000
line 158000
line 159000
line 160000
line 161000
line 162000
line 163000
line 164000
line 165000
line 166000
line 167000
line 168000
line 169000
line 170000
line 171000
line 172000
line 173000
line 174000
line 175000
line 176000
line 177000
line 178000
line 179000
line 180000
line 181000
line 182000
line 183000
line 184000
line 185000
line 186000
line 187000
line 188000
line 189000
line 190000
line 191000
line 192000
line 193000
line 194000
line 195000
line 196000
line 197000
line 198000
line 199000
line 200000
line 201000
line 202000
line 203000
line 204000
line 205000
line 206000
line 207000
line 208000
line 209000
line 210000
line 211000
line 212000
line 213000
line 214000
line 215000
line 216000
line 217000
line 218000
line 219000
line 220000
line 221000
line 222000
line 223000
line 224000
line 225000
line 226000
line 227000
line 228000
line 229000
line 230000
line 231000
line 232000
line 233000
line 234000
line 235000
line 236000
line 237000
line 238000
line 239000
line 240000
line 241000
line 242000
line 243000
line 244000
line 245000
line 246000
line 247000
line 248000
line 249000
line 250000
line 251000
line 252000
line 253000
line 254000
line 255000
line 256000
line 257000
line 258000
line 259000
line 260000
line 261000
line 262000
line 263000
line 264000
line 265000
line 266000
line 267000
line 268000
line 269000
line 270000
line 271000
line 272000
line 273000
line 274000
line 275000
line 276000
line 277000
line 278000
line 279000
line 280000
line 281000
line 282000
line 283000
line 284000
line 285000
line 286000
line 287000
line 288000
line 289000
line 290000
line 291000
line 292000
line 293000
line 294000
line 295000
line 296000
line 297000
line 298000
line 299000
line 300000
line 301000
line 302000
line 303000
line 304000
line 305000
line 306000
line 307000
line 308000
line 309000
line 310000
line 311000
line 312000
line 313000
line 314000
line 315000
line 316000
line 317000
line 318000
line 319000
line 320000
line 321000
line 322000
line 323000
line 324000
line 325000
line 326000
line 327000
line 328000
line 329000
line 330000
line 331000
line 332000
line 333000
line 334000
line 335000
line 336000
line 337000
line 338000
line 339000
line 340000
line 341000
line 342000
line 343000
line 344000
line 345000
line 346000
line 347000
line 348000
line 349000
line 350000
line 351000
line 352000
line 353000
line 354000
line 355000
line 356000
line 357000
line 358000
line 359000
line 360000
line 361000
line 362000
line 363000
line 364000
line 365000
line 366000
line 367000
line 368000
line 369000
line 370000
line 371000
line 372000
line 373000
line 374000
line 375000
line 376000
line 377000
line 378000
line 379000
line 380000
line 381000
line 382000
line 383000
line 384000
line 385000
line 386000
line 387000
line 388000
line 389000
line 390000
line 391000
line 392000
line 393000
line 394000
line 395000
line 396000
line 397000
line 398000
line 399000
line 400000
END.
(2.1b) running giza si-ta @ Sun Nov  3 00:29:00 +0530 2019
/home/anuadmin/moses/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/anuadmin/moses/working/train/giza.si-ta/si-ta.cooc -c /home/anuadmin/moses/working/train/corpus/si-ta-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/anuadmin/moses/working/train/giza.si-ta/si-ta -onlyaldumps 1 -p0 0.999 -s /home/anuadmin/moses/working/train/corpus/ta.vcb -t /home/anuadmin/moses/working/train/corpus/si.vcb
Executing: /home/anuadmin/moses/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/anuadmin/moses/working/train/giza.si-ta/si-ta.cooc -c /home/anuadmin/moses/working/train/corpus/si-ta-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/anuadmin/moses/working/train/giza.si-ta/si-ta -onlyaldumps 1 -p0 0.999 -s /home/anuadmin/moses/working/train/corpus/ta.vcb -t /home/anuadmin/moses/working/train/corpus/si.vcb
/home/anuadmin/moses/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/anuadmin/moses/working/train/giza.si-ta/si-ta.cooc -c /home/anuadmin/moses/working/train/corpus/si-ta-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/anuadmin/moses/working/train/giza.si-ta/si-ta -onlyaldumps 1 -p0 0.999 -s /home/anuadmin/moses/working/train/corpus/ta.vcb -t /home/anuadmin/moses/working/train/corpus/si.vcb
Parameter 'coocurrencefile' changed from '' to '/home/anuadmin/moses/working/train/giza.si-ta/si-ta.cooc'
Parameter 'c' changed from '' to '/home/anuadmin/moses/working/train/corpus/si-ta-int-train.snt'
Parameter 'm3' changed from '5' to '3'
Parameter 'm4' changed from '5' to '3'
Parameter 'model1dumpfrequency' changed from '0' to '1'
Parameter 'model4smoothfactor' changed from '0.2' to '0.4'
Parameter 'nodumps' changed from '0' to '1'
Parameter 'nsmooth' changed from '64' to '4'
Parameter 'o' changed from '119-11-03.002900.anuadmin' to '/home/anuadmin/moses/working/train/giza.si-ta/si-ta'
Parameter 'onlyaldumps' changed from '0' to '1'
Parameter 'p0' changed from '-1' to '0.999'
Parameter 's' changed from '' to '/home/anuadmin/moses/working/train/corpus/ta.vcb'
Parameter 't' changed from '' to '/home/anuadmin/moses/working/train/corpus/si.vcb'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 119-11-03.002900.anuadmin.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/anuadmin/moses/working/train/giza.si-ta/si-ta  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/anuadmin/moses/working/train/corpus/si-ta-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/anuadmin/moses/working/train/corpus/ta.vcb  (source vocabulary file name)
t = /home/anuadmin/moses/working/train/corpus/si.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 119-11-03.002900.anuadmin.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/anuadmin/moses/working/train/giza.si-ta/si-ta  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/anuadmin/moses/working/train/corpus/si-ta-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/anuadmin/moses/working/train/corpus/ta.vcb  (source vocabulary file name)
t = /home/anuadmin/moses/working/train/corpus/si.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Reading vocabulary file from:/home/anuadmin/moses/working/train/corpus/ta.vcb
Reading vocabulary file from:/home/anuadmin/moses/working/train/corpus/si.vcb
Source vocabulary list has 343 unique tokens 
Target vocabulary list has 505 unique tokens 
Calculating vocabulary frequencies from corpus /home/anuadmin/moses/working/train/corpus/si-ta-int-train.snt
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:200000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:300000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:400000]
Reading more sentence pairs into memory ... 
 Train total # sentence pairs (weighted): 400000
Size of source portion of the training corpus: 2.84918e+06 tokens
Size of the target portion of the training corpus: 2.53762e+06 tokens 
In source portion of the training corpus, only 342 unique tokens appeared
In target portion of the training corpus, only 503 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 2.53762e+06/(3.24918e+06-400000)== 0.890651
There are 41876 41876 entries in table
==========================================================
Model1 Training Started at: Sun Nov  3 00:29:01 2019

-----------
Model1: Iteration 1
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:200000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:300000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:400000]
Reading more sentence pairs into memory ... 
Model1: (1) TRAIN CROSS-ENTROPY 9.45795 PERPLEXITY 703.278
Model1: (1) VITERBI TRAIN CROSS-ENTROPY 12.5095 PERPLEXITY 5830.76
Model 1 Iteration: 1 took: 4 seconds
-----------
Model1: Iteration 2
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:200000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:300000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:400000]
Reading more sentence pairs into memory ... 
Model1: (2) TRAIN CROSS-ENTROPY 5.73311 PERPLEXITY 53.1909
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 7.01946 PERPLEXITY 129.738
Model 1 Iteration: 2 took: 3 seconds
-----------
Model1: Iteration 3
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:200000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:300000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:400000]
Reading more sentence pairs into memory ... 
Model1: (3) TRAIN CROSS-ENTROPY 4.75455 PERPLEXITY 26.9936
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 5.47526 PERPLEXITY 44.4855
Model 1 Iteration: 3 took: 3 seconds
-----------
Model1: Iteration 4
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:200000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:300000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:400000]
Reading more sentence pairs into memory ... 
Model1: (4) TRAIN CROSS-ENTROPY 4.45121 PERPLEXITY 21.875
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 4.98312 PERPLEXITY 31.6279
Model 1 Iteration: 4 took: 4 seconds
-----------
Model1: Iteration 5
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:200000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:300000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:400000]
Reading more sentence pairs into memory ... 
Model1: (5) TRAIN CROSS-ENTROPY 4.36408 PERPLEXITY 20.593
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 4.81746 PERPLEXITY 28.1969
Model 1 Iteration: 5 took: 3 seconds
Entire Model1 Training took: 17 seconds
NOTE: I am doing iterations with the HMM model!
Read classes: #words: 342  #classes: 51
Read classes: #words: 504  #classes: 51

==========================================================
Hmm Training Started at: Sun Nov  3 00:29:18 2019

-----------
Hmm: Iteration 1
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:200000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:300000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:400000]
Reading more sentence pairs into memory ... 
A/D table contains 4717 parameters.
Hmm: (1) TRAIN CROSS-ENTROPY 4.33318 PERPLEXITY 20.1567
Hmm: (1) VITERBI TRAIN CROSS-ENTROPY 4.74641 PERPLEXITY 26.8418

Hmm Iteration: 1 took: 16 seconds

-----------
Hmm: Iteration 2
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:200000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:300000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:400000]
Reading more sentence pairs into memory ... 
A/D table contains 4717 parameters.
Hmm: (2) TRAIN CROSS-ENTROPY 2.59739 PERPLEXITY 6.05191
Hmm: (2) VITERBI TRAIN CROSS-ENTROPY 2.76022 PERPLEXITY 6.77501

Hmm Iteration: 2 took: 17 seconds

-----------
Hmm: Iteration 3
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:200000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:300000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:400000]
Reading more sentence pairs into memory ... 
A/D table contains 4717 parameters.
Hmm: (3) TRAIN CROSS-ENTROPY 2.21688 PERPLEXITY 4.64886
Hmm: (3) VITERBI TRAIN CROSS-ENTROPY 2.32567 PERPLEXITY 5.013

Hmm Iteration: 3 took: 17 seconds

-----------
Hmm: Iteration 4
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:200000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:300000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:400000]
Reading more sentence pairs into memory ... 
A/D table contains 4717 parameters.
Hmm: (4) TRAIN CROSS-ENTROPY 2.11619 PERPLEXITY 4.33548
Hmm: (4) VITERBI TRAIN CROSS-ENTROPY 2.20692 PERPLEXITY 4.61688

Hmm Iteration: 4 took: 16 seconds

-----------
Hmm: Iteration 5
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:200000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:300000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:400000]
Reading more sentence pairs into memory ... 
A/D table contains 4717 parameters.
Hmm: (5) TRAIN CROSS-ENTROPY 2.08034 PERPLEXITY 4.22906
Hmm: (5) VITERBI TRAIN CROSS-ENTROPY 2.15407 PERPLEXITY 4.45082

Hmm Iteration: 5 took: 16 seconds

Entire Hmm Training took: 82 seconds
==========================================================
Read classes: #words: 342  #classes: 51
Read classes: #words: 504  #classes: 51
Read classes: #words: 342  #classes: 51
Read classes: #words: 504  #classes: 51

==========================================================
Starting H333444:  Viterbi Training
 H333444 Training Started at: Sun Nov  3 00:30:40 2019


---------------------
THTo3: Iteration 1
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
[sent:100000]
60000
70000
80000
90000
100000
Reading more sentence pairs into memory ... 
110000
120000
130000
140000
150000
Reading more sentence pairs into memory ... 
[sent:200000]
160000
170000
180000
190000
200000
Reading more sentence pairs into memory ... 
210000
220000
230000
240000
250000
Reading more sentence pairs into memory ... 
[sent:300000]
260000
270000
280000
290000
300000
Reading more sentence pairs into memory ... 
310000
320000
330000
340000
350000
Reading more sentence pairs into memory ... 
[sent:400000]
360000
370000
380000
390000
400000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 67.1837 #alsophisticatedcountcollection: 0 #hcsteps: 0
#peggingImprovements: 0
A/D table contains 4717 parameters.
A/D table contains 4020 parameters.
NTable contains 3430 parameter.
p0_count is 2.40932e+06 and p1 is 63905.4; p0 is 0.999 p1: 0.001
THTo3: TRAIN CROSS-ENTROPY 1.58839 PERPLEXITY 3.00714
THTo3: (1) TRAIN VITERBI CROSS-ENTROPY 1.64796 PERPLEXITY 3.13389

THTo3 Viterbi Iteration : 1 took: 16 seconds

---------------------
Model3: Iteration 2
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
[sent:100000]
60000
70000
80000
90000
100000
Reading more sentence pairs into memory ... 
110000
120000
130000
140000
150000
Reading more sentence pairs into memory ... 
[sent:200000]
160000
170000
180000
190000
200000
Reading more sentence pairs into memory ... 
210000
220000
230000
240000
250000
Reading more sentence pairs into memory ... 
[sent:300000]
260000
270000
280000
290000
300000
Reading more sentence pairs into memory ... 
310000
320000
330000
340000
350000
Reading more sentence pairs into memory ... 
[sent:400000]
360000
370000
380000
390000
400000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 67.1857 #alsophisticatedcountcollection: 0 #hcsteps: 1.16489
#peggingImprovements: 0
A/D table contains 4717 parameters.
A/D table contains 4019 parameters.
NTable contains 3430 parameter.
p0_count is 2.46921e+06 and p1 is 34209.7; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 2.10405 PERPLEXITY 4.29916
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 2.13671 PERPLEXITY 4.39757

Model3 Viterbi Iteration : 2 took: 18 seconds

---------------------
Model3: Iteration 3
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
[sent:100000]
60000
70000
80000
90000
100000
Reading more sentence pairs into memory ... 
110000
120000
130000
140000
150000
Reading more sentence pairs into memory ... 
[sent:200000]
160000
170000
180000
190000
200000
Reading more sentence pairs into memory ... 
210000
220000
230000
240000
250000
Reading more sentence pairs into memory ... 
[sent:300000]
260000
270000
280000
290000
300000
Reading more sentence pairs into memory ... 
310000
320000
330000
340000
350000
Reading more sentence pairs into memory ... 
[sent:400000]
360000
370000
380000
390000
400000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 67.1521 #alsophisticatedcountcollection: 0 #hcsteps: 1.10603
#peggingImprovements: 0
A/D table contains 4717 parameters.
A/D table contains 4019 parameters.
NTable contains 3430 parameter.
p0_count is 2.48652e+06 and p1 is 25554.6; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 2.05344 PERPLEXITY 4.15094
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 2.08388 PERPLEXITY 4.23946

Model3 Viterbi Iteration : 3 took: 17 seconds

---------------------
T3To4: Iteration 4
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
[sent:100000]
60000
70000
80000
90000
100000
Reading more sentence pairs into memory ... 
110000
120000
130000
140000
150000
Reading more sentence pairs into memory ... 
[sent:200000]
160000
170000
180000
190000
200000
Reading more sentence pairs into memory ... 
210000
220000
230000
240000
250000
Reading more sentence pairs into memory ... 
[sent:300000]
260000
270000
280000
290000
300000
Reading more sentence pairs into memory ... 
310000
320000
330000
340000
350000
Reading more sentence pairs into memory ... 
[sent:400000]
360000
370000
380000
390000
400000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 67.1206 #alsophisticatedcountcollection: 3.59478 #hcsteps: 1.13503
#peggingImprovements: 0
D4 table contains 516838 parameters.
A/D table contains 4717 parameters.
A/D table contains 4019 parameters.
NTable contains 3430 parameter.
p0_count is 2.5076e+06 and p1 is 15011.5; p0 is 0.999 p1: 0.001
T3To4: TRAIN CROSS-ENTROPY 2.02952 PERPLEXITY 4.0827
T3To4: (4) TRAIN VITERBI CROSS-ENTROPY 2.04968 PERPLEXITY 4.14013

T3To4 Viterbi Iteration : 4 took: 20 seconds

---------------------
Model4: Iteration 5
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
[sent:100000]
60000
70000
80000
90000
100000
Reading more sentence pairs into memory ... 
110000
120000
130000
140000
150000
Reading more sentence pairs into memory ... 
[sent:200000]
160000
170000
180000
190000
200000
Reading more sentence pairs into memory ... 
210000
220000
230000
240000
250000
Reading more sentence pairs into memory ... 
[sent:300000]
260000
270000
280000
290000
300000
Reading more sentence pairs into memory ... 
310000
320000
330000
340000
350000
Reading more sentence pairs into memory ... 
[sent:400000]
360000
370000
380000
390000
400000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 67.1189 #alsophisticatedcountcollection: 3.32136 #hcsteps: 1.12948
#peggingImprovements: 0
D4 table contains 516838 parameters.
A/D table contains 4717 parameters.
A/D table contains 4020 parameters.
NTable contains 3430 parameter.
p0_count is 2.51676e+06 and p1 is 10434.3; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 2.24148 PERPLEXITY 4.72881
Model4: (5) TRAIN VITERBI CROSS-ENTROPY 2.25426 PERPLEXITY 4.77088

Model4 Viterbi Iteration : 5 took: 32 seconds

---------------------
Model4: Iteration 6
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
[sent:100000]
60000
70000
80000
90000
100000
Reading more sentence pairs into memory ... 
110000
120000
130000
140000
150000
Reading more sentence pairs into memory ... 
[sent:200000]
160000
170000
180000
190000
200000
Reading more sentence pairs into memory ... 
210000
220000
230000
240000
250000
Reading more sentence pairs into memory ... 
[sent:300000]
260000
270000
280000
290000
300000
Reading more sentence pairs into memory ... 
310000
320000
330000
340000
350000
Reading more sentence pairs into memory ... 
[sent:400000]
360000
370000
380000
390000
400000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 67.1185 #alsophisticatedcountcollection: 3.2438 #hcsteps: 1.12312
#peggingImprovements: 0
D4 table contains 516838 parameters.
A/D table contains 4717 parameters.
A/D table contains 4019 parameters.
NTable contains 3430 parameter.
p0_count is 2.51708e+06 and p1 is 10273.1; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 2.22419 PERPLEXITY 4.67248
Model4: (6) TRAIN VITERBI CROSS-ENTROPY 2.23226 PERPLEXITY 4.69869

Model4 Viterbi Iteration : 6 took: 38 seconds
H333444 Training Finished at: Sun Nov  3 00:33:01 2019


Entire Viterbi H333444 Training took: 141 seconds
==========================================================

Entire Training took: 241 seconds
Program Finished at: Sun Nov  3 00:33:01 2019

==========================================================
Executing: rm -f /home/anuadmin/moses/working/train/giza.si-ta/si-ta.A3.final.gz
Executing: gzip /home/anuadmin/moses/working/train/giza.si-ta/si-ta.A3.final
(3) generate word alignment @ Sun Nov  3 00:33:04 +0530 2019
Combining forward and inverted alignment from files:
  /home/anuadmin/moses/working/train/giza.ta-si/ta-si.A3.final.{bz2,gz}
  /home/anuadmin/moses/working/train/giza.si-ta/si-ta.A3.final.{bz2,gz}
Executing: mkdir -p /home/anuadmin/moses/working/train/model
Executing: /home/anuadmin/moses/mosesdecoder/scripts/training/giza2bal.pl -d "gzip -cd /home/anuadmin/moses/working/train/giza.si-ta/si-ta.A3.final.gz" -i "gzip -cd /home/anuadmin/moses/working/train/giza.ta-si/ta-si.A3.final.gz" |/home/anuadmin/moses/mosesdecoder/scripts/../bin/symal -alignment="grow" -diagonal="yes" -final="yes" -both="yes" > /home/anuadmin/moses/working/train/model/aligned.grow-diag-final-and
symal: computing grow alignment: diagonal (1) final (1)both-uncovered (1)
skip=<0> counts=<400000>
(4) generate lexical translation table 0-0 @ Sun Nov  3 00:33:47 +0530 2019
(/home/anuadmin/moses/corpus/sinhalese-ta-si/data.ta-si.clean.ta,/home/anuadmin/moses/corpus/sinhalese-ta-si/data.ta-si.clean.si,/home/anuadmin/moses/working/train/model/lex)
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
Saved: /home/anuadmin/moses/working/train/model/lex.f2e and /home/anuadmin/moses/working/train/model/lex.e2f
FILE: /home/anuadmin/moses/corpus/sinhalese-ta-si/data.ta-si.clean.si
FILE: /home/anuadmin/moses/corpus/sinhalese-ta-si/data.ta-si.clean.ta
FILE: /home/anuadmin/moses/working/train/model/aligned.grow-diag-final-and
(5) extract phrases @ Sun Nov  3 00:34:02 +0530 2019
/home/anuadmin/moses/mosesdecoder/scripts/generic/extract-parallel.perl 4 split "sort    " /home/anuadmin/moses/mosesdecoder/scripts/../bin/extract /home/anuadmin/moses/corpus/sinhalese-ta-si/data.ta-si.clean.si /home/anuadmin/moses/corpus/sinhalese-ta-si/data.ta-si.clean.ta /home/anuadmin/moses/working/train/model/aligned.grow-diag-final-and /home/anuadmin/moses/working/train/model/extract 7 orientation --model wbe-msd --GZOutput 
Executing: /home/anuadmin/moses/mosesdecoder/scripts/generic/extract-parallel.perl 4 split "sort    " /home/anuadmin/moses/mosesdecoder/scripts/../bin/extract /home/anuadmin/moses/corpus/sinhalese-ta-si/data.ta-si.clean.si /home/anuadmin/moses/corpus/sinhalese-ta-si/data.ta-si.clean.ta /home/anuadmin/moses/working/train/model/aligned.grow-diag-final-and /home/anuadmin/moses/working/train/model/extract 7 orientation --model wbe-msd --GZOutput 
MAX 7 1 0
Started Sun Nov  3 00:34:02 2019
using gzip 
isBSDSplit=0 
Executing: mkdir -p /home/anuadmin/moses/working/train/model/tmp.12632; ls -l /home/anuadmin/moses/working/train/model/tmp.12632 
total=400000 line-per-split=100001 
split -d -l 100001 -a 7 /home/anuadmin/moses/corpus/sinhalese-ta-si/data.ta-si.clean.si /home/anuadmin/moses/working/train/model/tmp.12632/target.split -d -l 100001 -a 7 /home/anuadmin/moses/corpus/sinhalese-ta-si/data.ta-si.clean.ta /home/anuadmin/moses/working/train/model/tmp.12632/source.split -d -l 100001 -a 7 /home/anuadmin/moses/working/train/model/aligned.grow-diag-final-and /home/anuadmin/moses/working/train/model/tmp.12632/align.merging extract / extract.inv
gunzip -c /home/anuadmin/moses/working/train/model/tmp.12632/extract.0000000.gz /home/anuadmin/moses/working/train/model/tmp.12632/extract.0000001.gz /home/anuadmin/moses/working/train/model/tmp.12632/extract.0000002.gz /home/anuadmin/moses/working/train/model/tmp.12632/extract.0000003.gz  | LC_ALL=C sort     -T /home/anuadmin/moses/working/train/model/tmp.12632 2>> /dev/stderr | gzip -c > /home/anuadmin/moses/working/train/model/extract.sorted.gz 2>> /dev/stderr 
gunzip -c /home/anuadmin/moses/working/train/model/tmp.12632/extract.0000000.inv.gz /home/anuadmin/moses/working/train/model/tmp.12632/extract.0000001.inv.gz /home/anuadmin/moses/working/train/model/tmp.12632/extract.0000002.inv.gz /home/anuadmin/moses/working/train/model/tmp.12632/extract.0000003.inv.gz  | LC_ALL=C sort     -T /home/anuadmin/moses/working/train/model/tmp.12632 2>> /dev/stderr | gzip -c > /home/anuadmin/moses/working/train/model/extract.inv.sorted.gz 2>> /dev/stderr 
gunzip -c /home/anuadmin/moses/working/train/model/tmp.12632/extract.0000000.o.gz /home/anuadmin/moses/working/train/model/tmp.12632/extract.0000001.o.gz /home/anuadmin/moses/working/train/model/tmp.12632/extract.0000002.o.gz /home/anuadmin/moses/working/train/model/tmp.12632/extract.0000003.o.gz  | LC_ALL=C sort     -T /home/anuadmin/moses/working/train/model/tmp.12632 2>> /dev/stderr | gzip -c > /home/anuadmin/moses/working/train/model/extract.o.sorted.gz 2>> /dev/stderr 
Finished Sun Nov  3 00:35:30 2019
(6) score phrases @ Sun Nov  3 00:35:30 +0530 2019
(6.1)  creating table half /home/anuadmin/moses/working/train/model/phrase-table.half.f2e @ Sun Nov  3 00:35:30 +0530 2019
/home/anuadmin/moses/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/anuadmin/moses/mosesdecoder/scripts/../bin/score /home/anuadmin/moses/working/train/model/extract.sorted.gz /home/anuadmin/moses/working/train/model/lex.f2e /home/anuadmin/moses/working/train/model/phrase-table.half.f2e.gz  0 
Executing: /home/anuadmin/moses/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/anuadmin/moses/mosesdecoder/scripts/../bin/score /home/anuadmin/moses/working/train/model/extract.sorted.gz /home/anuadmin/moses/working/train/model/lex.f2e /home/anuadmin/moses/working/train/model/phrase-table.half.f2e.gz  0 
using gzip 
Started Sun Nov  3 00:35:30 2019
/home/anuadmin/moses/mosesdecoder/scripts/../bin/score /home/anuadmin/moses/working/train/model/tmp.12684/extract.0.gz /home/anuadmin/moses/working/train/model/lex.f2e /home/anuadmin/moses/working/train/model/tmp.12684/phrase-table.half.0000000.gz  2>> /dev/stderr 
/home/anuadmin/moses/working/train/model/tmp.12684/run.0.sh/home/anuadmin/moses/working/train/model/tmp.12684/run.3.sh/home/anuadmin/moses/working/train/model/tmp.12684/run.1.sh/home/anuadmin/moses/working/train/model/tmp.12684/run.2.shmv /home/anuadmin/moses/working/train/model/tmp.12684/phrase-table.half.0000000.gz /home/anuadmin/moses/working/train/model/phrase-table.half.f2e.gzrm -rf /home/anuadmin/moses/working/train/model/tmp.12684 
Finished Sun Nov  3 00:35:57 2019
(6.3)  creating table half /home/anuadmin/moses/working/train/model/phrase-table.half.e2f @ Sun Nov  3 00:35:57 +0530 2019
/home/anuadmin/moses/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/anuadmin/moses/mosesdecoder/scripts/../bin/score /home/anuadmin/moses/working/train/model/extract.inv.sorted.gz /home/anuadmin/moses/working/train/model/lex.e2f /home/anuadmin/moses/working/train/model/phrase-table.half.e2f.gz --Inverse 1 
Executing: /home/anuadmin/moses/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/anuadmin/moses/mosesdecoder/scripts/../bin/score /home/anuadmin/moses/working/train/model/extract.inv.sorted.gz /home/anuadmin/moses/working/train/model/lex.e2f /home/anuadmin/moses/working/train/model/phrase-table.half.e2f.gz --Inverse 1 
using gzip 
Started Sun Nov  3 00:35:57 2019
/home/anuadmin/moses/mosesdecoder/scripts/../bin/score /home/anuadmin/moses/working/train/model/tmp.12711/extract.0.gz /home/anuadmin/moses/working/train/model/lex.e2f /home/anuadmin/moses/working/train/model/tmp.12711/phrase-table.half.0000000.gz --Inverse  2>> /dev/stderr 
/home/anuadmin/moses/working/train/model/tmp.12711/run.0.sh/home/anuadmin/moses/working/train/model/tmp.12711/run.3.sh/home/anuadmin/moses/working/train/model/tmp.12711/run.1.sh/home/anuadmin/moses/working/train/model/tmp.12711/run.2.shgunzip -c /home/anuadmin/moses/working/train/model/tmp.12711/phrase-table.half.*.gz 2>> /dev/stderr| LC_ALL=C sort     -T /home/anuadmin/moses/working/train/model/tmp.12711  | gzip -c > /home/anuadmin/moses/working/train/model/phrase-table.half.e2f.gz  2>> /dev/stderr rm -rf /home/anuadmin/moses/working/train/model/tmp.12711 
Finished Sun Nov  3 00:36:29 2019
(6.6) consolidating the two halves @ Sun Nov  3 00:36:29 +0530 2019
Executing: /home/anuadmin/moses/mosesdecoder/scripts/../bin/consolidate /home/anuadmin/moses/working/train/model/phrase-table.half.f2e.gz /home/anuadmin/moses/working/train/model/phrase-table.half.e2f.gz /dev/stdout | gzip -c > /home/anuadmin/moses/working/train/model/phrase-table.gz
Consolidate v2.0 written by Philipp Koehn
consolidating direct and indirect rule tables
.........
Executing: rm -f /home/anuadmin/moses/working/train/model/phrase-table.half.*
(7) learn reordering model @ Sun Nov  3 00:36:37 +0530 2019
(7.1) [no factors] learn reordering model @ Sun Nov  3 00:36:37 +0530 2019
(7.2) building tables @ Sun Nov  3 00:36:37 +0530 2019
Executing: /home/anuadmin/moses/mosesdecoder/scripts/../bin/lexical-reordering-score /home/anuadmin/moses/working/train/model/extract.o.sorted.gz 0.5 /home/anuadmin/moses/working/train/model/reordering-table. --model "wbe msd wbe-msd-bidirectional-fe"
Lexical Reordering Scorer
scores lexical reordering models of several types (hierarchical, phrase-based and word-based-extraction
(8) learn generation model @ Sun Nov  3 00:36:48 +0530 2019
  no generation model requested, skipping step
(9) create moses.ini @ Sun Nov  3 00:36:48 +0530 2019
